{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9cb05-7abc-41c2-a6fd-154eb4409112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f89299-b113-43bc-9605-d4fa6d451568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "052c6234-e8b0-4110-bf60-dc46c0a5d4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: backports.entry-points-selectable in c:\\users\\michel.marien_icarew\\appdata\\roaming\\python\\python311\\site-packages (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: backports.tempfile in c:\\programdata\\anaconda3\\lib\\site-packages (1.0)\n",
      "Requirement already satisfied: backports.weakref in c:\\programdata\\anaconda3\\lib\\site-packages (from backports.tempfile) (1.0.post1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: setuptools==65.5.0 in c:\\users\\michel.marien_icarew\\appdata\\roaming\\python\\python311\\site-packages (65.5.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fastdtw in c:\\users\\michel.marien_icarew\\appdata\\roaming\\python\\python311\\site-packages (0.3.4)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from fastdtw) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "#!pip install ffmpeg-python>=0.2.0\n",
    "#!pip install matplotlib>=3.0.2\n",
    "#!pip install munkres>=1.1.2\n",
    "#!pip install numpy>=1.16\n",
    "#!pip install opencv-python>=3.4\n",
    "#!pip install Pillow>=5.4\n",
    "#!pip install vidgear>=0.1.4\n",
    "#!pip install torch>=1.4.0\n",
    "#!pip install torchvision>=0.5.0\n",
    "#!pip install tqdm>=4.26\n",
    "#!pip install tensorboard>=1.11\n",
    "#!pip install tensorboardX>=1.4\n",
    "#!pip install backports.entry-points-selectable\n",
    "#!pip install backports.tempfile\n",
    "#!pip uninstall -y setuptools\n",
    "#!pip install setuptools==65.5.0\n",
    "#!pip install fastdtw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd39210-49ef-44d2-98c4-707846926938",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0e6e19b7-7a76-4316-8ebd-4d9e021ef5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pathlib\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from SimpleHRNet import SimpleHRNet\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.signal import find_peaks\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a048790-fa43-4c11-a08b-ed6476983fe0",
   "metadata": {},
   "source": [
    "### Constants & Configuration\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell defines all the essential constants, file paths, processing parameters, and exercise-specific configurations used throughout the notebook. It centralizes settings, making it easier to modify the script's behavior without searching through multiple functions.\n",
    "\n",
    "* Paths determine data sources and model location.\n",
    "* Joint settings ensure consistency with the pose model.\n",
    "* Processing parameters directly influence normalization, matching sensitivity (thresholds), and repetition counting accuracy (peak detection, smoothing). The parameters grouped under \"User Settings & Tuning Parameters\" are the ones most likely needing adjustment to optimize performance for your specific videos and exercises.\n",
    "* `EXERCISE_CONFIG` dictates the core signal processing logic used to analyze the movement pattern for repetition analysis for each distinct exercise type.\n",
    "\n",
    "**Outcomes:**\n",
    "\n",
    "Running this cell does not return any values directly but defines the following variables in the notebook's memory, making them available to subsequent cells:\n",
    "\n",
    "* **File/Model Paths (User Must Set):**\n",
    "    * `MODEL_PATH`: ( `pathlib.Path`) - The full path to the downloaded SimpleHRNet model weights file (e.g., `.pth`).\n",
    "    * `BASELINE_ROOT_DIR`: ( `pathlib.Path`) - The full path to the main folder containing subfolders for each baseline exercise video (ensure filenames include angle and rep count, e.g., `squat_10reps_front_video1.mp4`).\n",
    "    * `VIDEO_TO_RECOGNIZE_PATH`: ( `pathlib.Path`) - The full path to the default test video file you want to analyze later.\n",
    "* **Model & Pose Structure:**\n",
    "    * `EXPECTED_JOINT_COUNT`: ( `int`, Unit = Count) - The number of keypoints the HRNet model detects (17 for COCO). Should match the loaded model.\n",
    "    * `JOINT_NAMES_LIST`: ( `list` of `str`, Unit = Name) - Ordered list of names corresponding to the keypoint indices output by the model. Order **must** match the model's output.\n",
    "* **Core Processing Settings:**\n",
    "* `TARGET_REP_LENGTH`: ( `int`, Unit = Frames / Timesteps)\n",
    "        * **Purpose:** This parameter defines a **standard length** (in terms of number of data points or \"frames\") for every single repetition segment after it has been detected. Real exercise repetitions naturally vary in duration (some reps are faster, some slower). To compare them fairly using DTW or to average them meaningfully for creating a baseline, they need to be represented over the same timescale.\n",
    "        * **Mechanism:** The `time_normalize_sequence` function takes a repetition segment (which might have, say, 30 frames or 50 frames) and resamples it using linear interpolation to produce a new sequence that always has exactly `TARGET_REP_LENGTH` frames (e.g., 100). This stretches or compresses the time dimension of the repetition.\n",
    "        * **Effect on DTW Comparison & Sensitivity:** DTW calculates a distance based on the alignment between two sequences. The absolute magnitude of this distance is influenced by the length of the sequences being compared. By normalizing all repetitions (both from baselines and the test video) to the *same target length*, we ensure that the DTW distances primarily reflect differences in the *shape* of the movement over that standard duration, rather than being heavily skewed just because one raw repetition took more frames than another. This standardization makes the `DTW_DISTANCE_THRESHOLD` more consistently applicable across different reps and exercises. Choosing the value (e.g., 100) is often a balance – long enough to capture the details of the movement, but not so long that it excessively increases computation time or overemphasizes noise.\n",
    "* **User Settings & Tuning Parameters (Require Experimentation):**\n",
    "    * `DTW_DISTANCE_THRESHOLD`: ( `float`, Unit = Unitless Distance) - **Fallback** threshold for DTW comparison. Used if a specific derived threshold isn't available. Lower values are stricter for matching. *Tuning recommended based on derived threshold outputs (Cell 6.5).*\n",
    "    * `TUNING_PROMINENCE_RANGE`: ( `list` of `float`, Unit = Unitless Ratio) - Defines the search space for the `peak_prominence` parameter during automatic tuning of repetition counting. Controls how much a peak must stand out relative to its surroundings (0.0 to 1.0). *Tuning the range might be needed.*\n",
    "    * `TUNING_DISTANCE_RANGE`: ( `list` of `int`, Unit = Frames) - Defines the search space for the `peak_distance` parameter during automatic tuning. Controls the minimum number of frames separating detected repetitions. *Tuning the range might be needed.*\n",
    "    * `DEFAULT_SMOOTHING_WINDOW`: ( `int`, Unit = Frames) - Size of the moving average window used to smooth the trajectory signal *before* peak detection during tuning and recognition. Helps reduce noise but can flatten small peaks. *Tuning recommended.*\n",
    "    * `DEFAULT_PEAK_PROMINENCE`: ( `float`, Unit = Unitless Ratio) - **Fallback** prominence value used for repetition counting if tuning fails or isn't performed for an exercise/angle. *Tuning recommended.*\n",
    "    * `DEFAULT_PEAK_DISTANCE`: ( `int`, Unit = Frames) - **Fallback** minimum distance between peaks used for repetition counting if tuning fails or isn't performed. *Tuning recommended.*\n",
    "    * `DEFAULT_THRESHOLD_STD_MULTIPLIER`: ( `float`, Unit = Unitless) - Factor (N) used when deriving thresholds automatically (Mean + N * StdDev). Higher values create larger, more lenient thresholds. *Tuning recommended.*\n",
    "    * `DEFAULT_MIN_REPS_FOR_THRESHOLD`: ( `int`, Unit = Count) - Minimum number of valid repetitions needed within a specific baseline angle category to attempt deriving its threshold.\n",
    "* **Exercise Configuration:**\n",
    "    * `EXERCISE_CONFIG`: ( `dict`) - A nested dictionary defining the specific logic for calculating the 1D trajectory used in segmentation/counting for each exercise.\n",
    "        * `metric`: ( `str`) Specifies the method (e.g., `'elbow_angle'`, `'hip_y'`).\n",
    "        * `joints`: ( `list` of `str`) Base names of joints needed for the metric.\n",
    "        * `invert_for_valley`: ( `bool`) Whether the lowest point (`True`) or highest point (`False`) of the metric signal typically marks the end/start of a repetition cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2ee4a028-1453-450b-bcfd-c76c98565fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = pathlib.Path(\"C:/Users/michel.marien_icarew/Documents/Privé/Opleiding/Mastervakken/Deep Neural Networks/Opdacht 2/simple-HRNet/weights/pose_hrnet_w48_256x192.pth\") # Path to HRNet weights\n",
    "BASELINE_ROOT_DIR = \"C:/Users/michel.marien_icarew/Documents/Privé/Opleiding/Mastervakken/Deep Neural Networks/Opdacht 2/baselines2\"\n",
    "VIDEO_TO_RECOGNIZE_PATH = \"C:/Users/michel.marien_icarew/Documents/Privé/Opleiding/Mastervakken/Deep Neural Networks/Opdacht 2/test_videos/test_excercise.mp4\" # Video file to analyze\n",
    "\n",
    "# Model & Pose Settings\n",
    "EXPECTED_JOINT_COUNT = 17 # COCO keypoints format\n",
    "JOINT_NAMES_LIST = [\"Nose\",\"Left Eye\",\"Right Eye\",\"Left Ear\",\"Right Ear\",\"Left Shoulder\",\n",
    "                    \"Right Shoulder\",\"Left Elbow\",\"Right Elbow\",\"Left Wrist\",\"Right Wrist\",\n",
    "                    \"Left Hip\",\"Right Hip\",\"Left Knee\",\"Right Knee\",\"Left Ankle\",\"Right Ankle\"]\n",
    "\n",
    "# Recognition & Processing Parameters\n",
    "TARGET_REP_LENGTH = 100\n",
    "DTW_DISTANCE_THRESHOLD = 500.0\n",
    "\n",
    "# Parameters for Repetition Counting Tuning (Grid Search Ranges)\n",
    "TUNING_PROMINENCE_RANGE = [0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14]\n",
    "TUNING_DISTANCE_RANGE = [5, 8, 10, 12, 15, 20, 25, 30, 35]\n",
    "\n",
    "# Parameters for Threshold Derivation\n",
    "DEFAULT_THRESHOLD_STD_MULTIPLIER = 2.0 # Default for mean + N*stddev calculation\n",
    "DEFAULT_MIN_REPS_FOR_THRESHOLD = 3     # Min reps needed in baseline angle category to derive threshold\n",
    "\n",
    "# Fallback if tuning fails or not enough data\n",
    "DEFAULT_SMOOTHING_WINDOW = 5\n",
    "DEFAULT_PEAK_PROMINENCE = 0.05\n",
    "DEFAULT_PEAK_DISTANCE = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f92e24-9d2c-44d9-8b95-1334741d488a",
   "metadata": {},
   "source": [
    "#### Configuration Block: `EXERCISE_CONFIG`\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This dictionary (`EXERCISE_CONFIG`) defines how the primary movement trajectory should be calculated for each specific exercise during the repetition counting steps. This dictionary stores the **exercise-specific rules** for quantifying the repetitive motion, which are then used by the processing functions. You can add or modify entries here to support new exercises or change the analysis method for existing ones.\n",
    "\n",
    "The keys of the dictionary (e.g., `'tricep_pushdown'`) should match the corresponding baseline subfolder names (pay attention to lowercase/underscores).\n",
    "\n",
    "**Outcomes:**\n",
    "\n",
    "Running this code block defines the `EXERCISE_CONFIG` variable in the notebook's memory.\n",
    "\n",
    "* `EXERCISE_CONFIG`: ( `dict`)\n",
    "    * **Keys:** Exercise label strings (e.g., `'squat'`).\n",
    "    * **Values:** A nested dictionary containing configuration for that specific exercise. Currently, it holds the `'trajectory_logic'`.\n",
    "\n",
    "**Types/Units/Meaning within `trajectory_logic`:**\n",
    "\n",
    "For each exercise, the nested `'trajectory_logic'` dictionary specifies:\n",
    "\n",
    "* `'metric'`: ( `str`)\n",
    "    * **Meaning:** Defines *what measurement* represents the cyclical motion of the repetition (e.g., `'elbow_angle'`, `'hip_y'`, `'shoulder_y'`). The `_calculate_trajectory_from_config` function uses this string to select the correct calculation method.\n",
    "    * Unit = Depends on the metric (e.g., degrees for angles, normalized Y-coordinate units for `_y` metrics).\n",
    "* `'joints'`: ( `list` of `str`)\n",
    "    * **Meaning:** Specifies the *base names* of the joints required to calculate the chosen `metric`. For metrics involving paired joints (like angles or averaging Y-coordinates), the code will automatically look for 'Left' and 'Right' versions (e.g., if 'Shoulder' is listed for a `_y` metric, it will try to find 'Left Shoulder' and 'Right Shoulder'). For angle metrics needing 3 joints, list them in order (e.g., `['Shoulder', 'Elbow', 'Wrist']`).\n",
    "    * Unit = Joint names (strings).\n",
    "* `'invert_for_valley'`: ( `bool`)\n",
    "    * **Meaning:** This crucial flag tells the peak-finding algorithm whether the defining moment of a repetition cycle corresponds to a *minimum* value (`True`) or a *maximum* value (`False`) in the calculated `trajectory` signal.\n",
    "        * Set to `True` if the lowest point marks the rep boundary (e.g., bottom of a squat, lowest point of a dip, most extended arm in a bicep curl). The code will invert the trajectory so `find_peaks` looks for these valleys.\n",
    "        * Set to `False` if the highest point marks the rep boundary (e.g., fully extended arm in a tricep pushdown). The code will use the trajectory directly with `find_peaks`.\n",
    "    * Unit = Boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "eaed6a0a-b22b-4fac-8ae2-17e4bf9881cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE_CONFIG loaded\n"
     ]
    }
   ],
   "source": [
    "EXERCISE_CONFIG = {\n",
    "    'tricep_pushdown': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'elbow_angle', # Use 'elbow_angle' # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder', 'Elbow', 'Wrist'], # Base joint names\n",
    "            'invert_for_valley': False # Peak angle marks end of repetition\n",
    "        },\n",
    "    },\n",
    "    'tricep_dips': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'shoulder_y', # Use Y-coordinate of shoulders # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder'], # Base joint names\n",
    "            'invert_for_valley': True # Lowest point (valley) marks end of repetition\n",
    "        },\n",
    "    },\n",
    "    'bicep_curl': {\n",
    "         'trajectory_logic': {\n",
    "            'metric': 'elbow_angle', # Use 'elbow_angle' # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder', 'Elbow', 'Wrist'],# Base joint names\n",
    "            'invert_for_valley': True # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'squat': {\n",
    "         'trajectory_logic': {\n",
    "             'metric': 'hip_y', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "             'joints': ['Hip'], # Base joint names\n",
    "             'invert_for_valley': True # Lowest point (valley) marks end of repetition\n",
    "         }\n",
    "    },\n",
    "    # 'lunges': {\n",
    "    #    'trajectory_logic': {\n",
    "    #        'metric': 'knee_angle',\n",
    "    #        'joints': ['Hip', 'Knee', 'Ankle'],\n",
    "    #        'invert_for_valley': True\n",
    "    #     },\n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d682a0b-456c-4287-be50-f7dfceeb6644",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell prepares and loads the core pose estimation model (`SimpleHRNet`). It first determines whether to use the GPU (`cuda`) or CPU for computation based on availability. Then, it attempts to load the pre-trained SimpleHRNet model weights from the file specified by `MODEL_PATH` (defined in Cell 2). It intelligently infers the model size parameter (`c=48` or `c=32`) from the weights filename. Crucially, it includes error handling to catch common issues like the weights file being missing or problems during model initialization, and it will stop execution if the model cannot be loaded successfully.\n",
    "\n",
    "**Outcomes:**\n",
    "\n",
    "Running this cell defines and potentially initializes the following:\n",
    "\n",
    "* `DEVICE`: ( `torch.device`)\n",
    "    * **Meaning:** Represents the hardware device (CPU or CUDA-enabled GPU) that the model will run on. Determined automatically based on `torch.cuda.is_available()`.\n",
    "    * Unit = Device identifier.\n",
    "* `HRNET_MODEL`: ( `SimpleHRNet` object or `None`)\n",
    "    * **Meaning:** If loading is successful, this variable holds the initialized SimpleHRNet model object, ready for pose prediction. If loading fails for any reason, it remains `None`.\n",
    "    * Unit = N/A (Python object).\n",
    "\n",
    "**Printed Output:**\n",
    "\n",
    "* Indicates which device (`cpu` or `cuda`) is being used.\n",
    "* Confirms successful model loading or prints specific error messages if loading fails (e.g., \"File not found\", \"SimpleHRNet class not found\", or other exceptions).\n",
    "* If loading fails, prints \"Halting execution...\" and raises a `RuntimeError`.\n",
    "\n",
    "**Calculated Means / Logic:**\n",
    "\n",
    "* **Device Selection:** Uses `torch.cuda.is_available()` to check for a usable NVIDIA GPU with CUDA drivers installed. Prefers GPU ('cuda') for faster processing if available, otherwise falls back to 'cpu'.\n",
    "* **Model Size Inference:** Extracts 'w48' or assumes 'w32' from the `MODEL_PATH` filename to set the `c` parameter (model channel complexity) correctly when initializing `SimpleHRNet`.\n",
    "* **File Existence Check:** Explicitly checks `MODEL_PATH.exists()` before attempting to load, providing a clearer error message if the weights file is missing.\n",
    "* **Model Initialization:** Calls the `SimpleHRNet` constructor with necessary parameters derived from Cell 2 constants (`EXPECTED_JOINT_COUNT`, `MODEL_PATH`) and the determined `DEVICE` and `c` value. Parameters like `multiperson=False` and `max_batch_size=16` configure the prediction behavior.\n",
    "* **Error Handling:** The `try...except` block catches potential errors during loading (missing files, import errors, general exceptions) and provides informative output instead of crashing abruptly. The final `if HRNET_MODEL is None:` check ensures that subsequent cells won't run if the model, which is essential, failed to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "3f059e07-4cb4-4b92-a9dd-1462294c841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "device: 'cpu'\n",
      "SimpleHRNet model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "HRNET_MODEL = None\n",
    "try:\n",
    "    model_c_value = 48\n",
    "    if not MODEL_PATH.exists():\n",
    "         raise FileNotFoundError(f\"Model checkpoint file not found at {MODEL_PATH}\")\n",
    "\n",
    "    HRNET_MODEL = SimpleHRNet(c=model_c_value,\n",
    "                              nof_joints=EXPECTED_JOINT_COUNT,\n",
    "                              checkpoint_path=MODEL_PATH,\n",
    "                              device=DEVICE,\n",
    "                              multiperson=False,\n",
    "                              max_batch_size=16)\n",
    "    print(\"SimpleHRNet model loaded successfully.\")\n",
    "except FileNotFoundError as nofile_error:\n",
    "    print(nofile_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23faa618-044d-4cf2-90ff-64c2479b1f47",
   "metadata": {},
   "source": [
    "### Subfunctions\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell defines several fundamental utility functions that perform common, low-level tasks required by the main processing functions later in the notebook. These helpers handle safe data access, geometric calculations, data cleaning, and sequence standardization.\n",
    "\n",
    "**Functions Defined:**\n",
    "\n",
    "1.  **`get_keypoint(frame_kps, kp_index)`:**\n",
    "    * **Purpose:** To safely retrieve the valid X, Y coordinates for a *single* specified joint (`kp_index`) from the data of a single frame (`frame_kps`, assumed to be reshaped to `(num_joints, 2)`).\n",
    "    * **Outcome:** The coordinates of the requested joint if found and not NaN, otherwise `None`.\n",
    "    *  `numpy.array` (shape (2,)) or `None`.\n",
    "    * Unit = Coordinate values (typically unitless after spatial normalization).\n",
    "    * **Meaning:** Provides a robust way to access joint data for calculations like angles, preventing errors if a joint wasn't detected or is out of bounds.\n",
    "\n",
    "2.  **`calculate_angle(p1, p2, p3)`:**\n",
    "    * **Purpose:** To compute the angle (in degrees) formed at point `p2` by the vectors connecting `p1-p2` and `p3-p2`. Commonly used for joint angles (e.g., elbow angle using shoulder, elbow, wrist points).\n",
    "    * **Outcome:** The calculated angle if all points are valid and form a non-zero angle, otherwise `None`.\n",
    "    *  `float` or `None`.\n",
    "    * Unit = Degrees.\n",
    "    * **Meaning:** Quantifies the flexion or extension between three connected body parts, which is often a key indicator for specific exercise movements. Includes checks for missing points and zero-length vectors.\n",
    "\n",
    "3.  **`handle_nan_values(sequence)`:**\n",
    "    * **Purpose:** To clean a pose sequence by removing entire frames (rows) that contain *any* `NaN` (Not a Number) values. This often occurs if one or more joints were not detected in that frame.\n",
    "    * **Outcome:** A `numpy.array` containing only the frames where all joints were detected. This output array might have fewer rows (frames) than the input `sequence`.\n",
    "    *  `numpy.array`.\n",
    "    * Unit = Same coordinate units as the input sequence.\n",
    "    * **Meaning:** This function provides a simple way to ensure subsequent calculations aren't affected by missing data points. However, it can lead to data loss if joints are frequently missing for short periods.\n",
    "\n",
    "4.  **`time_normalize_sequence(sequence, target_length)`:**\n",
    "    * **Purpose:** To standardize the temporal length of a pose sequence segment (like a single repetition) to a fixed number of steps (`target_length`). It resamples the sequence using linear interpolation.\n",
    "    * **Outcome:** A `numpy.array` with shape (`target_length`, number_of_coordinates), where the movement from the original `sequence` has been stretched or compressed in time.\n",
    "    *  `numpy.array`.\n",
    "    * Unit = Same coordinate units as the input sequence; the \"time\" axis is now normalized to `target_length` steps.\n",
    "    * **Meaning:** Essential for comparing sequences of different original durations using DTW or for averaging multiple repetition segments together to create a baseline. It ensures comparisons focus on movement shape over a standard duration. Includes robust NaN filling using pandas `ffill`/`bfill` as a fallback after interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "621e5c70-0704-441d-a1e3-868f0674be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoint(frame_kps, kp_index):\n",
    "    \"\"\"Safely get keypoint coordinates (x, y) from a frame's keypoint array.\"\"\"\n",
    "    if frame_kps is not None and 0 <= kp_index < frame_kps.shape[0]:\n",
    "        coords = frame_kps[kp_index]\n",
    "        if not np.isnan(coords).any():\n",
    "            return coords\n",
    "    return None\n",
    "\n",
    "def calculate_angle(p1, p2, p3):\n",
    "    \"\"\"Calculates the angle (in degrees) at p2 formed by p1-p2-p3.\"\"\"\n",
    "    if p1 is None or p2 is None or p3 is None: return None\n",
    "    v1, v2 = np.array(p1) - np.array(p2), np.array(p3) - np.array(p2)\n",
    "    mag1, mag2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if mag1 * mag2 == 0 or np.isclose(mag1 * mag2, 0): return None\n",
    "    cos_angle = np.clip(np.dot(v1, v2) / (mag1 * mag2), -1.0, 1.0)\n",
    "    angle = np.degrees(np.arccos(cos_angle))\n",
    "    return angle\n",
    "\n",
    "def handle_nan_values(sequence):\n",
    "    \"\"\"Handles NaN values in a pose sequence (frames, coords). Removes frames containing any NaNs.\"\"\"\n",
    "    if sequence is None or sequence.ndim != 2 or sequence.shape[0] == 0:\n",
    "        return np.empty((0, sequence.shape[1] if sequence is not None and sequence.ndim == 2 else 0))\n",
    "    valid_frames_mask = ~np.isnan(sequence).any(axis=1)\n",
    "    return sequence[valid_frames_mask]\n",
    "\n",
    "def time_normalize_sequence(sequence, target_length):\n",
    "    \"\"\"Resamples a pose sequence (frames, coords) to a target length using linear interpolation.\"\"\"\n",
    "    if sequence is None or sequence.shape[0] < 2:\n",
    "        num_coords = sequence.shape[1] if sequence is not None and sequence.ndim == 2 else EXPECTED_JOINT_COUNT * 2\n",
    "        return np.full((target_length, num_coords), np.nan)\n",
    "\n",
    "    num_frames, num_coords = sequence.shape\n",
    "    original_indices = np.linspace(0, num_frames - 1, num_frames)\n",
    "    target_indices = np.linspace(0, num_frames - 1, target_length)\n",
    "    normalized_sequence = np.zeros((target_length, num_coords))\n",
    "\n",
    "    for j in range(num_coords):\n",
    "        valid_mask = ~np.isnan(sequence[:, j])\n",
    "        if np.sum(valid_mask) < 2:\n",
    "            normalized_sequence[:, j] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                normalized_sequence[:, j] = np.interp(target_indices, original_indices[valid_mask], sequence[valid_mask, j])\n",
    "            except Exception: normalized_sequence[:, j] = np.nan\n",
    "\n",
    "    if np.isnan(normalized_sequence).any():\n",
    "        try:\n",
    "             df = pd.DataFrame(normalized_sequence)\n",
    "             df.ffill(inplace=True)\n",
    "             df.bfill(inplace=True)\n",
    "             normalized_sequence = df.to_numpy()\n",
    "        except NameError:\n",
    "             normalized_sequence = np.nan_to_num(normalized_sequence, nan=0.0)\n",
    "\n",
    "    return np.nan_to_num(normalized_sequence, nan=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf4b36-8fe5-4e0b-a3d9-bccd615b1c66",
   "metadata": {},
   "source": [
    "### Angle Detection & Imports for Core Functions\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell defines the `detect_camera_angle` function, which attempts to automatically estimate the camera's viewing angle relative to the person in the video ('left', 'right', or 'front'). It uses a heuristic (rule-of-thumb) approach based on the average horizontal positions of the shoulders. This cell also includes necessary imports for the functions defined across all subsequent Cell 5 sub-cells.\n",
    "\n",
    "**Functions Defined:**\n",
    "\n",
    "1.  **`detect_camera_angle(pose_sequence, joint_names, min_valid_frames=10, width_threshold_factor=0.35)`:**\n",
    "    * **Purpose:** To estimate the camera angle ('left', 'right', 'front') by analyzing the average horizontal (X-coordinate) positions of the 'Left Shoulder' and 'Right Shoulder' joints over time. It assumes that in a side view, one shoulder will appear consistently further left or right on screen than the other. **Note:** This is a heuristic and may require tuning or a more sophisticated approach for high accuracy.\n",
    "    * **Arguments:**\n",
    "        * `pose_sequence`: ( `numpy.array`, Unit = Normalized coordinates) - The input pose data, expected shape (frames, num_coords). Should ideally be spatially normalized.\n",
    "        * `joint_names`: ( `list` of `str`) - Ordered list of joint names.\n",
    "        * `min_valid_frames`: ( `int`, Unit = Frames) - The minimum number of frames where *both* shoulders must be detected successfully (not NaN) for the angle detection to proceed.\n",
    "        * `width_threshold_factor`: ( `float`, Unit = Unitless ratio) - A factor determining sensitivity. The absolute difference in average shoulder X-positions must exceed this fraction of the average detected shoulder width to be classified as a side view.\n",
    "    * **Outcome:** A string indicating the estimated angle.\n",
    "    *  `str`.\n",
    "    * Unit = Category label ('left', 'right', 'front', 'unknown').\n",
    "    * **Calculated Means / Logic:**\n",
    "        * Identifies frames where both left and right shoulders are reliably detected (not NaN).\n",
    "        * Returns 'unknown' if there are too few such reliable frames (`< min_valid_frames`).\n",
    "        * Calculates the average X-position of each shoulder across the reliable frames (`avg_x_left`, `avg_x_right`).\n",
    "        * Calculates the average horizontal distance between the shoulders (`avg_shoulder_width`) in reliable frames, using this as a dynamic reference scale.\n",
    "        * Calculates the difference (`x_difference = avg_x_right - avg_x_left`).\n",
    "        * Compares the absolute `x_difference` to a threshold (`avg_shoulder_width * width_threshold_factor`).\n",
    "        * If the difference is large and positive (right shoulder significantly further right on screen), returns 'right' (camera view from the right).\n",
    "        * If the difference is large and negative (right shoulder significantly further left on screen), returns 'left' (camera view from the left).\n",
    "        * Otherwise (shoulders relatively aligned horizontally), returns 'front'.\n",
    "        * Returns 'unknown' if necessary joints aren't found or other errors occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5da4b54a-7d05-429c-aa8a-ff0a328aacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_camera_angle(pose_sequence, joint_names, min_valid_frames=10, width_threshold_factor=0.35):\n",
    "    \"\"\"\n",
    "    Analyzes pose sequence for camera angle based on shoulder positions.\n",
    "    Placeholder heuristic - needs validation and likely improvement.\n",
    "    \"\"\"\n",
    "    if pose_sequence is None or pose_sequence.ndim != 2 or pose_sequence.shape[0] < min_valid_frames:\n",
    "        return 'unknown'\n",
    "\n",
    "    try:\n",
    "        lshoulder_idx = joint_names.index('Left Shoulder')\n",
    "        rshoulder_idx = joint_names.index('Right Shoulder')\n",
    "        lshoulder_x_idx, rshoulder_x_idx = 2 * lshoulder_idx, 2 * rshoulder_idx\n",
    "\n",
    "        if max(lshoulder_x_idx, rshoulder_x_idx) >= pose_sequence.shape[1]:\n",
    "             return 'unknown'\n",
    "\n",
    "        valid_mask = ~np.isnan(pose_sequence[:, lshoulder_x_idx]) & \\\n",
    "                     ~np.isnan(pose_sequence[:, rshoulder_x_idx])\n",
    "\n",
    "        if np.sum(valid_mask) < min_valid_frames:\n",
    "            return 'unknown'\n",
    "\n",
    "        valid_lsx = pose_sequence[valid_mask, lshoulder_x_idx]\n",
    "        valid_rsx = pose_sequence[valid_mask, rshoulder_x_idx]\n",
    "\n",
    "        avg_x_left = np.mean(valid_lsx)\n",
    "        avg_x_right = np.mean(valid_rsx)\n",
    "\n",
    "        avg_shoulder_width = np.mean(np.abs(valid_lsx - valid_rsx))\n",
    "\n",
    "        if avg_shoulder_width < 1e-6:\n",
    "             return 'front'\n",
    "\n",
    "        x_difference = avg_x_right - avg_x_left\n",
    "\n",
    "        threshold = avg_shoulder_width * width_threshold_factor\n",
    "\n",
    "        detected_angle = 'front'\n",
    "        if x_difference > threshold:\n",
    "            detected_angle = 'right'\n",
    "        elif x_difference < -threshold:\n",
    "            detected_angle = 'left'\n",
    "\n",
    "        return detected_angle\n",
    "\n",
    "    except (ValueError, IndexError):\n",
    "        return 'unknown'\n",
    "    except Exception as e:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc88ba9-7abf-42bd-98f6-3e98f5ef20d8",
   "metadata": {},
   "source": [
    "### Video Processing en Normalization\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell defines two crucial functions for the initial processing stages: `process_single_video` extracts the raw pose data from a video file using the loaded HRNet model, and `normalize_pose_sequence` performs spatial normalization on the extracted raw poses to make them comparable.\n",
    "\n",
    "**Functions Defined:**\n",
    "\n",
    "1.  **`process_single_video(video_path, model, expected_joint_count)`:**\n",
    "    * **Purpose:** To read a specified video file frame-by-frame, apply the loaded pose estimation `model` (`HRNET_MODEL`) to each frame, and extract the raw joint coordinates. It assumes a single person is the primary subject.\n",
    "    * **Outcome:** Returns a 2D NumPy array where each row represents a frame and columns contain the flattened (x, y) coordinates for all `expected_joint_count` joints (e.g., shape `(num_frames, 34)` for 17 joints). If pose detection fails on a frame, or the wrong number of joints/coords are detected, that frame's row will be filled with `NaN` values. Returns `None` if the video file cannot be opened or no frames are processed.\n",
    "    *  `numpy.array` or `None`.\n",
    "    * Unit = Pixel coordinates (relative to the video frame dimensions).\n",
    "    * **Meaning:** This function translates the visual information in the video into a time series of raw skeletal joint positions. It's the first step in converting the video into analyzable data.\n",
    "\n",
    "2.  **`normalize_pose_sequence(pose_sequence, joint_names, ref_joint1_name=\"Left Shoulder\", ref_joint2_name=\"Right Shoulder\")`:**\n",
    "    * **Purpose:** To perform spatial normalization on the raw pose sequence obtained from `process_single_video`. This aims to make the pose data invariant to the person's location within the video frame and their scale (distance from the camera).\n",
    "    * **Mechanism:** For each frame, it calculates a center point (midpoint between `ref_joint1` and `ref_joint2`, typically Left/Right Shoulder) and a scale factor (distance between `ref_joint1` and `ref_joint2`). It then recalculates all joint coordinates relative to this center point and scales them by the calculated scale factor.\n",
    "    * **Outcome:** Returns a `numpy.array` of the same shape as the input `pose_sequence`, but containing the *normalized* coordinates. If the reference joints cannot be found in a frame or the scale factor is zero, that frame's row will be filled with `NaN` values.\n",
    "    *  `numpy.array`.\n",
    "    * Unit = Unitless normalized coordinates. The values represent positions relative to the reference joints, scaled by the distance between them.\n",
    "    * **Meaning:** This step is crucial for reliable comparison (like DTW). It ensures that the subsequent analysis focuses on the *shape* and *dynamics* of the movement, rather than being affected by whether the person was standing further left/right or closer/further from the camera in different videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "50d41674-f0cd-406b-8689-dcff98dd8672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing and normalization functions\n"
     ]
    }
   ],
   "source": [
    "def process_single_video(video_path, model, expected_joint_count=EXPECTED_JOINT_COUNT):\n",
    "    \"\"\"Processes a single video, extracts joints, returns NumPy array (frames, num_coords).\"\"\"\n",
    "    if not os.path.exists(video_path): print(f\"E: Video not found: {video_path}\"); return None\n",
    "    video = cv2.VideoCapture(video_path);\n",
    "    if not video.isOpened(): print(f\"E: Cannot open video: {video_path}\"); return None\n",
    "    video_joints, frame_count = [], 0; expected_coords = expected_joint_count * 2\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = video.read();\n",
    "            if not ret: break\n",
    "            try:\n",
    "                joints = model.predict(frame)\n",
    "                if joints is None or joints.shape[0]==0 or joints.shape[1]!=expected_joint_count: frame_coords=np.full(expected_coords,np.nan)\n",
    "                else: frame_coords=joints[0,:,:2].flatten(); frame_coords = frame_coords if frame_coords.shape[0]==expected_coords else np.full(expected_coords,np.nan)\n",
    "            except Exception as e: frame_coords = np.full(expected_coords, np.nan)\n",
    "            video_joints.append(frame_coords); frame_count += 1\n",
    "    finally: video.release()\n",
    "    return np.array(video_joints) if video_joints else None\n",
    "\n",
    "def normalize_pose_sequence(pose_sequence, joint_names, ref_joint1_name=\"Left Shoulder\", ref_joint2_name=\"Right Shoulder\"):\n",
    "    \"\"\"Spatially normalizes a pose sequence based on reference joints.\"\"\"\n",
    "    if pose_sequence is None or pose_sequence.shape[0] == 0: return np.array([])\n",
    "    try: ref_joint1_idx, ref_joint2_idx = joint_names.index(ref_joint1_name), joint_names.index(ref_joint2_name)\n",
    "    except ValueError: print(f\"E: Ref joints not found in normalize\"); return np.full_like(pose_sequence, np.nan)\n",
    "    normalized_frames = []; num_coords = pose_sequence.shape[1]\n",
    "    if num_coords == 0 or num_coords % 2 != 0 or num_coords != len(joint_names) * 2:\n",
    "        print(f\"W: Coord mismatch/zero coords in normalize_pose_sequence ({num_coords} vs {len(joint_names)*2}). Returning NaNs.\")\n",
    "        return np.full_like(pose_sequence, np.nan)\n",
    "\n",
    "    for frame in pose_sequence:\n",
    "        try:\n",
    "             if frame.shape[0] != num_coords: normalized_frames.append(np.full(num_coords, np.nan)); continue\n",
    "             frame_joints = frame.reshape(-1, 2);\n",
    "             if frame_joints.shape[0] != len(joint_names): normalized_frames.append(np.full(num_coords, np.nan)); continue\n",
    "             ref1, ref2 = get_keypoint(frame_joints, ref_joint1_idx), get_keypoint(frame_joints, ref_joint2_idx)\n",
    "             if ref1 is None or ref2 is None: normalized_frames.append(np.full(num_coords, np.nan)); continue\n",
    "             center, scale = (np.array(ref1) + np.array(ref2)) / 2.0, np.linalg.norm(np.array(ref1) - np.array(ref2))\n",
    "             if scale == 0 or np.isclose(scale, 0) or np.isnan(scale): normalized_frames.append(np.full(num_coords, np.nan)); continue\n",
    "             normalized_frames.append(((frame_joints - center) / scale).flatten())\n",
    "        except Exception as e: normalized_frames.append(np.full(num_coords, np.nan)); continue\n",
    "    return np.array(normalized_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899eedbc-f5dd-4823-9d27-0d92d1696548",
   "metadata": {},
   "source": [
    "### Trajectory Calculation\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell is to take a pose sequence and, based on the specific configuration defined for an exercise in `EXERCISE_CONFIG`, calculate a 1-dimensional time series (the \"trajectory\") that represents the key repetitive movement of that exercise. This trajectory is the signal that will later be analyzed (smoothed and peak-detected) for segmentation and repetition counting.\n",
    "\n",
    "**Functions Defined:**\n",
    "\n",
    "1.  **`_calculate_trajectory_from_config(pose_sequence, joint_names, trajectory_config)`:**\n",
    "    * **Purpose:** To calculate the 1D trajectory signal according to the rules specified in the `trajectory_config` dictionary (which comes from the main `EXERCISE_CONFIG`). It handles different calculation methods ('metrics') like joint angles or coordinate positions and attempts to use both left and right sides of the body where appropriate.\n",
    "    * **Arguments:**\n",
    "        * `pose_sequence`: ( `numpy.array`, Unit = Normalized coordinates) - The cleaned, spatially normalized pose sequence (frames, coords).\n",
    "        * `joint_names`: ( `list` of `str`) - Ordered list of joint names.\n",
    "        * `trajectory_config`: ( `dict`) - The specific `trajectory_logic` dictionary for the current exercise, extracted from `EXERCISE_CONFIG`. Contains keys like `'metric'`, `'joints'`, `'invert_for_valley'`.\n",
    "    * **Outcome:** Returns a tuple containing:\n",
    "        * `trajectory`: ( `numpy.array` or `None`) - A 1D NumPy array of shape `(num_frames,)` representing the calculated metric over time. Contains `NaN` values filled via interpolation and mean/zero-filling. Returns `None` if calculation fails.\n",
    "        * `invert_for_valley`: ( `bool`) - The boolean flag directly passed through from the input `trajectory_config`, indicating if the peak finding logic should invert this trajectory to find valleys.\n",
    "    * Unit = Depends on the `'metric'` defined in the config (e.g., degrees for `_angle` metrics, normalized units for `_y` or `_x` metrics).\n",
    "    * **Calculated Means / Logic:**\n",
    "        * **Configuration Parsing:** Reads the `'metric'`, `'joints'`, and `'invert_for_valley'` keys from the input `trajectory_config`.\n",
    "        * **Metric-Based Calculation:**\n",
    "            * **`_angle` metrics:** Expects 3 base joint names. It attempts to find corresponding Left and Right joints (e.g., 'Left Shoulder', 'Right Shoulder'). It calculates the angle for each side frame-by-frame using `calculate_angle` (handling missing points). The final trajectory is the average of the left and right angles for each frame (using `np.nanmean`, which intelligently handles frames where only one side is valid). Includes a fallback to calculate using non-paired joints if L/R versions aren't found.\n",
    "            * **`_y` / `_x` metrics:** Expects 1 base joint name. It attempts to find the Left and Right corresponding joints and extracts the specified coordinate (Y or X). The final trajectory is the average of the left and right coordinates for each frame (using `np.nanmean`). Includes a fallback to use a single non-paired joint if L/R versions aren't found.\n",
    "        * **Post-Processing:** After calculating the raw trajectory based on the metric, it performs robust NaN handling: first interpolating missing values, then filling any remaining NaNs (e.g., at the ends) with the mean or zero. This ensures the peak finding function receives a clean, complete signal.\n",
    "        * **Error Handling:** Includes checks for missing config keys, incorrect number of joints for a metric, and errors during joint index lookups or calculations, returning `None, False` upon failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2a950803-5f12-464a-9fcf-543c676cdc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory calculation function loaded\n"
     ]
    }
   ],
   "source": [
    "def _calculate_trajectory_from_config(pose_sequence, joint_names, trajectory_config):\n",
    "    \"\"\"Calculates the 1D trajectory signal based on configuration.\"\"\"\n",
    "    if pose_sequence is None or pose_sequence.ndim!= 2 or pose_sequence.shape[0] < 2: return None, False\n",
    "\n",
    "    metric = trajectory_config.get('metric', 'unknown')\n",
    "    base_joint_names = trajectory_config.get('joints', [])\n",
    "    invert_for_valley = trajectory_config.get('invert_for_valley', False)\n",
    "    trajectory = None\n",
    "\n",
    "    try:\n",
    "        if not base_joint_names: raise ValueError(\"No 'joints' specified in trajectory_config\")\n",
    "\n",
    "        if metric.endswith('_angle'):\n",
    "            if len(base_joint_names) != 3: raise ValueError(f\"{metric} needs 3 joints in config\")\n",
    "            j1_base, j2_base, j3_base = base_joint_names[0], base_joint_names[1], base_joint_names[2]\n",
    "            angles_left, angles_right = [], []\n",
    "            try: \n",
    "                l_j1_idx = joint_names.index(f'Left {j1_base}')\n",
    "                l_j2_idx = joint_names.index(f'Left {j2_base}')\n",
    "                l_j3_idx = joint_names.index(f'Left {j3_base}')\n",
    "                r_j1_idx = joint_names.index(f'Right {j1_base}')\n",
    "                r_j2_idx = joint_names.index(f'Right {j2_base}')\n",
    "                r_j3_idx = joint_names.index(f'Right {j3_base}')\n",
    "                indices_valid = True\n",
    "            except (ValueError, IndexError): indices_valid = False # L/R pair doesn't exist\n",
    "\n",
    "            if indices_valid:\n",
    "                for frame_coords in pose_sequence:\n",
    "                    f_j = frame_coords.reshape(-1, 2)\n",
    "                    p1l,p2l,p3l = get_keypoint(f_j,l_j1_idx), get_keypoint(f_j,l_j2_idx), get_keypoint(f_j,l_j3_idx)\n",
    "                    p1r,p2r,p3r = get_keypoint(f_j,r_j1_idx), get_keypoint(f_j,r_j2_idx), get_keypoint(f_j,r_j3_idx)\n",
    "                    angles_left.append(calculate_angle(p1l, p2l, p3l) if all(p is not None for p in [p1l, p2l, p3l]) else np.nan)\n",
    "                    angles_right.append(calculate_angle(p1r, p2r, p3r) if all(p is not None for p in [p1r, p2r, p3r]) else np.nan)\n",
    "                traj_l, traj_r = np.array(angles_left), np.array(angles_right)\n",
    "                trajectory = np.nanmean([traj_l, traj_r], axis=0) \n",
    "            else: \n",
    "                 j1_idx, j2_idx, j3_idx = joint_names.index(j1_base), joint_names.index(j2_base), joint_names.index(j3_base)\n",
    "                 angles = []\n",
    "                 for frame_coords in pose_sequence:\n",
    "                      f_j = frame_coords.reshape(-1, 2)\n",
    "                      p1, p2, p3 = get_keypoint(f_j, j1_idx), get_keypoint(f_j, j2_idx), get_keypoint(f_j, j3_idx)\n",
    "                      angles.append(calculate_angle(p1, p2, p3) if all(p is not None for p in [p1, p2, p3]) else np.nan)\n",
    "                 trajectory = np.array(angles)\n",
    "\n",
    "        elif metric.endswith('_y') or metric.endswith('_x'):\n",
    "            coord_idx = 1 if metric.endswith('_y') else 0 \n",
    "            base_joint = base_joint_names[0]\n",
    "            try: \n",
    "                 l_joint_idx = joint_names.index(f'Left {base_joint}')\n",
    "                 r_joint_idx = joint_names.index(f'Right {base_joint}')\n",
    "                 if max(2*l_joint_idx+coord_idx, 2*r_joint_idx+coord_idx) >= pose_sequence.shape[1]: raise ValueError(\"Joint index out of bounds\")\n",
    "                 l_coord = pose_sequence[:, 2 * l_joint_idx + coord_idx]\n",
    "                 r_coord = pose_sequence[:, 2 * r_joint_idx + coord_idx]\n",
    "                 trajectory = np.nanmean([l_coord, r_coord], axis=0)\n",
    "            except (ValueError, IndexError):\n",
    "                 joint_idx = joint_names.index(base_joint)\n",
    "                 if 2*joint_idx+coord_idx >= pose_sequence.shape[1]: raise ValueError(\"Joint index out of bounds\")\n",
    "                 trajectory = pose_sequence[:, 2 * joint_idx + coord_idx]\n",
    "        else:\n",
    "             raise ValueError(f\"Unknown metric type specified in config: '{metric}'\")\n",
    "\n",
    "        if trajectory is None or trajectory.shape[0] == 0 or np.all(np.isnan(trajectory)): return None, False\n",
    "        nan_mask = np.isnan(trajectory)\n",
    "        if np.any(nan_mask):\n",
    "            indices = np.arange(len(trajectory)); valid_indices = np.flatnonzero(~nan_mask)\n",
    "            if len(valid_indices) < 2: return None, False\n",
    "            trajectory[nan_mask] = np.interp(indices[nan_mask], valid_indices, trajectory[valid_indices])\n",
    "        if np.any(np.isnan(trajectory)):\n",
    "             trajectory = np.nan_to_num(trajectory, nan=np.nanmean(trajectory))\n",
    "             if np.any(np.isnan(trajectory)): trajectory = np.nan_to_num(trajectory, nan=0.0)\n",
    "\n",
    "\n",
    "        return trajectory, invert_for_valley\n",
    "\n",
    "    except (ValueError, IndexError) as e: print(f\"W: Error finding/using joints for metric '{metric}': {e}\"); return None, False\n",
    "    except Exception as e: print(f\"E: Error calculating trajectory with metric '{metric}': {e}\"); return None, False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41085b6e-da27-401a-a003-c4208016dfd1",
   "metadata": {},
   "source": [
    "### Peak Finding Helper Function\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell is to take a 1D numerical trajectory (representing movement over time, like an angle or position) and identify the locations (frame indices) of significant peaks within that signal. By using the `invert_trajectory` flag, it can also be used to find significant valleys (minima). This peak/valley detection is used for identifying individual repetition boundaries in the functions `segment_repetitions` and `count_repetitions`.\n",
    "\n",
    "**Functions Defined:**\n",
    "\n",
    "1.  **`_extract_trajectory_and_find_peaks(trajectory, smoothing_window, prominence, distance, invert_trajectory=False)`:**\n",
    "    * **Purpose:** To smooth an input trajectory signal and find the indices of peaks that meet specific prominence and distance criteria.\n",
    "    * **Arguments:**\n",
    "        * `trajectory`: ( `numpy.array`, Unit = Varies - e.g., degrees, normalized units) - The 1D input signal calculated by `_calculate_trajectory_from_config`.\n",
    "        * `smoothing_window`: ( `int`, Unit = Frames) - The number of frames to use for the moving average filter applied before peak detection.\n",
    "        * `prominence`: ( `float`, Unit = Unitless ratio) - Relative prominence factor. A peak must stand out from its surrounding troughs by at least this fraction of the smoothed signal's overall range.\n",
    "        * `distance`: ( `int`, Unit = Frames) - The minimum number of frames required between consecutive detected peaks.\n",
    "        * `invert_trajectory`: ( `bool`) - If `True`, the trajectory is inverted (`-trajectory`) before smoothing and peak finding. This effectively finds significant valleys (local minima) in the original signal.\n",
    "    * **Outcome:** A 1D NumPy array containing the integer frame indices where valid peaks were detected in the *original* (un-smoothed) trajectory's timeline, or `None` if the input is invalid, smoothing/peak finding fails, or no peaks are found.\n",
    "    *  `numpy.array` (int) or `None`.\n",
    "    * Unit = Frame index (unitless count).\n",
    "    * **Calculated Means / Logic:**\n",
    "        * **Input Validation:** Checks if the input `trajectory` is valid and long enough for the specified smoothing and distance parameters.\n",
    "        * **Inversion:** Optionally inverts the signal using `processed_trajectory = -trajectory` if `invert_trajectory` is `True`, allowing the same peak-finding logic to find valleys.\n",
    "        * **Smoothing:** Applies a moving average filter (`np.convolve` with `mode='valid'`) of size `smoothing_window` to reduce noise and make peak detection more stable.\n",
    "        * **Dynamic Prominence:** Calculates the required absolute `prominence` for `find_peaks` based on the input `prominence` factor (relative) and the peak-to-peak `data_range` of the *smoothed* signal. Ensures a minimum small prominence even for flat signals.\n",
    "        * **Peak Detection:** Uses `scipy.signal.find_peaks` on the smoothed signal, filtering peaks based on the calculated `required_prominence` and the minimum horizontal `distance` between them.\n",
    "        * **Index Correction:** Adjusts the indices found in the *smoothed* signal back to correspond to the indices in the *original* input `trajectory` by accounting for the offset introduced by the 'valid' convolution mode.\n",
    "        * **Bounds Checking:** Ensures returned indices are valid within the length of the original trajectory.\n",
    "        * **Error Handling:** Returns `None` if errors occur during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7acf015d-9c4d-4fae-9b1e-5f31bed2a119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak finding function defined\n"
     ]
    }
   ],
   "source": [
    "def _extract_trajectory_and_find_peaks(trajectory, smoothing_window, prominence, distance, invert_trajectory=False):\n",
    "    \"\"\"Internal helper to smooth a trajectory and find peaks.\"\"\"\n",
    "    if trajectory is None or trajectory.ndim != 1 or trajectory.shape[0] < max(distance * 2, smoothing_window, 1): return None\n",
    "\n",
    "    processed_trajectory = trajectory.copy()\n",
    "    if invert_trajectory: processed_trajectory = -processed_trajectory\n",
    "    if len(processed_trajectory) < smoothing_window: return None\n",
    "\n",
    "    smoothed_trajectory = np.convolve(processed_trajectory, np.ones(smoothing_window)/smoothing_window, mode='valid')\n",
    "    if smoothed_trajectory.shape[0] < max(distance, 1): return None\n",
    "\n",
    "    data_range = np.ptp(smoothed_trajectory)\n",
    "    required_prominence = max(data_range * prominence, 1e-6) if data_range > 1e-9 else 1e-6\n",
    "\n",
    "    try:\n",
    "        smoothed_indices_offset = (len(processed_trajectory) - len(smoothed_trajectory)) // 2\n",
    "        peak_indices_smoothed, _ = find_peaks(smoothed_trajectory, prominence=required_prominence, distance=distance)\n",
    "        peak_indices_original = peak_indices_smoothed + smoothed_indices_offset\n",
    "        peak_indices_original = peak_indices_original[(peak_indices_original >= 0) & (peak_indices_original < len(processed_trajectory))]\n",
    "        return peak_indices_original\n",
    "    except Exception as e: print(f\"W: Peak finding failed: {e}\"); return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83c81f-6351-4d6b-b9e9-2e14acae48db",
   "metadata": {},
   "source": [
    "### Segmentation and Counting\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell is analyzing the movement trajectory to identify repetitions: `segment_repetitions` breaks a sequence into individual repetition clips, and `count_repetitions` estimates the total number of repetitions performed.\n",
    "\n",
    "**Functions Defined:**\n",
    "\n",
    "1.  **`segment_repetitions(pose_sequence, exercise_label, joint_names, exercise_config, smoothing_window=DEFAULT_SMOOTHING_WINDOW, peak_prominence=DEFAULT_PEAK_PROMINENCE, peak_distance=DEFAULT_PEAK_DISTANCE)`:**\n",
    "    * **Purpose:** To divide a given `pose_sequence` (assumed to contain multiple repetitions of the specified `exercise_label`) into a list of smaller sequences, where each smaller sequence corresponds to a single detected repetition.\n",
    "    * **Arguments:**\n",
    "        * `pose_sequence`: ( `numpy.array`, Unit = Normalized coordinates) - The input sequence to segment.\n",
    "        * `exercise_label`: ( `str`) - The name of the exercise, used to look up the correct processing logic in `exercise_config`.\n",
    "        * `joint_names`: ( `list` of `str`) - Ordered list of joint names.\n",
    "        * `exercise_config`: ( `dict`) - The main configuration dictionary containing trajectory logic for all exercises.\n",
    "        * `smoothing_window`, `peak_prominence`, `peak_distance`: ( `int`/`float`, Unit = Frames/Unitless ratio) - Parameters controlling the peak detection process (currently uses defaults defined in Cell 2, but could be overridden).\n",
    "    * **Outcome:** A list, where each element is a `numpy.array` representing the pose data for a single detected repetition segment (shape `(segment_frames, num_coords)`). Returns an empty list if no valid repetitions (at least 2 peaks) are found or if the config is missing.\n",
    "    *  `list` of `numpy.array` (float).\n",
    "    * Unit = Each array contains normalized coordinates.\n",
    "    * **Meaning:** This function isolates the individual motion cycles (reps) within a longer performance. This is crucial for creating the averaged baselines (where each element being averaged should be one rep) and for the per-repetition analysis during recognition. It uses the default peak finding parameters to consistently extract segments for baseline creation.\n",
    "\n",
    "2.  **`count_repetitions(pose_sequence, exercise_label, joint_names, exercise_config, prominence=DEFAULT_PEAK_PROMINENCE, distance=DEFAULT_PEAK_DISTANCE, smoothing_window=DEFAULT_SMOOTHING_WINDOW)`:**\n",
    "    * **Purpose:** To estimate the total number of repetitions within the input `pose_sequence` for the given `exercise_label`. It allows passing specific `prominence` and `distance` parameters, typically the *tuned* values found during baseline loading.\n",
    "    * **Arguments:**\n",
    "        * `pose_sequence`: ( `numpy.array`, Unit = Normalized coordinates) - The input sequence to analyze.\n",
    "        * `exercise_label`: ( `str`) - The name of the exercise.\n",
    "        * `joint_names`: ( `list` of `str`) - Ordered list of joint names.\n",
    "        * `exercise_config`: ( `dict`) - The main configuration dictionary.\n",
    "        * `prominence`, `distance`, `smoothing_window`: ( `float`/`int`, Unit = Unitless ratio/Frames) - Peak detection parameters. Crucially, *tuned* values for `prominence` and `distance` should ideally be passed here when counting reps for a recognized exercise.\n",
    "    * **Outcome:** The estimated number of repetitions detected in the sequence. Returns 0 if the config is missing or no peaks are found.\n",
    "    *  `int`.\n",
    "    * Unit = Count (unitless).\n",
    "    * **Meaning:** Provides the final repetition count estimate. Its accuracy depends heavily on the quality of the pose data, the appropriateness of the trajectory logic defined in `EXERCISE_CONFIG`, and the effectiveness of the (potentially tuned) `prominence` and `distance` parameters used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "737a1455-c2c8-49c6-93b7-b664c3ea6d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation and counting functions defined \n"
     ]
    }
   ],
   "source": [
    "def segment_repetitions(pose_sequence, exercise_label, joint_names, exercise_config,\n",
    "                        smoothing_window=DEFAULT_SMOOTHING_WINDOW,\n",
    "                        peak_prominence=DEFAULT_PEAK_PROMINENCE,\n",
    "                        peak_distance=DEFAULT_PEAK_DISTANCE):\n",
    "    \"\"\"Segments a pose sequence into individual repetitions using EXERCISE_CONFIG.\"\"\"\n",
    "    current_exercise_cfg_dict = exercise_config.get(exercise_label, {})\n",
    "    trajectory_logic_cfg = current_exercise_cfg_dict.get('trajectory_logic')\n",
    "    if not trajectory_logic_cfg:\n",
    "        return []\n",
    "\n",
    "    trajectory, invert = _calculate_trajectory_from_config(pose_sequence, joint_names, trajectory_logic_cfg)\n",
    "    peak_indices = _extract_trajectory_and_find_peaks(trajectory, smoothing_window, peak_prominence, peak_distance, invert_trajectory=invert)\n",
    "\n",
    "    if peak_indices is None or len(peak_indices) < 2: return []\n",
    "    repetitions = [pose_sequence[peak_indices[i]:peak_indices[i+1], :] for i in range(len(peak_indices) - 1)]\n",
    "    return [rep for rep in repetitions if rep.shape[0] > 1]\n",
    "\n",
    "def count_repetitions(pose_sequence, exercise_label, joint_names, exercise_config,\n",
    "                      prominence=DEFAULT_PEAK_PROMINENCE, distance=DEFAULT_PEAK_DISTANCE,\n",
    "                      smoothing_window=DEFAULT_SMOOTHING_WINDOW):\n",
    "    \"\"\"Counts repetitions using EXERCISE_CONFIG and specified parameters.\"\"\"\n",
    "    current_exercise_cfg_dict = exercise_config.get(exercise_label)\n",
    "    if not current_exercise_cfg_dict: return 0\n",
    "    trajectory_logic_cfg = current_exercise_cfg_dict.get('trajectory_logic')\n",
    "    if not trajectory_logic_cfg: return 0\n",
    "\n",
    "    trajectory, invert = _calculate_trajectory_from_config(pose_sequence, joint_names, trajectory_logic_cfg)\n",
    "    peak_indices = _extract_trajectory_and_find_peaks(trajectory, smoothing_window, prominence, distance, invert_trajectory=invert)\n",
    "\n",
    "    return len(peak_indices) if peak_indices is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6048d-cf9c-47ef-88c9-7ba12dca2b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a50c6ace-9be4-46d9-961e-6117a780a5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter tuning function defined\n"
     ]
    }
   ],
   "source": [
    "def tune_counting_parameters(exercise_label, labeled_video_data, joint_names, exercise_config,\n",
    "                             prominence_range, distance_range):\n",
    "    \"\"\"Performs grid search using the refactored peak finding and config.\"\"\"\n",
    "    print(f\"  Tuning repetition counting parameters for: {exercise_label}...\")\n",
    "    best_params = (DEFAULT_PEAK_PROMINENCE, DEFAULT_PEAK_DISTANCE) # Start with defaults\n",
    "    min_total_error = float('inf')\n",
    "    if not labeled_video_data: print(\"W: No labeled data provided for tuning.\"); return best_params\n",
    "\n",
    "    current_exercise_cfg_dict = exercise_config.get(exercise_label)\n",
    "    if not current_exercise_cfg_dict: print(f\"W: No config for {exercise_label} in tuning\"); return best_params\n",
    "    trajectory_logic_cfg = current_exercise_cfg_dict.get('trajectory_logic')\n",
    "    if not trajectory_logic_cfg: print(f\"W: No trajectory_logic in config for {exercise_label}\"); return best_params\n",
    "\n",
    "    num_combinations = len(prominence_range) * len(distance_range)\n",
    "    default_error = 0\n",
    "    precalculated_trajectories = {}\n",
    "\n",
    "    for i, (vid_idx, (sequence, true_reps)) in enumerate(labeled_video_data): \n",
    "        if sequence is None or sequence.ndim != 2 or sequence.shape[0] < 2: continue\n",
    "        trajectory, invert = _calculate_trajectory_from_config(sequence, joint_names, trajectory_logic_cfg)\n",
    "        if trajectory is not None: precalculated_trajectories[vid_idx] = (trajectory, invert, true_reps)\n",
    "\n",
    "    if not precalculated_trajectories: print(\"W: No valid trajectories calculated for tuning.\"); return best_params\n",
    "\n",
    "    for vid_idx, (traj, inv, true_r) in precalculated_trajectories.items():\n",
    "         peak_indices_def = _extract_trajectory_and_find_peaks(traj, DEFAULT_SMOOTHING_WINDOW, DEFAULT_PEAK_PROMINENCE, DEFAULT_PEAK_DISTANCE, invert_trajectory=inv)\n",
    "         calc_reps_def = len(peak_indices_def) if peak_indices_def is not None else 0\n",
    "         default_error += abs(calc_reps_def - true_r)\n",
    "\n",
    "    # Grid search\n",
    "    for p_idx, p in enumerate(prominence_range):\n",
    "        for d_idx, d in enumerate(distance_range):\n",
    "            current_total_error = 0\n",
    "            for vid_idx, (traj, inv, true_r) in precalculated_trajectories.items():\n",
    "                peak_indices = _extract_trajectory_and_find_peaks(traj, DEFAULT_SMOOTHING_WINDOW, p, d, invert_trajectory=inv)\n",
    "                calculated_reps = len(peak_indices) if peak_indices is not None else 0\n",
    "                current_total_error += abs(calculated_reps - true_r)\n",
    "\n",
    "            if current_total_error < min_total_error:\n",
    "                min_total_error = current_total_error\n",
    "                best_params = (p, d)\n",
    "            if min_total_error == 0: break\n",
    "        if min_total_error == 0: break\n",
    "\n",
    "    print(f\"Tuning complete. Best Params: P={best_params[0]:.3f}, D={best_params[1]}, Min Error={min_total_error}. (Default Error={default_error})\")\n",
    "\n",
    "    if precalculated_trajectories:\n",
    "        print(f\"Detected reps using BEST params (P={best_params[0]:.3f}, D={best_params[1]}):\")\n",
    "        total_calculated, total_true = 0, 0\n",
    "\n",
    "        for vid_idx, (traj, inv, true_r) in sorted(precalculated_trajectories.items()):\n",
    "            peak_indices = _extract_trajectory_and_find_peaks(traj, DEFAULT_SMOOTHING_WINDOW, best_params[0], best_params[1], invert_trajectory=inv)\n",
    "            calculated_reps = len(peak_indices) if peak_indices is not None else 0\n",
    "            print(f\"Video Index {vid_idx}: Detected={calculated_reps}, True={true_r}\")\n",
    "            total_calculated += calculated_reps; total_true += true_r\n",
    "        print(f\"Total Reps: Detected={total_calculated}, True={total_true}, Overall Error={abs(total_calculated - total_true)}\")\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227bafb-c16c-4528-9652-469cd4b23994",
   "metadata": {},
   "source": [
    "### Parameter Tuning Function\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell is to automatically find the optimal `peak_prominence` and `peak_distance` values for the repetition counting logic for a specific exercise. It does this by systematically trying out different parameter combinations on the baseline videos for that exercise (where the true rep count is known from the filename) and selecting the parameter pair that minimizes the total counting error.\n",
    "\n",
    "**Functions Defined:**\n",
    "\n",
    "1.  **`tune_counting_parameters(exercise_label, labeled_video_data, joint_names, exercise_config, prominence_range, distance_range)`:**\n",
    "    * **Purpose:** To perform a grid search over `prominence_range` and `distance_range` to find the combination of `peak_prominence` and `peak_distance` that most accurately counts repetitions across the provided `labeled_video_data` for the given `exercise_label`, using the trajectory logic defined in `exercise_config`.\n",
    "    * **Arguments:**\n",
    "        * `exercise_label`: ( `str`) - The name of the exercise being tuned.\n",
    "        * `labeled_video_data`: ( `list` of `tuple`) - Data for tuning. Each element is `(video_index, (cleaned_pose_sequence, true_reps))`. The `cleaned_pose_sequence` is the spatially normalized sequence *before* time normalization. `true_reps` is the ground truth count parsed from the filename.\n",
    "        * `joint_names`: ( `list` of `str`) - Ordered list of joint names.\n",
    "        * `exercise_config`: ( `dict`) - The main configuration dictionary.\n",
    "        * `prominence_range`: ( `list` of `float`) - List of `peak_prominence` values to test.\n",
    "        * `distance_range`: ( `list` of `int`) - List of `peak_distance` values to test.\n",
    "    * **Outcome:** Returns a tuple containing the best `(prominence, distance)` parameters found for this exercise. Also prints detailed logs about the tuning process, including the best parameters, the minimum error achieved, the error using default parameters, and a comparison of detected vs. true reps for each video using the best parameters.\n",
    "    *  `tuple` containing (`float`, `int`).\n",
    "    * Unit = Prominence is a unitless ratio, Distance is in frames.\n",
    "    * **Calculated Means / Logic:**\n",
    "        * **Pre-calculation:** It first calculates the movement trajectory for each input video sequence using `_calculate_trajectory_from_config` based on the `exercise_config`. This avoids recalculating it repeatedly inside the loops.\n",
    "        * **Error Metric:** The \"error\" for a given parameter set is defined as the **sum of absolute differences** between the number of peaks detected (`calculated_reps`) and the ground truth (`true_reps`) across all the `labeled_video_data` provided for that exercise.\n",
    "        * **Grid Search:** It systematically iterates through every possible pair of (`prominence`, `distance`) values taken from the input ranges. For each pair, it calculates the total error.\n",
    "        * **`Min Error`:** The lowest total error value encountered during the grid search. A value of 0 indicates a parameter set was found that perfectly matched the true counts on all provided labeled videos.\n",
    "        * **`Best Params`:** The (`prominence`, `distance`) tuple that resulted in the `Min Error`.\n",
    "        * **`Default Error`:** The total error calculated using the globally defined `DEFAULT_PEAK_PROMINENCE` and `DEFAULT_PEAK_DISTANCE` values. This provides a baseline to judge if the tuning was beneficial.\n",
    "        * **Output Breakdown:** The final printed list (`Detected reps using BEST params...`) shows the performance of the chosen `Best Params` on each individual labeled baseline video, comparing the detected count to the true count provided in the filename."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989fb71e-d093-4be5-90a6-fc017222b166",
   "metadata": {},
   "source": [
    "### Loading baseline videos\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell processing all baseline video files. It iterates through each exercise folder provided in `BASELINE_ROOT_DIR`.\n",
    "\n",
    "1.  **Parses Information:** Extracts the ground truth repetition count (`XX`) and camera angle (`'left'`, `'right'`, or `'front'`) from each baseline video's filename (using the format `exercise_label_XXreps_angle_description.mp4`). Videos without a recognized angle are marked as `'unknown'`.\n",
    "2.  **Processes Videos:** Extracts and spatially normalizes pose sequences for each video.\n",
    "3.  **Segments Repetitions:** Breaks down each video's cleaned sequence into individual repetition segments using default parameters (leveraging the exercise-specific trajectory logic from `EXERCISE_CONFIG`).\n",
    "4.  **Groups by Angle:** Collects the time-normalized repetition segments and the labeled data (sequence + true reps) into groups based on the `parsed_angle` for each exercise.\n",
    "5.  **Tunes Parameters per Angle:** For each angle category ('left', 'right', 'front', 'unknown') that has sufficient labeled data, it calls `tune_counting_parameters` to find the optimal peak detection settings.\n",
    "6.  **Averages Baselines per Angle:** For each angle category with data, it calculates and stores the average time-normalized repetition sequence.\n",
    "7.  **Derives Thresholds per Angle:** For each angle category, it calculates and stores a suggested DTW recognition threshold based on the variation between individual reps and the angle-specific average.\n",
    "8.  **Calculates Overall Baseline & Threshold:** After processing all angles, it averages *all* valid repetitions for an exercise to create an 'overall' baseline and calculates an 'overall' derived threshold.\n",
    "\n",
    "**Functions Defined:**\n",
    "\n",
    "1.  **`load_baseline_data(baseline_root_dir, hrnet_model, joint_names, target_rep_length, exercise_config, prominence_range, distance_range, threshold_std_multiplier=DEFAULT_THRESHOLD_STD_MULTIPLIER, min_reps_for_threshold=DEFAULT_MIN_REPS_FOR_THRESHOLD)`:**\n",
    "    * **Purpose:** To process all baseline videos, organize data by exercise and parsed filename angle, tune counting parameters per angle, create averaged baselines per angle and overall, and derive recognition thresholds per angle and overall.\n",
    "    * **Arguments:** All necessary inputs including paths, the HRNet model, configuration dictionaries, tuning ranges, and parameters for threshold calculation.\n",
    "    * **Outcome:** Returns a tuple containing three nested dictionaries:\n",
    "        1.  `averaged_baseline_data`: ( `dict`) Structure: `{exercise_label: {angle_key: numpy.array}}`. Contains the averaged time-normalized pose sequence for each exercise under keys for detected angles ('left', 'right', 'front', 'unknown') and 'overall'. The value might be `None` if averaging failed (e.g., no reps).\n",
    "        2.  `tuned_counting_params`: ( `dict`) Structure: `{exercise_label: {angle_key: (prominence, distance)}}`. Contains the best `(prominence, distance)` tuple found for counting reps for each exercise/angle combination based on labeled data. Uses defaults if tuning fails or no labeled data exists for an angle.\n",
    "        3.  `exercise_specific_thresholds`: ( `dict`) Structure: `{exercise_label: {angle_key: float or None}}`. Contains the derived DTW threshold calculated for each exercise/angle combination based on intra-exercise repetition variance. Value is `None` if calculation failed (e.g., too few reps).\n",
    "    * Unit = See types above. Arrays contain normalized coordinates, parameters are floats/ints (unitless or frames), thresholds are floats (unitless distance).\n",
    "    * **Calculated Means / Logic:** This function orchestrates the entire baseline creation pipeline. Key calculations include:\n",
    "        * **Filename Parsing:** Uses regular expressions (`re.search`) to extract rep counts and angle labels.\n",
    "        * **Data Aggregation:** Groups processed data (`labeled_data_by_angle`, `all_reps_by_angle`) based on the parsed angle.\n",
    "        * **Per-Angle Tuning/Averaging/Thresholding:** Executes the `tune_counting_parameters` function, calculates `np.nanmean` for averaging, and calculates DTW distances + statistics (mean, std dev) to derive thresholds *independently* for each angle category ('left', 'right', 'front', 'unknown') within each exercise.\n",
    "        * **Overall Averaging/Thresholding:** Performs a final average and threshold calculation using *all* valid repetitions collected across *all* angles for an exercise, storing results under the 'overall' key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "6547ecfa-5d8b-4ab7-b7ef-79ef916d4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_baseline_data(baseline_root_dir, hrnet_model, joint_names, target_rep_length, exercise_config, # Pass config\n",
    "                       prominence_range, distance_range, # Pass tuning ranges\n",
    "                       threshold_std_multiplier=DEFAULT_THRESHOLD_STD_MULTIPLIER,\n",
    "                       min_reps_for_threshold=DEFAULT_MIN_REPS_FOR_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Loads baselines, uses EXERCISE_CONFIG, parses angle from filename,\n",
    "    tunes params per angle, averages per angle AND overall, calculates thresholds.\n",
    "    Returns nested dicts: averaged_baselines, tuned_counting_params, exercise_specific_thresholds\n",
    "    \"\"\"\n",
    "    averaged_baseline_data = {}\n",
    "    tuned_counting_params = {}\n",
    "    exercise_specific_thresholds = {}\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    print(f\"Loading Baselines (Config-Driven, Angle from Filename)... from: {baseline_root_dir}\")\n",
    "    if not os.path.isdir(baseline_root_dir):\n",
    "        print(f\"E: Baseline directory not found: {baseline_root_dir}\")\n",
    "        return averaged_baseline_data, tuned_counting_params, exercise_specific_thresholds\n",
    "\n",
    "    for exercise_label_raw in sorted(os.listdir(baseline_root_dir)):\n",
    "        exercise_folder_path = os.path.join(baseline_root_dir, exercise_label_raw)\n",
    "        if not os.path.isdir(exercise_folder_path): continue\n",
    "        exercise_label = exercise_label_raw.lower().replace(\" \", \"_\")\n",
    "        print(f\"\\nProcessing baseline exercise: {exercise_label} (from folder: {exercise_label_raw})\")\n",
    "\n",
    "        current_exercise_config_dict = exercise_config.get(exercise_label)\n",
    "        if not current_exercise_config_dict:\n",
    "             print(f\"  W: No config found for '{exercise_label}'. Skipping.\"); continue\n",
    "\n",
    "        all_reps_by_angle = {'left': [], 'right': [], 'front': [], 'unknown': []}\n",
    "        labeled_data_by_angle = {'left': [], 'right': [], 'front': [], 'unknown': []} # Stores (vid_idx, (sequence, true_reps))\n",
    "        averaged_baseline_data[exercise_label] = {}\n",
    "        tuned_counting_params[exercise_label] = {}\n",
    "        exercise_specific_thresholds[exercise_label] = {}\n",
    "\n",
    "        video_files = sorted([f for f in os.listdir(exercise_folder_path) if f.lower().endswith(('.mp4', '.avi', '.mov'))])\n",
    "        print(f\"  Found {len(video_files)} video file(s).\")\n",
    "\n",
    "        for video_idx, video_file in enumerate(video_files):\n",
    "            video_path = os.path.join(exercise_folder_path, video_file)\n",
    "            true_reps, parsed_angle = None, 'unknown'\n",
    "            rep_match = re.search(r'_(\\d+)reps', video_file, re.IGNORECASE)\n",
    "            if rep_match: true_reps = int(rep_match.group(1))\n",
    "            angle_match = re.search(r'_(left|right|front)', video_file, re.IGNORECASE)\n",
    "            if angle_match: parsed_angle = angle_match.group(1).lower()\n",
    "\n",
    "            raw_seq = process_single_video(video_path, hrnet_model)\n",
    "            if raw_seq is None or raw_seq.shape[0] == 0: continue\n",
    "            spatially_norm_seq = normalize_pose_sequence(raw_seq, joint_names)\n",
    "            if spatially_norm_seq is None or np.all(np.isnan(spatially_norm_seq)): continue\n",
    "            cleaned_seq = handle_nan_values(spatially_norm_seq)\n",
    "            if cleaned_seq.shape[0] < 2: continue\n",
    "\n",
    "            if true_reps is not None:\n",
    "                 labeled_data_by_angle[parsed_angle].append((video_idx, (cleaned_seq, true_reps)))\n",
    "\n",
    "            repetitions = segment_repetitions(cleaned_seq, exercise_label, joint_names, exercise_config)\n",
    "            if not repetitions: continue\n",
    "\n",
    "            for rep_segment in repetitions:\n",
    "                if rep_segment is not None and rep_segment.shape[0] >= 2:\n",
    "                    time_norm_rep = time_normalize_sequence(rep_segment, target_rep_length)\n",
    "                    expected_coords = len(joint_names) * 2\n",
    "                    if not np.isnan(time_norm_rep).all() and time_norm_rep.shape == (target_rep_length, expected_coords):\n",
    "                        all_reps_by_angle[parsed_angle].append(time_norm_rep)\n",
    "\n",
    "        all_reps_across_angles = []\n",
    "        for angle in ['left', 'right', 'front', 'unknown']:\n",
    "            labeled_data_for_this_angle = labeled_data_by_angle[angle]\n",
    "            normalized_reps_for_this_angle = all_reps_by_angle[angle]\n",
    "            if not normalized_reps_for_this_angle: continue\n",
    "            all_reps_across_angles.extend(normalized_reps_for_this_angle)\n",
    "\n",
    "            if labeled_data_for_this_angle:\n",
    "                 tuned_p, tuned_d = tune_counting_parameters(exercise_label, labeled_data_for_this_angle,\n",
    "                                                            joint_names, exercise_config, prominence_range, distance_range)\n",
    "                 tuned_counting_params[exercise_label][angle] = (tuned_p, tuned_d)\n",
    "            else: tuned_counting_params[exercise_label][angle] = (DEFAULT_PEAK_PROMINENCE, DEFAULT_PEAK_DISTANCE)\n",
    "\n",
    "            reps_stack = np.stack(normalized_reps_for_this_angle, axis=0)\n",
    "            averaged_sequence_angle = np.nanmean(reps_stack, axis=0)\n",
    "            averaged_baseline_data[exercise_label][angle] = np.nan_to_num(averaged_sequence_angle, nan=0.0)\n",
    "\n",
    "            derived_threshold_angle = None\n",
    "            if len(normalized_reps_for_this_angle) >= min_reps_for_threshold:\n",
    "                intra_distances_angle = []\n",
    "                safe_avg_angle = np.nan_to_num(averaged_sequence_angle, nan=0.0)\n",
    "                for norm_rep in normalized_reps_for_this_angle:\n",
    "                     safe_rep = np.nan_to_num(norm_rep, nan=0.0)\n",
    "                     if safe_rep.shape == safe_avg_angle.shape:\n",
    "                         try: dist, _ = fastdtw(safe_rep, safe_avg_angle, dist=euclidean); intra_distances_angle.append(dist)\n",
    "                         except Exception: pass\n",
    "                if len(intra_distances_angle) >= min_reps_for_threshold:\n",
    "                     mean_dist, std_dist = np.mean(intra_distances_angle), np.std(intra_distances_angle)\n",
    "                     derived_threshold_angle = mean_dist + threshold_std_multiplier * std_dist\n",
    "            exercise_specific_thresholds[exercise_label][angle] = derived_threshold_angle\n",
    "\n",
    "        if len(all_reps_across_angles) >= min_reps_for_threshold:\n",
    "             overall_reps_stack = np.stack(all_reps_across_angles, axis=0)\n",
    "             overall_averaged_sequence = np.nanmean(overall_reps_stack, axis=0)\n",
    "             averaged_baseline_data[exercise_label]['overall'] = np.nan_to_num(overall_averaged_sequence, nan=0.0)\n",
    "             overall_intra_distances = []\n",
    "             safe_overall_avg = np.nan_to_num(overall_averaged_sequence, nan=0.0)\n",
    "             for norm_rep in all_reps_across_angles:\n",
    "                  safe_rep = np.nan_to_num(norm_rep, nan=0.0)\n",
    "                  if safe_rep.shape == safe_overall_avg.shape:\n",
    "                       try: dist, _ = fastdtw(safe_rep, safe_overall_avg, dist=euclidean); overall_intra_distances.append(dist)\n",
    "                       except Exception: pass\n",
    "             if len(overall_intra_distances) >= min_reps_for_threshold:\n",
    "                  mean_dist, std_dist = np.mean(overall_intra_distances), np.std(overall_intra_distances)\n",
    "                  derived_threshold_overall = mean_dist + threshold_std_multiplier * std_dist\n",
    "                  exercise_specific_thresholds[exercise_label]['overall'] = derived_threshold_overall\n",
    "             else: exercise_specific_thresholds[exercise_label]['overall'] = None\n",
    "        else:\n",
    "             averaged_baseline_data[exercise_label]['overall'] = None\n",
    "             exercise_specific_thresholds[exercise_label]['overall'] = None\n",
    "\n",
    "    end_time_total = time.time()\n",
    "    print(f\"\\nBaseline processing complete in {time.time() - start_time_total:.2f}s.\")\n",
    "    return averaged_baseline_data, tuned_counting_params, exercise_specific_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62343d38-6938-421c-ba14-7f5a5350572c",
   "metadata": {},
   "source": [
    "### Recognition Function\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell takes a new video file and analyzes it against the previously loaded baseline data to classify the exercise performed in each detected repetition.\n",
    "\n",
    "**Functions Defined:**\n",
    "\n",
    "1.  **`recognize_exercise(new_video_path, averaged_baseline_data, hrnet_model, joint_names, exercise_config, target_rep_length, tuned_counting_params, exercise_specific_thresholds, hint_angle=None, fallback_threshold=DTW_DISTANCE_THRESHOLD)`:**\n",
    "    * **Purpose:** To process a new video, segment it into repetitions (using parameters potentially tuned for an initial best guess exercise and the hinted angle), compare each time-normalized repetition segment against angle-specific (or overall) averaged baselines using DTW, apply exercise- and angle-specific (or overall/fallback) thresholds, and return the classification result for each repetition.\n",
    "    * **Arguments:**\n",
    "        * `new_video_path`: ( `str` or `pathlib.Path`) - Path to the video file to be analyzed.\n",
    "        * `averaged_baseline_data`: ( `dict`) - Nested dictionary `{exercise: {angle: avg_sequence}}` returned by `load_baseline_data`.\n",
    "        * `hrnet_model`: ( `SimpleHRNet` object) - The loaded pose estimation model.\n",
    "        * `joint_names`: ( `list` of `str`) - Ordered list of joint names.\n",
    "        * `exercise_config`: ( `dict`) - The main configuration dictionary defining exercise-specific logic.\n",
    "        * `target_rep_length`: ( `int`, Unit = Frames) - Target length for time normalization.\n",
    "        * `tuned_counting_params`: ( `dict`) - Nested dictionary `{exercise: {angle: (prominence, distance)}}` returned by `load_baseline_data`.\n",
    "        * `exercise_specific_thresholds`: ( `dict`) - Nested dictionary `{exercise: {angle: threshold}}` returned by `load_baseline_data`.\n",
    "        * `hint_angle`: ( `str` or `None`, Unit = Category label) - Optional hint ('left', 'right', 'front'). If provided, comparison uses this angle's baselines/thresholds. If `None`, uses 'overall' baselines/thresholds. Default is `None`.\n",
    "        * `fallback_threshold`: ( `float`, Unit = Unitless Distance) - The global DTW threshold used if no specific derived threshold is available for the exercise/angle combination being compared.\n",
    "    * **Outcome:** Returns a tuple containing:\n",
    "        1.  `repetition_results`: ( `list` of `tuple`) - A list where each tuple represents a detected repetition: `(rep_index, matched_label, min_distance)`.\n",
    "            * `rep_index`: ( `int`) 1-based index.\n",
    "            * `matched_label`: ( `str`) Contains the name of the matched exercise if `min_distance` was below the relevant threshold, otherwise a string like \"No Match / Inconclusive (...)\" or \"Error...\".\n",
    "            * `min_distance`: ( `float`) The lowest DTW distance found when comparing this repetition against the relevant baselines.\n",
    "        2.  `num_detected_reps`: ( `int`) - The total number of segments detected in the video by `segment_repetitions`.\n",
    "    * Unit = See types above. Distances are unitless. Counts are integers. Labels are strings.\n",
    "    * **Calculated Means / Logic:**\n",
    "        * **Initial Processing:** Reads the video, extracts raw poses, performs spatial normalization, and cleans NaN frames (`process_single_video`, `normalize_pose_sequence`, `handle_nan_values`).\n",
    "        * **Initial Guess:** Time-normalizes the *entire cleaned sequence* and compares it against the 'overall' averaged baseline for each exercise to get a quick `initial_best_label`.\n",
    "        * **Segmentation Parameter Selection:** Uses the `initial_best_label` and the `hint_angle` (or 'front' as fallback) to look up the *tuned* counting parameters (`segment_p`, `segment_d`) from `tuned_counting_params`. Defaults are used if tuning failed or the exercise/angle wasn't found.\n",
    "        * **Test Video Segmentation:** Calls `segment_repetitions` on the *cleaned, non-time-normalized* sequence, passing the `exercise_config` (for the guessed exercise if available) and the selected `segment_p`, `segment_d` to break the video into `test_repetitions`.\n",
    "        * **Angle Key Selection:** Determines the `comparison_angle_key` ('left', 'right', 'front', or 'overall') based on the `hint_angle`.\n",
    "        * **Per-Repetition Comparison:** Loops through each `rep_segment`:\n",
    "            * Time-normalizes the segment to `target_rep_length`.\n",
    "            * Compares this normalized segment against the averaged baseline corresponding to the `comparison_angle_key` for *every* exercise using DTW.\n",
    "            * Finds the exercise with the minimum DTW distance (`best_rep_label_for_this_rep`, `min_rep_distance`).\n",
    "        * **Angle-Aware Thresholding:** Determines the correct threshold to use by checking `exercise_specific_thresholds` for the `best_rep_label_for_this_rep` and the `comparison_angle_key`. It includes fallbacks: first checks the specific angle threshold, then the 'overall' threshold for that exercise, then the global `fallback_threshold`.\n",
    "        * **Rep Classification:** Compares `min_rep_distance` against the determined `threshold_to_use` to decide the final `matched_label_for_rep`.\n",
    "        * **Result Aggregation:** Stores the results for each repetition in the `repetition_results` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "592253a8-f2e7-4180-89c3-ce99e497ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_exercise(new_video_path, averaged_baseline_data, hrnet_model, joint_names, exercise_config,\n",
    "                       target_rep_length, tuned_counting_params, exercise_specific_thresholds,\n",
    "                       hint_angle=None, fallback_threshold=DTW_DISTANCE_THRESHOLD):\n",
    "    \"\"\"Recognizes exercise using angle hint, config-driven logic, angle-specific baselines/thresholds.\"\"\"\n",
    "    print(f\"\\n--- Recognizing Exercise (Angle Hint='{hint_angle}') for: {os.path.basename(new_video_path)} ---\")\n",
    "    start_time = time.time()\n",
    "    if tuned_counting_params is None: tuned_counting_params = {}\n",
    "    if exercise_specific_thresholds is None: exercise_specific_thresholds = {}\n",
    "    default_p, default_d = DEFAULT_PEAK_PROMINENCE, DEFAULT_PEAK_DISTANCE\n",
    "\n",
    "    new_raw_seq = process_single_video(new_video_path, hrnet_model)\n",
    "    if new_raw_seq is None or new_raw_seq.shape[0] == 0: print(\"E: Failed processing video.\"); return [], 0\n",
    "    new_spatially_norm_seq = normalize_pose_sequence(new_raw_seq, joint_names)\n",
    "    if new_spatially_norm_seq is None or np.all(np.isnan(new_spatially_norm_seq)): print(\"E: Failed spatial normalization.\"); return [], 0\n",
    "    new_sequence_clean = handle_nan_values(new_spatially_norm_seq)\n",
    "    if new_sequence_clean.shape[0] < 2: print(\"W: Sequence too short after cleaning.\"); return [], 0\n",
    "\n",
    "    new_sequence_time_norm = time_normalize_sequence(new_sequence_clean, target_rep_length)\n",
    "    if np.isnan(new_sequence_time_norm).all(): print(\"E: Time normalization failed for whole sequence.\"); return [], 0\n",
    "    new_sequence_time_norm = np.nan_to_num(new_sequence_time_norm, nan=0.0)\n",
    "\n",
    "    initial_min_distance, initial_best_label = float('inf'), None\n",
    "    segment_p, segment_d = default_p, default_d\n",
    "    segment_exercise_config_dict = None\n",
    "\n",
    "    if not averaged_baseline_data: print(\"E: No baseline data.\"); return [], 0\n",
    "    for ex_label, angle_dict in averaged_baseline_data.items():\n",
    "        avg_base_seq = angle_dict.get('overall')\n",
    "        if avg_base_seq is None: continue\n",
    "        if avg_base_seq.shape[0] != target_rep_length or avg_base_seq.ndim != 2 or \\\n",
    "           new_sequence_time_norm.shape[1] != avg_base_seq.shape[1]: continue\n",
    "        safe_base = np.nan_to_num(avg_base_seq, nan=0.0)\n",
    "        try: dist, _ = fastdtw(new_sequence_time_norm, safe_base, dist=euclidean)\n",
    "        except Exception: continue\n",
    "        if dist < initial_min_distance: initial_min_distance, initial_best_label = dist, ex_label\n",
    "\n",
    "    angle_for_tuning_lookup = hint_angle if hint_angle in ['left', 'right', 'front'] else 'front'\n",
    "\n",
    "    if initial_best_label:\n",
    "        segment_exercise_config_dict = exercise_config.get(initial_best_label)\n",
    "        if segment_exercise_config_dict:\n",
    "             tuned_p, tuned_d = tuned_counting_params.get(initial_best_label, {}).get(angle_for_tuning_lookup, (default_p, default_d))\n",
    "             segment_p, segment_d = tuned_p, tuned_d\n",
    "        else:\n",
    "             print(f\"W: No config found for initial guess '{initial_best_label}'. Using default segmentation params.\")\n",
    "             initial_best_label = None\n",
    "    else:\n",
    "        print(f\"  Initial guess failed. Using default\")\n",
    "\n",
    "    segment_label_for_logic = initial_best_label if initial_best_label else \"unknown\"\n",
    "    test_repetitions = segment_repetitions(new_sequence_clean, segment_label_for_logic, joint_names, exercise_config,\n",
    "                                           peak_prominence=segment_p, peak_distance=segment_d)\n",
    "    num_detected_reps = len(test_repetitions)\n",
    "    print(f\"  Detected {num_detected_reps} potential repetitions in test video.\")\n",
    "    if num_detected_reps == 0: return [], 0\n",
    "\n",
    "    repetition_results = []\n",
    "    comparison_angle_key = hint_angle if hint_angle in ['left', 'right', 'front'] else 'overall'\n",
    "    print(f\"  Comparing each detected repetition against '{comparison_angle_key}' baselines...\")\n",
    "\n",
    "    for i, rep_segment in enumerate(test_repetitions):\n",
    "        if rep_segment is None or rep_segment.shape[0] < 2: repetition_results.append( (i+1, \"Error: Invalid Segment\", float('inf')) ); continue\n",
    "        time_norm_rep_segment = time_normalize_sequence(rep_segment, target_rep_length)\n",
    "        if np.isnan(time_norm_rep_segment).all(): repetition_results.append( (i+1, \"Error: Time Norm Failed\", float('inf')) ); continue\n",
    "        time_norm_rep_segment = np.nan_to_num(time_norm_rep_segment, nan=0.0)\n",
    "\n",
    "        min_rep_distance, best_rep_label_for_this_rep = float('inf'), None\n",
    "        for exercise_label, angle_dict in averaged_baseline_data.items():\n",
    "            baseline_to_compare = angle_dict.get(comparison_angle_key)\n",
    "            if baseline_to_compare is None or time_norm_rep_segment.shape != baseline_to_compare.shape: continue\n",
    "\n",
    "            safe_baseline = np.nan_to_num(baseline_to_compare, nan=0.0)\n",
    "            try: distance, _ = fastdtw(time_norm_rep_segment, safe_baseline, dist=euclidean)\n",
    "            except Exception as e: print(f\"W: DTW error Rep {i+1} vs {exercise_label}/{comparison_angle_key}: {e}\"); continue\n",
    "            if distance < min_rep_distance: min_rep_distance, best_rep_label_for_this_rep = distance, exercise_label\n",
    "\n",
    "        matched_label_for_rep = \"No Match / Inconclusive\"\n",
    "        threshold_to_use = fallback_threshold\n",
    "        if best_rep_label_for_this_rep:\n",
    "            threshold_specific = exercise_specific_thresholds.get(best_rep_label_for_this_rep, {}).get(comparison_angle_key)\n",
    "            if threshold_specific is None and comparison_angle_key != 'overall': \n",
    "                 threshold_specific = exercise_specific_thresholds.get(best_rep_label_for_this_rep, {}).get('overall')\n",
    "\n",
    "            if threshold_specific is not None: \n",
    "                 threshold_to_use = threshold_specific\n",
    "            if min_rep_distance < threshold_to_use:\n",
    "                 matched_label_for_rep = best_rep_label_for_this_rep\n",
    "            else: \n",
    "                 matched_label_for_rep = f\"No Match (Closest: {best_rep_label_for_this_rep}, Dist: {min_rep_distance:.2f} >= {threshold_to_use:.2f})\"\n",
    "\n",
    "        repetition_results.append( (i+1, matched_label_for_rep, min_rep_distance) )\n",
    "\n",
    "    return repetition_results, num_detected_reps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee2b3b-9754-4c36-aad8-20016a9ef706",
   "metadata": {},
   "source": [
    "### Load baseline data\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell executes the baseline creation and processing pipeline\n",
    "\n",
    "**Outcomes:**\n",
    "\n",
    "Running this cell populates the three key dictionaries that store the results of the baseline processing:\n",
    "\n",
    "1.  `averaged_baselines`: ( `dict`)\n",
    "    * **Structure:** `{exercise_label: {angle_key: numpy.array}}` (e.g., `{'squat': {'front': array([...]), 'overall': array([...])}}`)\n",
    "    * **Meaning:** Contains the final averaged, time-normalized pose sequence for each detected angle ('left', 'right', 'front', 'unknown') and the 'overall' average for each exercise. These arrays represent the \"template\" movements used for comparison. Returns an empty dictionary if processing fails entirely.\n",
    "    * Unit = Arrays contain unitless normalized coordinates.\n",
    "2.  `tuned_params`: ( `dict`)\n",
    "    * **Structure:** `{exercise_label: {angle_key: (prominence, distance)}}` (e.g., `{'squat': {'front': (0.06, 15), 'overall': (0.05, 10)}}`)\n",
    "    * **Meaning:** Stores the optimal `(peak_prominence, peak_distance)` tuple found by the `tune_counting_parameters` function for each exercise and angle category based on the labeled baseline videos. Contains default parameters if tuning failed or no labeled videos were found for a specific category. Returns an empty dictionary if processing fails entirely.\n",
    "    * Unit = Prominence (float) is unitless ratio, Distance (int) is in frames.\n",
    "3.  `derived_thresholds`: ( `dict`)\n",
    "    * **Structure:** `{exercise_label: {angle_key: threshold_value or None}}` (e.g., `{'squat': {'front': 450.7, 'overall': 510.2}}`)\n",
    "    * **Meaning:** Stores the suggested DTW recognition threshold calculated for each exercise and angle category (including 'overall') based on the variance within its baseline repetitions (e.g., Mean + N * StdDev of intra-exercise distances). Contains `None` if a threshold couldn't be derived (e.g., too few repetitions). Returns an empty dictionary if processing fails entirely.\n",
    "    * Unit = Float, unitless distance (same scale as DTW results).\n",
    "\n",
    "**Printed Output:**\n",
    "\n",
    "* Extensive logs generated *during* the execution of `load_baseline_data` (showing progress per exercise, tuning results, derived thresholds, etc.).\n",
    "* A final summary message indicating whether the baseline data loading/processing was considered successful (based on `averaged_baselines` being populated).\n",
    "* A printout of the `derived_thresholds` dictionary (or a message if it's empty/missing), allowing the user to inspect the suggested thresholds.\n",
    "\n",
    "**Calculated Means:**\n",
    "\n",
    "This cell triggers the execution of the entire complex pipeline defined in `load_baseline_data`. The \"meaning\" is the generation and storage of the structured baseline information (`averaged_baselines`, `tuned_params`, `derived_thresholds`) which encapsulates the learned characteristics (average movement, optimal counting parameters, typical variation) for each exercise and angle, ready to be used for recognition in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "536d7e64-46a1-4bcc-b85e-4a1ffd144adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Baselines (Config-Driven, Angle from Filename)... from: C:/Users/michel.marien_icarew/Documents/Privé/Opleiding/Mastervakken/Deep Neural Networks/Opdacht 2/baselines2\n",
      "\n",
      "Processing baseline exercise: tricep_pushdown (from folder: tricep Pushdown)\n",
      "  Found 4 video file(s).\n",
      "  Tuning repetition counting parameters for: tricep_pushdown...\n",
      "Tuning complete. Best Params: P=0.020, D=30, Min Error=1. (Default Error=8)\n",
      "Detected reps using BEST params (P=0.020, D=30):\n",
      "Video Index 2: Detected=4, True=4\n",
      "Video Index 3: Detected=4, True=5\n",
      "Total Reps: Detected=8, True=9, Overall Error=1\n",
      "  Tuning repetition counting parameters for: tricep_pushdown...\n",
      "Tuning complete. Best Params: P=0.020, D=35, Min Error=1. (Default Error=3)\n",
      "Detected reps using BEST params (P=0.020, D=35):\n",
      "Video Index 0: Detected=3, True=3\n",
      "Video Index 1: Detected=3, True=4\n",
      "Total Reps: Detected=6, True=7, Overall Error=1\n",
      "\n",
      "Processing baseline exercise: tricep_dips (from folder: tricep dips)\n",
      "  Found 3 video file(s).\n",
      "\n",
      "Baseline processing complete in 1971.23s.\n",
      "\n",
      " Baseline Data Loaded/Processed Successfully👍\n",
      "\n",
      " Derived Recognition Thresholds (Per Angle + Overall)\n",
      "{'tricep_pushdown': {'left': 5015.230155410494, 'front': 278.4184631181901, 'overall': 4925.864656126366}, 'tricep_dips': {'overall': None}}\n"
     ]
    }
   ],
   "source": [
    "# In case of re-running the cell\n",
    "averaged_baselines = {}\n",
    "tuned_params = {}\n",
    "derived_thresholds = {}\n",
    "\n",
    "# Running\n",
    "averaged_baselines, tuned_params, derived_thresholds = load_baseline_data(\n",
    "    BASELINE_ROOT_DIR,\n",
    "    HRNET_MODEL,\n",
    "    JOINT_NAMES_LIST,\n",
    "    TARGET_REP_LENGTH,\n",
    "    EXERCISE_CONFIG,\n",
    "    TUNING_PROMINENCE_RANGE,\n",
    "    TUNING_DISTANCE_RANGE,\n",
    "    threshold_std_multiplier=DEFAULT_THRESHOLD_STD_MULTIPLIER,\n",
    "    min_reps_for_threshold=DEFAULT_MIN_REPS_FOR_THRESHOLD\n",
    ")\n",
    "\n",
    "if averaged_baselines:\n",
    "    print(\"\\n Baseline Data Loaded/Processed Successfully👍\")\n",
    "    print(\"\\n Derived recognition thresholds (Per Angle and Overall)\")\n",
    "else:\n",
    "    print(\"\\n Baseline Data Loading/Processing Failed👎\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17905dae-3b22-4395-8d8c-f400f4bdcf9b",
   "metadata": {},
   "source": [
    "### Testvideo\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "This cell analyzing a new video file.\n",
    "\n",
    "**User Inputs in this Cell:**\n",
    "\n",
    "* `video_to_test`: ( `str` or `pathlib.Path`)\n",
    "    * **Meaning:** Specifies the **full path** to the video file you want to classify. You should **modify this variable** each time you want to test a different video without re-running previous cells (especially Cell 6). It defaults to `VIDEO_TO_RECOGNIZE_PATH` from Cell 2 if not changed here.\n",
    "* `manual_angle_hint`: ( `str` or `None`)\n",
    "    * **Meaning:** Allows you to *optionally* tell the recognition function the expected camera angle ('left', 'right', 'front'). If set to one of these strings, the function will prioritize comparing the test video's repetitions against the baselines and thresholds specifically created for that angle. If left as `None`, the function will use the 'overall' baselines and thresholds (averaged across all angles).\n",
    "    * Unit = Category label (`'left'`, `'right'`, `'front'`) or `None`.\n",
    "\n",
    "**Outcomes:**\n",
    "\n",
    "This cell does not assign results to Python variables but produces **Printed Output** to the notebook interface, summarizing the recognition attempt:\n",
    "\n",
    "* **Header:** Indicates which video is being processed and if an angle hint was provided.\n",
    "* **Detected Repetitions:** The total number of potential repetitions identified by the segmentation step.\n",
    "    *  `int`, Unit = Count.\n",
    "* **Per-Repetition Analysis:** A detailed list showing the result for each detected repetition:\n",
    "    * Repetition Index: ( `int`).\n",
    "    * Classification Status: ( `str`) - Indicates either:\n",
    "        * A successful match (e.g., `-> Match: squat (Dist: 350.25)`).\n",
    "        * A failure to meet the threshold (e.g., `-> No Match (Closest: squat, Dist: 600.50 >= 550.00)`).\n",
    "        * An error during processing for that rep (e.g., `-> Error: Time Norm Failed`).\n",
    "    * Minimum DTW Distance: ( `float`, Unit = Unitless Distance) - The lowest DTW distance found between this repetition and the relevant baseline it was compared against.\n",
    "* **Overall Summary:**\n",
    "    * Predominant Match: ( `str`) - The exercise label that was successfully matched most often across all repetitions (majority vote).\n",
    "    * Match Counts: ( `int`/`int`, Unit = Count) - Shows how many reps were successfully matched compared to the total number detected by segmentation.\n",
    "\n",
    "**Calculated Means / Logic:**\n",
    "\n",
    "* **Pre-run Checks:** Verifies that all necessary variables (`averaged_baselines`, `tuned_params`, `derived_thresholds`, `HRNET_MODEL`, `exercise_config`, constants) are defined and that the `video_to_test` file exists before proceeding. Prevents errors if previous cells failed or weren't run.\n",
    "* **Function Call:** Executes the main `recognize_exercise` function, passing all the required data structures generated by `load_baseline_data`, the configuration, and the user inputs (`video_to_test`, `manual_angle_hint`).\n",
    "* **Result Interpretation:** Processes the list returned by `recognize_exercise` to count successful matches, determine the most frequent match (majority vote), and format the output clearly for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8c0238e4-ad57-41b9-93ee-2c80906d2c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================================================================\n",
      " Recognizing Exercise in Video\n",
      " Video: test_excercise.mp4 \n",
      "==========================================================================================================================================\n",
      "\n",
      "--- Recognizing Exercise (Angle Hint='None') for: test_excercise.mp4 ---\n",
      "  Detected 9 potential repetitions in test video.\n",
      "  Comparing each detected repetition against 'overall' baselines...\n",
      "\n",
      "==========================================================================================================================================\n",
      " Results for test_excercise.mp4\n",
      "==========================================================================================================================================\n",
      " Detected 9 Repetitions.\n",
      "\n",
      " Per repetition Analysis ---\n",
      "    Rep 1: -> Match: tricep_pushdown (Dist: 1966.02)\n",
      "    Rep 2: -> Match: tricep_pushdown (Dist: 1803.06)\n",
      "    Rep 3: -> Match: tricep_pushdown (Dist: 1765.82)\n",
      "    Rep 4: -> Match: tricep_pushdown (Dist: 2251.53)\n",
      "    Rep 5: -> Match: tricep_pushdown (Dist: 2018.40)\n",
      "    Rep 6: -> Match: tricep_pushdown (Dist: 2566.13)\n",
      "    Rep 7: -> Match: tricep_pushdown (Dist: 2169.83)\n",
      "    Rep 8: -> Match: tricep_pushdown (Dist: 1948.93)\n",
      "    Rep 9: -> Match: tricep_pushdown (Dist: 1990.43)\n",
      "\n",
      " Overall Summary \n",
      "  Overall Predominant Match: tricep_pushdown (9/9 successfully matched reps)\n",
      " Total Reps Successfully Matched: 9 / 9\n",
      "==========================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_to_test = VIDEO_TO_RECOGNIZE_PATH\n",
    "\n",
    "# Optional angle hint\n",
    "manual_angle_hint = None #'left', 'right', 'front'\n",
    "\n",
    "if proceed_to_recognition:\n",
    "    print(\"==========================================================================================================================================\")\n",
    "    print(f\" Recognizing Exercise in Video\")\n",
    "    print(f\" Video: {os.path.basename(video_to_test)} \")\n",
    "    print(\"==========================================================================================================================================\")\n",
    "\n",
    "    list_of_rep_results, total_reps_detected = recognize_exercise(\n",
    "        new_video_path=video_to_test,\n",
    "        averaged_baseline_data=averaged_baselines,\n",
    "        hrnet_model=HRNET_MODEL,\n",
    "        joint_names=JOINT_NAMES_LIST,\n",
    "        exercise_config=EXERCISE_CONFIG,\n",
    "        target_rep_length=TARGET_REP_LENGTH,\n",
    "        tuned_counting_params=tuned_params,\n",
    "        exercise_specific_thresholds=derived_thresholds,\n",
    "        hint_angle=manual_angle_hint,\n",
    "        fallback_threshold=DTW_DISTANCE_THRESHOLD)\n",
    "\n",
    "    print(\"\\n==========================================================================================================================================\")\n",
    "    print(f\" Results for {os.path.basename(video_to_test)}\")\n",
    "    print(\"==========================================================================================================================================\")\n",
    "    print(f\" Detected {total_reps_detected} Repetitions.\")\n",
    "\n",
    "    if not list_of_rep_results:\n",
    "        print(\"No valid repetition results to display\")\n",
    "    else:\n",
    "        print(\"\\n Per repetition Analysis\")\n",
    "        match_counts = {}\n",
    "        successful_reps = 0\n",
    "        overall_best_label = None\n",
    "        valid_matches = [res[1] for res in list_of_rep_results if isinstance(res[1], str) and not res[1].startswith(\"Error\") and not res[1].startswith(\"No Match\")]\n",
    "        if valid_matches:\n",
    "             temp_match_counts = {}\n",
    "             for label in valid_matches: temp_match_counts[label] = temp_match_counts.get(label, 0) + 1\n",
    "             if temp_match_counts: overall_best_label = max(temp_match_counts, key=temp_match_counts.get)\n",
    "\n",
    "        for rep_index, matched_label, min_distance in list_of_rep_results:\n",
    "            status = \"\"\n",
    "            if isinstance(matched_label, str) and matched_label.startswith(\"Error\"):\n",
    "                 status = f\"-> {matched_label}\"\n",
    "            elif isinstance(matched_label, str) and matched_label.startswith(\"No Match\"):\n",
    "                 status = f\"-> {matched_label}\"\n",
    "            else:\n",
    "                status = f\"-> Match: {matched_label} (Dist: {min_distance:.2f})\"\n",
    "                match_counts[matched_label] = match_counts.get(matched_label, 0) + 1\n",
    "                successful_reps += 1\n",
    "            print(f\"    Rep {rep_index}: {status}\")\n",
    "\n",
    "        print(\"\\n Overall Summary\")\n",
    "        if successful_reps > 0:\n",
    "             if match_counts:\n",
    "                 majority_label = max(match_counts, key=match_counts.get)\n",
    "                 majority_count = match_counts[majority_label]\n",
    "                 print(f\"Overall match: {majority_label} ({majority_count}/{successful_reps} successfully matched reps)\")\n",
    "             else:\n",
    "                  print(\" No specific exercises were matched consistently.\")\n",
    "             print(f\" Total reps successfully matched: {successful_reps} / {total_reps_detected}\")\n",
    "        elif total_reps_detected > 0:\n",
    "             print(\"No repetitions were successfully matched to any baseline exercise.\")\n",
    "        else:\n",
    "             print(\"No repetitions were detected in the video\")\n",
    "    print(\"==========================================================================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d936f-aa4d-4189-af8e-9cb9b30c153e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f13c00-437d-44a2-9e57-7ba32c506f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f48337-dbe1-48a5-869d-4195b3351c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c35394-eb2f-48de-acbb-9ed25ab71e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ce48a-06b7-47e3-9853-27d180b487dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "dab70c16-ae23-40c9-abec-20322cfe0846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle detection function defined (detect_camera_angle) with shoulder logic.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5.0 (or 5.1): Angle Detection Function (with Logic) ===\n",
    "import numpy as np # Ensure numpy is imported\n",
    "\n",
    "def detect_camera_angle(pose_sequence, joint_names, min_valid_frames=10, width_threshold_factor=0.35):\n",
    "    \"\"\"\n",
    "    Analyzes pose sequence to determine camera angle (left, right, front)\n",
    "    based on average shoulder horizontal positioning.\n",
    "\n",
    "    Args:\n",
    "        pose_sequence (np.array): Spatially normalized pose sequence (frames, coords).\n",
    "                                    Should handle potential NaNs.\n",
    "        joint_names (list): List of joint names.\n",
    "        min_valid_frames (int): Minimum number of frames where both shoulders must be detected.\n",
    "        width_threshold_factor (float): Percentage of average shoulder width used as a threshold\n",
    "                                         to determine side view (e.g., 0.35 = 35%). Needs tuning.\n",
    "\n",
    "    Returns:\n",
    "        str: 'left', 'right', 'front', or 'unknown'.\n",
    "    \"\"\"\n",
    "    if pose_sequence is None or pose_sequence.shape[0] < min_valid_frames:\n",
    "        # print(\"DEBUG Angle: Sequence too short.\") # Optional Debug\n",
    "        return 'unknown' # Not enough data\n",
    "\n",
    "    try:\n",
    "        # Get indices for shoulders (ensure they exist in your JOINT_NAMES_LIST)\n",
    "        lshoulder_idx = joint_names.index('Left Shoulder')\n",
    "        rshoulder_idx = joint_names.index('Right Shoulder')\n",
    "\n",
    "        # Get X coordinates (indices are 2*joint_idx + 0)\n",
    "        lshoulder_x_idx = 2 * lshoulder_idx\n",
    "        rshoulder_x_idx = 2 * rshoulder_idx\n",
    "\n",
    "        # Filter frames where *both* shoulders are detected (not NaN)\n",
    "        valid_frames_mask = ~np.isnan(pose_sequence[:, lshoulder_x_idx]) & \\\n",
    "                            ~np.isnan(pose_sequence[:, rshoulder_x_idx])\n",
    "\n",
    "        if np.sum(valid_frames_mask) < min_valid_frames:\n",
    "            # print(f\"DEBUG Angle: Not enough valid shoulder frames found ({np.sum(valid_frames_mask)} < {min_valid_frames}).\") # Optional Debug\n",
    "            return 'unknown' # Not enough reliable data\n",
    "\n",
    "        valid_lshoulder_x = pose_sequence[valid_frames_mask, lshoulder_x_idx]\n",
    "        valid_rshoulder_x = pose_sequence[valid_frames_mask, rshoulder_x_idx]\n",
    "\n",
    "        # Calculate average X positions\n",
    "        avg_x_left = np.mean(valid_lshoulder_x)\n",
    "        avg_x_right = np.mean(valid_rshoulder_x)\n",
    "\n",
    "        # Calculate average horizontal distance between shoulders (our reference width)\n",
    "        avg_shoulder_width = np.mean(np.abs(valid_lshoulder_x - valid_rshoulder_x))\n",
    "\n",
    "        if avg_shoulder_width < 1e-6: # Avoid division by zero / handle overlapping points\n",
    "             # print(\"DEBUG Angle: Average shoulder width near zero.\") # Optional Debug\n",
    "             return 'front' # Or 'unknown', front seems safer if points overlap\n",
    "\n",
    "        # Calculate the difference (Right X - Left X)\n",
    "        # Note: In standard image coordinates (origin top-left), smaller X is to the left.\n",
    "        # Adjust if your normalization flips axes.\n",
    "        x_difference = avg_x_right - avg_x_left\n",
    "\n",
    "        # Determine view based on the difference relative to the average width\n",
    "        threshold = avg_shoulder_width * width_threshold_factor\n",
    "\n",
    "        detected_angle = 'front' # Default assumption\n",
    "        if x_difference > threshold:\n",
    "            # Right shoulder X is significantly larger than Left shoulder X\n",
    "            # -> We are likely seeing the person's left side more prominently\n",
    "            # -> Camera is likely positioned to the person's RIGHT\n",
    "            detected_angle = 'right'\n",
    "        elif x_difference < -threshold:\n",
    "            # Left shoulder X is significantly larger than Right shoulder X\n",
    "            # -> We are likely seeing the person's right side more prominently\n",
    "            # -> Camera is likely positioned to the person's LEFT\n",
    "            detected_angle = 'left'\n",
    "        # Else: Shoulders are relatively aligned horizontally -> assume front view\n",
    "\n",
    "        # print(f\"DEBUG Angle: AvgL={avg_x_left:.2f}, AvgR={avg_x_right:.2f}, Diff={x_difference:.2f}, Width={avg_shoulder_width:.2f}, Thresh={threshold:.2f} -> Angle={detected_angle}\") # Optional Debug\n",
    "\n",
    "        return detected_angle\n",
    "\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Warning: Could not find required joints ('Left Shoulder', 'Right Shoulder') for angle detection: {e}\")\n",
    "        return 'unknown'\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error during angle detection: {e}\")\n",
    "        return 'unknown'\n",
    "\n",
    "print(\"Angle detection function defined (detect_camera_angle) with shoulder logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "dcb6426c-b910-492d-b321-740217d488ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "device: 'cpu'\n",
      "SimpleHRNet model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "HRNET_MODEL = None\n",
    "try:\n",
    "    HRNET_MODEL = SimpleHRNet(c=48, \n",
    "                              nof_joints=EXPECTED_JOINT_COUNT,\n",
    "                              checkpoint_path=MODEL_PATH,\n",
    "                              device=DEVICE,\n",
    "                              multiperson=False, # Logic currently assumes single person\n",
    "                              max_batch_size=16) # Added default batch size\n",
    "    print(\"SimpleHRNet model loaded successfully.\")\n",
    "except NameError:\n",
    "    print(\"Error: SimpleHRNet class not found. Check import in Cell 1.\")\n",
    "except FileNotFoundError as fnf_error:\n",
    "    print(fnf_error) # Print the specific error\n",
    "except Exception as e:\n",
    "    print(f\"Error loading SimpleHRNet model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e44266-b5c8-48aa-aca6-fd441abd1fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc8ef2-992f-4aa6-8413-e20d8c8785b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "4d11521f-dc25-4e33-984c-c1f2faa4705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def get_keypoint(frame_kps, kp_index):\n",
    "    \"\"\"Safely get keypoint coordinates (x, y) from a frame's keypoint array.\"\"\"\n",
    "    if frame_kps is not None and kp_index < frame_kps.shape[0]:\n",
    "        coords = frame_kps[kp_index, :2] # Take only x, y\n",
    "        if not np.isnan(coords).any():\n",
    "            return coords\n",
    "    return None\n",
    "\n",
    "def calculate_angle(p1, p2, p3):\n",
    "    \"\"\"Calculates the angle (in degrees) at p2 formed by p1p2p3.\"\"\"\n",
    "    if p1 is None or p2 is None or p3 is None: return None\n",
    "    v1, v2 = np.array(p1) - np.array(p2), np.array(p3) - np.array(p2)\n",
    "    mag1, mag2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if mag1 * mag2 == 0: return None\n",
    "    cos_angle = np.clip(np.dot(v1, v2) / (mag1 * mag2), 1.0, 1.0)\n",
    "    return np.degrees(np.arccos(cos_angle))\n",
    "\n",
    "def handle_nan_values(sequence):\n",
    "    \"\"\"Handles NaN values in a pose sequence (frames, coords). Removes frames containing any NaNs.\"\"\"\n",
    "    if sequence is None or sequence.ndim != 2 or sequence.shape[0] == 0:\n",
    "        return np.empty((0, sequence.shape[1] if sequence is not None and sequence.ndim == 2 else 0))\n",
    "    valid_frames_mask = ~np.isnan(sequence).any(axis=1)\n",
    "    return sequence[valid_frames_mask]\n",
    "\n",
    "def time_normalize_sequence(sequence, target_length):\n",
    "    \"\"\"Resamples a pose sequence (frames, coords) to a target length using linear interpolation.\"\"\"\n",
    "    if sequence is None or sequence.shape[0] < 2:\n",
    "        num_coords = sequence.shape[1] if sequence is not None and sequence.ndim == 2 else EXPECTED_JOINT_COUNT * 2\n",
    "        return np.full((target_length, num_coords), np.nan)\n",
    "\n",
    "    num_frames, num_coords = sequence.shape\n",
    "    original_indices = np.linspace(0, num_frames - 1, num_frames)\n",
    "    target_indices = np.linspace(0, num_frames - 1, target_length)\n",
    "    normalized_sequence = np.zeros((target_length, num_coords))\n",
    "\n",
    "    for j in range(num_coords):\n",
    "        valid_mask = ~np.isnan(sequence[:, j])\n",
    "        if np.sum(valid_mask) < 2:\n",
    "            normalized_sequence[:, j] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                normalized_sequence[:, j] = np.interp(target_indices, original_indices[valid_mask], sequence[valid_mask, j])\n",
    "            except Exception:\n",
    "                normalized_sequence[:, j] = np.nan\n",
    "\n",
    "    nan_mask = np.isnan(normalized_sequence)\n",
    "    if np.any(nan_mask):\n",
    "        idx = np.where(~nan_mask, np.arange(nan_mask.shape[0])[:, None], 0)\n",
    "        np.maximum.accumulate(idx, axis=0, out=idx)\n",
    "        normalized_sequence[nan_mask] = normalized_sequence[idx[nan_mask], np.nonzero(nan_mask)[1]]\n",
    "        nan_mask = np.isnan(normalized_sequence)\n",
    "        if np.any(nan_mask):\n",
    "            idx = np.where(~nan_mask, np.arange(nan_mask.shape[0])[:, None], nan_mask.shape[0] - 1)\n",
    "            idx = np.minimum.accumulate(idx[::1], axis=0)[::1]\n",
    "            normalized_sequence[nan_mask] = normalized_sequence[idx[nan_mask], np.nonzero(nan_mask)[1]]\n",
    "\n",
    "    return np.nan_to_num(normalized_sequence, nan=0.0)\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a35a2e7-9d2e-4f58-b7d4-e309c1ee29b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9f27eb5f-574f-4625-9e5a-996303fc2363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle detection function defined (detect_camera_angle). Imports for Cell 5 loaded.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5.0: Angle Detection & Imports for Core Functions ===\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re # For parsing filenames\n",
    "import time\n",
    "import math\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.signal import find_peaks\n",
    "import scipy.ndimage # For smoothing (alternative)\n",
    "import pandas as pd # For robust NaN filling\n",
    "\n",
    "# Make sure constants from Cell 2 are accessible (e.g., JOINT_NAMES_LIST)\n",
    "# Make sure helpers from Cell 4 are accessible (get_keypoint, calculate_angle)\n",
    "\n",
    "def detect_camera_angle(pose_sequence, joint_names, min_valid_frames=10, width_threshold_factor=0.35):\n",
    "    \"\"\"\n",
    "    Analyzes pose sequence for camera angle based on shoulder positions.\n",
    "    Placeholder heuristic - needs validation and likely improvement.\n",
    "    \"\"\"\n",
    "    if pose_sequence is None or pose_sequence.shape[0] < min_valid_frames: return 'unknown'\n",
    "    try:\n",
    "        lshoulder_idx = joint_names.index('Left Shoulder')\n",
    "        rshoulder_idx = joint_names.index('Right Shoulder')\n",
    "        lshoulder_x_idx, rshoulder_x_idx = 2 * lshoulder_idx, 2 * rshoulder_idx\n",
    "        # Ensure indices are valid before accessing pose_sequence columns\n",
    "        if max(lshoulder_x_idx, rshoulder_x_idx) >= pose_sequence.shape[1]:\n",
    "             # print(\"W: Shoulder indices out of bounds for pose_sequence in angle detection.\")\n",
    "             return 'unknown'\n",
    "\n",
    "        valid_mask = ~np.isnan(pose_sequence[:, lshoulder_x_idx]) & ~np.isnan(pose_sequence[:, rshoulder_x_idx])\n",
    "        if np.sum(valid_mask) < min_valid_frames: return 'unknown'\n",
    "\n",
    "        valid_lsx = pose_sequence[valid_mask, lshoulder_x_idx]\n",
    "        valid_rsx = pose_sequence[valid_mask, rshoulder_x_idx]\n",
    "        avg_x_left, avg_x_right = np.mean(valid_lsx), np.mean(valid_rsx)\n",
    "        avg_shoulder_width = np.mean(np.abs(valid_lsx - valid_rsx))\n",
    "\n",
    "        if avg_shoulder_width < 1e-6: return 'front'\n",
    "        x_difference = avg_x_right - avg_x_left\n",
    "        threshold = avg_shoulder_width * width_threshold_factor\n",
    "\n",
    "        if x_difference > threshold: return 'right' # Seeing left side -> Camera on right\n",
    "        elif x_difference < -threshold: return 'left' # Seeing right side -> Camera on left\n",
    "        else: return 'front'\n",
    "    except (ValueError, IndexError): # Catch if joints aren't in list\n",
    "        return 'unknown'\n",
    "    except Exception as e:\n",
    "        return 'unknown'\n",
    "\n",
    "print(\"Angle detection function defined (detect_camera_angle). Imports for Cell 5 loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "64f96b65-b93e-4dff-96f1-3e3eb4b6d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 5.1: Basic Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "27853e93-9c04-4a42-95aa-a1e8d4b478c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing and normalization functions defined.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5.1: Video Processing & Normalization Functions ===\n",
    "# Requires numpy, os, cv2, get_keypoint\n",
    "# Assumes EXPECTED_JOINT_COUNT, JOINT_NAMES_LIST are defined\n",
    "\n",
    "def process_single_video(video_path, model, expected_joint_count=EXPECTED_JOINT_COUNT):\n",
    "    \"\"\"Processes a single video, extracts joints, returns NumPy array (frames, num_coords).\"\"\"\n",
    "    if not os.path.exists(video_path): print(f\"E: Video not found: {video_path}\"); return None\n",
    "    video = cv2.VideoCapture(video_path);\n",
    "    if not video.isOpened(): print(f\"E: Cannot open video: {video_path}\"); return None\n",
    "    video_joints, frame_count = [], 0; expected_coords = expected_joint_count * 2\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = video.read();\n",
    "            if not ret: break\n",
    "            try:\n",
    "                joints = model.predict(frame)\n",
    "                if joints is None or joints.shape[0]==0 or joints.shape[1]!=expected_joint_count: frame_coords=np.full(expected_coords,np.nan)\n",
    "                else: frame_coords=joints[0,:,:2].flatten(); frame_coords = frame_coords if frame_coords.shape[0]==expected_coords else np.full(expected_coords,np.nan)\n",
    "            except Exception as e: frame_coords = np.full(expected_coords, np.nan)\n",
    "            video_joints.append(frame_coords); frame_count += 1\n",
    "    finally: video.release()\n",
    "    return np.array(video_joints) if video_joints else None\n",
    "\n",
    "def normalize_pose_sequence(pose_sequence, joint_names, ref_joint1_name=\"Left Shoulder\", ref_joint2_name=\"Right Shoulder\"):\n",
    "    \"\"\"Spatially normalizes a pose sequence based on reference joints.\"\"\"\n",
    "    if pose_sequence is None or pose_sequence.shape[0] == 0: return np.array([])\n",
    "    try: ref_joint1_idx, ref_joint2_idx = joint_names.index(ref_joint1_name), joint_names.index(ref_joint2_name)\n",
    "    except ValueError: print(f\"E: Ref joints not found in normalize\"); return np.full_like(pose_sequence, np.nan)\n",
    "    normalized_frames = []; num_coords = pose_sequence.shape[1]\n",
    "    if num_coords == 0 or num_coords % 2 != 0 or num_coords != len(joint_names) * 2:\n",
    "        print(f\"W: Coord mismatch/zero coords in normalize_pose_sequence ({num_coords} vs {len(joint_names)*2}). Returning NaNs.\")\n",
    "        return np.full_like(pose_sequence, np.nan)\n",
    "\n",
    "    for frame in pose_sequence:\n",
    "        try:\n",
    "             if frame.shape[0] != num_coords: normalized_frames.append(np.full(num_coords, np.nan)); continue\n",
    "             frame_joints = frame.reshape(-1, 2);\n",
    "             if frame_joints.shape[0] != len(joint_names): normalized_frames.append(np.full(num_coords, np.nan)); continue\n",
    "             ref1, ref2 = get_keypoint(frame_joints, ref_joint1_idx), get_keypoint(frame_joints, ref_joint2_idx)\n",
    "             if ref1 is None or ref2 is None: normalized_frames.append(np.full(num_coords, np.nan)); continue\n",
    "             center, scale = (np.array(ref1) + np.array(ref2)) / 2.0, np.linalg.norm(np.array(ref1) - np.array(ref2))\n",
    "             if scale == 0 or np.isclose(scale, 0) or np.isnan(scale): normalized_frames.append(np.full(num_coords, np.nan)); continue\n",
    "             normalized_frames.append(((frame_joints - center) / scale).flatten())\n",
    "        except Exception: normalized_frames.append(np.full(num_coords, np.nan)); continue\n",
    "    return np.array(normalized_frames)\n",
    "\n",
    "print(\"Video processing and normalization functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ba25419a-86e7-432a-b88b-73417253f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5.2: Video Processing & Normalization Functions ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6b918-3731-4282-9b2e-e56b64ca606a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "25be27fb-18c3-45ed-8e2c-b6986855bc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory calculation helper defined (_calculate_trajectory_from_config).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5.2: Trajectory Calculation Helper ===\n",
    "# Requires numpy, get_keypoint, calculate_angle\n",
    "# Assumes JOINT_NAMES_LIST is defined\n",
    "\n",
    "def _calculate_trajectory_from_config(pose_sequence, joint_names, trajectory_config):\n",
    "    \"\"\"Calculates the 1D trajectory signal based on configuration.\"\"\"\n",
    "    if pose_sequence is None or pose_sequence.shape[0] < 2: return None, False\n",
    "\n",
    "    metric = trajectory_config.get('metric', 'unknown')\n",
    "    base_joint_names = trajectory_config.get('joints', [])\n",
    "    invert_for_valley = trajectory_config.get('invert_for_valley', False)\n",
    "    trajectory = None\n",
    "\n",
    "    try:\n",
    "        if not base_joint_names: raise ValueError(\"No 'joints' specified in trajectory_config\")\n",
    "\n",
    "        # --- Logic based on metric ---\n",
    "        if metric.endswith('_angle'):\n",
    "            if len(base_joint_names) != 3: raise ValueError(f\"{metric} needs 3 joints in config\")\n",
    "            j1_base, j2_base, j3_base = base_joint_names[0], base_joint_names[1], base_joint_names[2]\n",
    "            angles_left, angles_right = [], []\n",
    "            try: # Try finding Left/Right pairs first\n",
    "                l_j1_idx, l_j2_idx, l_j3_idx = joint_names.index(f'Left {j1_base}'), joint_names.index(f'Left {j2_base}'), joint_names.index(f'Left {j3_base}')\n",
    "                r_j1_idx, r_j2_idx, r_j3_idx = joint_names.index(f'Right {j1_base}'), joint_names.index(f'Right {j2_base}'), joint_names.index(f'Right {j3_base}')\n",
    "                for frame_coords in pose_sequence:\n",
    "                    f_j = frame_coords.reshape(-1, 2)\n",
    "                    p1l,p2l,p3l = get_keypoint(f_j,l_j1_idx), get_keypoint(f_j,l_j2_idx), get_keypoint(f_j,l_j3_idx)\n",
    "                    p1r,p2r,p3r = get_keypoint(f_j,r_j1_idx), get_keypoint(f_j,r_j2_idx), get_keypoint(f_j,r_j3_idx)\n",
    "                    angles_left.append(calculate_angle(p1l, p2l, p3l) if all(p is not None for p in [p1l, p2l, p3l]) else np.nan)\n",
    "                    angles_right.append(calculate_angle(p1r, p2r, p3r) if all(p is not None for p in [p1r, p2r, p3r]) else np.nan)\n",
    "                traj_l, traj_r = np.array(angles_left), np.array(angles_right)\n",
    "                trajectory = np.nanmean([traj_l, traj_r], axis=0) # Average left/right\n",
    "            except ValueError: # Fallback if L/R pairing doesn't exist\n",
    "                j1_idx, j2_idx, j3_idx = joint_names.index(j1_base), joint_names.index(j2_base), joint_names.index(j3_base)\n",
    "                angles = []\n",
    "                for frame_coords in pose_sequence:\n",
    "                     f_j = frame_coords.reshape(-1, 2)\n",
    "                     p1, p2, p3 = get_keypoint(f_j, j1_idx), get_keypoint(f_j, j2_idx), get_keypoint(f_j, j3_idx)\n",
    "                     angles.append(calculate_angle(p1, p2, p3) if all(p is not None for p in [p1, p2, p3]) else np.nan)\n",
    "                trajectory = np.array(angles)\n",
    "\n",
    "        elif metric.endswith('_y') or metric.endswith('_x'): # Coordinate metric\n",
    "            coord_idx = 1 if metric.endswith('_y') else 0 # 1 for Y, 0 for X\n",
    "            base_joint = base_joint_names[0]\n",
    "            try: # Try L/R average\n",
    "                 l_joint_idx = joint_names.index(f'Left {base_joint}')\n",
    "                 r_joint_idx = joint_names.index(f'Right {base_joint}')\n",
    "                 # Check if indices are valid before accessing pose_sequence\n",
    "                 if max(2*l_joint_idx+coord_idx, 2*r_joint_idx+coord_idx) >= pose_sequence.shape[1]: raise ValueError(\"Joint index out of bounds\")\n",
    "                 l_coord = pose_sequence[:, 2 * l_joint_idx + coord_idx]\n",
    "                 r_coord = pose_sequence[:, 2 * r_joint_idx + coord_idx]\n",
    "                 trajectory = np.nanmean([l_coord, r_coord], axis=0)\n",
    "            except ValueError: # Fallback to single joint coord\n",
    "                 joint_idx = joint_names.index(base_joint)\n",
    "                 if 2*joint_idx+coord_idx >= pose_sequence.shape[1]: raise ValueError(\"Joint index out of bounds\")\n",
    "                 trajectory = pose_sequence[:, 2 * joint_idx + coord_idx]\n",
    "        else:\n",
    "             raise ValueError(f\"Unknown metric type specified in config: '{metric}'\")\n",
    "\n",
    "        # --- Post-processing trajectory ---\n",
    "        if trajectory is None or trajectory.shape[0] == 0 or np.all(np.isnan(trajectory)): return None, False\n",
    "        nan_mask = np.isnan(trajectory)\n",
    "        if np.any(nan_mask):\n",
    "            indices = np.arange(len(trajectory)); valid_indices = np.flatnonzero(~nan_mask)\n",
    "            if len(valid_indices) < 2: return None, False # Cannot interpolate\n",
    "            trajectory[nan_mask] = np.interp(indices[nan_mask], valid_indices, trajectory[valid_indices])\n",
    "        if np.any(np.isnan(trajectory)): trajectory = np.nan_to_num(trajectory, nan=np.nanmean(trajectory)) # Fill with mean as fallback\n",
    "\n",
    "        return trajectory, invert_for_valley\n",
    "\n",
    "    except (ValueError, IndexError) as e: print(f\"W: Error finding/using joints for metric '{metric}': {e}\"); return None, False\n",
    "    except Exception as e: print(f\"E: Error calculating trajectory with metric '{metric}': {e}\"); return None, False\n",
    "\n",
    "print(\"Trajectory calculation helper defined (_calculate_trajectory_from_config).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c912efeb-3fc3-4136-a572-32c9a3a77aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5.3: Trajectory Calculation Function ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f561e4f-bf8c-4c59-8762-f75e6ce73d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2b152959-12e8-47b1-b522-cd1dfdd160dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak finding helper function defined (_extract_trajectory_and_find_peaks).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5.3: Peak Finding Helper Function ===\n",
    "# Requires numpy, scipy.signal.find_peaks\n",
    "\n",
    "def _extract_trajectory_and_find_peaks(trajectory, smoothing_window, prominence, distance, invert_trajectory=False):\n",
    "    \"\"\"Internal helper to smooth a trajectory and find peaks.\"\"\"\n",
    "    if trajectory is None or trajectory.ndim != 1 or trajectory.shape[0] < max(distance * 2, smoothing_window, 1): return None # Added 1 for edge case\n",
    "    processed_trajectory = trajectory.copy()\n",
    "    if invert_trajectory: processed_trajectory = -processed_trajectory\n",
    "    if len(processed_trajectory) < smoothing_window: return None # Cannot smooth\n",
    "\n",
    "    # Use valid mode for convolution, check length after smoothing\n",
    "    smoothed_trajectory = np.convolve(processed_trajectory, np.ones(smoothing_window)/smoothing_window, mode='valid')\n",
    "    if smoothed_trajectory.shape[0] < max(distance, 1): return None # Need enough points for distance check in find_peaks\n",
    "\n",
    "    data_range = np.ptp(smoothed_trajectory)\n",
    "    required_prominence = max(data_range * prominence, 1e-6) if data_range > 1e-9 else 1e-6\n",
    "\n",
    "    try:\n",
    "        # Calculate offset correctly for 'valid' mode convolution\n",
    "        smoothed_indices_offset = (len(processed_trajectory) - len(smoothed_trajectory)) // 2\n",
    "        peak_indices_smoothed, _ = find_peaks(smoothed_trajectory, prominence=required_prominence, distance=distance)\n",
    "        peak_indices_original = peak_indices_smoothed + smoothed_indices_offset\n",
    "        # Ensure indices are within bounds of original trajectory\n",
    "        peak_indices_original = peak_indices_original[(peak_indices_original >= 0) & (peak_indices_original < len(processed_trajectory))]\n",
    "        return peak_indices_original\n",
    "    except Exception as e: print(f\"W: Peak finding failed: {e}\"); return None\n",
    "\n",
    "print(\"Peak finding helper function defined (_extract_trajectory_and_find_peaks).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "649f8394-8b81-4cd4-912b-b0e1d2aef3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5.4: Peak Finding Helper Function ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c1782-40e9-47eb-aaa1-8fa1d19d027a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e10142d6-6914-452f-8810-822e4b2a5d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation and counting functions defined (using EXERCISE_CONFIG).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5.4: Segmentation & Counting Functions ===\n",
    "# Requires _calculate_trajectory_from_config, _extract_trajectory_and_find_peaks\n",
    "# Assumes constants like DEFAULT_SMOOTHING_WINDOW etc. are defined\n",
    "\n",
    "def segment_repetitions(pose_sequence, exercise_label, joint_names, exercise_config, # Pass full config dict\n",
    "                        smoothing_window=DEFAULT_SMOOTHING_WINDOW,\n",
    "                        peak_prominence=DEFAULT_PEAK_PROMINENCE,\n",
    "                        peak_distance=DEFAULT_PEAK_DISTANCE):\n",
    "    \"\"\"Segments a pose sequence into individual repetitions using EXERCISE_CONFIG.\"\"\"\n",
    "    # Get specific config for this exercise\n",
    "    current_exercise_cfg_dict = exercise_config.get(exercise_label)\n",
    "    # If no specific config, maybe try a default or skip? For now, skip.\n",
    "    if not current_exercise_cfg_dict:\n",
    "        # print(f\"W: No config found for {exercise_label} in segment_repetitions\") # Optional Warning\n",
    "        return []\n",
    "    trajectory_logic_cfg = current_exercise_cfg_dict.get('trajectory_logic')\n",
    "    if not trajectory_logic_cfg:\n",
    "         # print(f\"W: No trajectory_logic found in config for {exercise_label}\") # Optional Warning\n",
    "         return []\n",
    "\n",
    "    # 1. Calculate trajectory based on config\n",
    "    trajectory, invert = _calculate_trajectory_from_config(pose_sequence, joint_names, trajectory_logic_cfg)\n",
    "\n",
    "    # 2. Find peaks using specified or default parameters\n",
    "    peak_indices = _extract_trajectory_and_find_peaks(trajectory, smoothing_window, peak_prominence, peak_distance, invert_trajectory=invert)\n",
    "\n",
    "    if peak_indices is None or len(peak_indices) < 2: return []\n",
    "\n",
    "    # 3. Segment the original pose_sequence based on peak indices\n",
    "    repetitions = [pose_sequence[peak_indices[i]:peak_indices[i+1], :] for i in range(len(peak_indices) - 1)]\n",
    "    return [rep for rep in repetitions if rep.shape[0] > 1] # Filter out tiny segments\n",
    "\n",
    "def count_repetitions(pose_sequence, exercise_label, joint_names, exercise_config, # Pass full config dict\n",
    "                      prominence=DEFAULT_PEAK_PROMINENCE, distance=DEFAULT_PEAK_DISTANCE, # Use potentially tuned params\n",
    "                      smoothing_window=DEFAULT_SMOOTHING_WINDOW):\n",
    "    \"\"\"Counts repetitions using EXERCISE_CONFIG and specified parameters.\"\"\"\n",
    "    current_exercise_cfg_dict = exercise_config.get(exercise_label)\n",
    "    if not current_exercise_cfg_dict: return 0 # Cannot count without config\n",
    "    trajectory_logic_cfg = current_exercise_cfg_dict.get('trajectory_logic')\n",
    "    if not trajectory_logic_cfg: return 0 # Cannot count without trajectory logic\n",
    "\n",
    "    # 1. Calculate trajectory based on config\n",
    "    trajectory, invert = _calculate_trajectory_from_config(pose_sequence, joint_names, trajectory_logic_cfg)\n",
    "\n",
    "    # 2. Find peaks using specified parameters\n",
    "    peak_indices = _extract_trajectory_and_find_peaks(trajectory, smoothing_window, prominence, distance, invert_trajectory=invert)\n",
    "\n",
    "    return len(peak_indices) if peak_indices is not None else 0\n",
    "\n",
    "print(\"Segmentation and counting functions defined (using EXERCISE_CONFIG).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "170af1a4-c0ed-4503-9146-d4ef1c9048e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5.5: Segmentation & Counting Functions ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fdbf6f-0c72-4b87-b129-48d33f186d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "919a4661-cf62-4357-bd46-e52e0fd58ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter tuning function defined (tune_counting_parameters).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5.5: Parameter Tuning Function ===\n",
    "# Requires _calculate_trajectory_from_config, _extract_trajectory_and_find_peaks\n",
    "# Assumes constants like DEFAULT_PEAK_PROMINENCE etc. are defined\n",
    "\n",
    "def tune_counting_parameters(exercise_label, labeled_video_data, joint_names, exercise_config, # Pass full config dict\n",
    "                             prominence_range, distance_range):\n",
    "    \"\"\"Performs grid search using the refactored peak finding and config.\"\"\"\n",
    "    print(f\"  Tuning repetition counting parameters for: {exercise_label}...\")\n",
    "    best_params = (DEFAULT_PEAK_PROMINENCE, DEFAULT_PEAK_DISTANCE)\n",
    "    min_total_error = float('inf')\n",
    "    if not labeled_video_data: print(\"W: No labeled data for tuning.\"); return best_params\n",
    "\n",
    "    current_exercise_cfg_dict = exercise_config.get(exercise_label)\n",
    "    if not current_exercise_cfg_dict: print(f\"W: No config for {exercise_label} in tuning\"); return best_params\n",
    "    trajectory_logic_cfg = current_exercise_cfg_dict.get('trajectory_logic')\n",
    "    if not trajectory_logic_cfg: print(f\"W: No trajectory_logic in config for {exercise_label}\"); return best_params\n",
    "\n",
    "    num_combinations = len(prominence_range) * len(distance_range)\n",
    "    print(f\"    Testing {num_combinations} parameter combinations...\")\n",
    "    default_error = 0\n",
    "    precalculated_trajectories = {}\n",
    "\n",
    "    # Pre-calculate trajectories\n",
    "    for i, (vid_idx, (sequence, true_reps)) in enumerate(labeled_video_data): # Expecting (original_video_idx, (sequence, true_reps))\n",
    "        trajectory, invert = _calculate_trajectory_from_config(sequence, joint_names, trajectory_logic_cfg)\n",
    "        if trajectory is not None: precalculated_trajectories[vid_idx] = (trajectory, invert, true_reps) # Use video_idx as key\n",
    "\n",
    "    if not precalculated_trajectories: print(\"W: No valid trajectories for tuning.\"); return best_params\n",
    "\n",
    "    # Calculate default error\n",
    "    for vid_idx, (traj, inv, true_r) in precalculated_trajectories.items():\n",
    "         peak_indices_def = _extract_trajectory_and_find_peaks(traj, DEFAULT_SMOOTHING_WINDOW, DEFAULT_PEAK_PROMINENCE, DEFAULT_PEAK_DISTANCE, invert_trajectory=inv)\n",
    "         calc_reps_def = len(peak_indices_def) if peak_indices_def is not None else 0\n",
    "         default_error += abs(calc_reps_def - true_r)\n",
    "\n",
    "    # Grid search\n",
    "    for p_idx, p in enumerate(prominence_range):\n",
    "        for d_idx, d in enumerate(distance_range):\n",
    "            current_total_error = 0\n",
    "            for vid_idx, (traj, inv, true_r) in precalculated_trajectories.items():\n",
    "                peak_indices = _extract_trajectory_and_find_peaks(traj, DEFAULT_SMOOTHING_WINDOW, p, d, invert_trajectory=inv)\n",
    "                calculated_reps = len(peak_indices) if peak_indices is not None else 0\n",
    "                current_total_error += abs(calculated_reps - true_r)\n",
    "            if current_total_error < min_total_error:\n",
    "                min_total_error = current_total_error\n",
    "                best_params = (p, d)\n",
    "            # Stop early if perfect match found (error=0)\n",
    "            if current_total_error == 0: break\n",
    "        if current_total_error == 0: break\n",
    "\n",
    "\n",
    "    print(f\"    Tuning complete. Best Params: P={best_params[0]:.3f}, D={best_params[1]}, Min Error={min_total_error}. (Default Error={default_error})\")\n",
    "\n",
    "    # Print detected reps with best params\n",
    "    if precalculated_trajectories:\n",
    "        print(f\"    Detected reps using BEST params (P={best_params[0]:.3f}, D={best_params[1]}):\")\n",
    "        total_calculated, total_true = 0, 0\n",
    "        # Sort by video index for consistent output\n",
    "        for vid_idx, (traj, inv, true_r) in sorted(precalculated_trajectories.items()):\n",
    "            peak_indices = _extract_trajectory_and_find_peaks(traj, DEFAULT_SMOOTHING_WINDOW, best_params[0], best_params[1], invert_trajectory=inv)\n",
    "            calculated_reps = len(peak_indices) if peak_indices is not None else 0\n",
    "            print(f\"      - Video Index {vid_idx}: Detected={calculated_reps}, True={true_r}\")\n",
    "            total_calculated += calculated_reps; total_true += true_r\n",
    "        print(f\"    -------------------------------------\")\n",
    "        print(f\"    Total Reps: Detected={total_calculated}, True={total_true}, Overall Error={abs(total_calculated - total_true)}\")\n",
    "\n",
    "    return best_params\n",
    "\n",
    "print(\"Parameter tuning function defined (tune_counting_parameters).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1b5d8bc6-43ec-4e56-907f-e81e206bfca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5.6: Parameter Tuning Function ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b8fa8-e43b-405f-a725-fd497c6d0f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "490a07ca-fc97-407b-a303-cd061939ab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " === All Core Processing Functions Defined (Config-Driven, Angle-Aware) === \n"
     ]
    }
   ],
   "source": [
    "# === Cell 5.6: Baseline Loading Function ===\n",
    "# Requires all helpers, processing functions, tuning function defined above\n",
    "# Assumes constants like BASELINE_ROOT_DIR etc. are defined\n",
    "\n",
    "def load_baseline_data(baseline_root_dir, hrnet_model, joint_names, target_rep_length, exercise_config, # Pass config\n",
    "                       prominence_range, distance_range, # Pass tuning ranges\n",
    "                       threshold_std_multiplier=DEFAULT_THRESHOLD_STD_MULTIPLIER,\n",
    "                       min_reps_for_threshold=DEFAULT_MIN_REPS_FOR_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Loads baselines, uses EXERCISE_CONFIG, parses angle from filename,\n",
    "    tunes params per angle, averages per angle AND overall, calculates thresholds.\n",
    "    Returns nested dicts: averaged_baselines, tuned_counting_params, exercise_specific_thresholds\n",
    "    \"\"\"\n",
    "    averaged_baseline_data = {} # Structure: {exercise: {angle: avg_sequence}}\n",
    "    tuned_counting_params = {}  # Structure: {exercise: {angle: (p, d)}}\n",
    "    exercise_specific_thresholds = {} # Structure: {exercise: {angle: threshold}}\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    print(f\"Loading Baselines (Config-Driven, Angle from Filename)... from: {baseline_root_dir}\")\n",
    "    if not os.path.isdir(baseline_root_dir): return averaged_baseline_data, tuned_counting_params, exercise_specific_thresholds\n",
    "\n",
    "    for exercise_label_raw in sorted(os.listdir(baseline_root_dir)):\n",
    "        exercise_folder_path = os.path.join(baseline_root_dir, exercise_label_raw)\n",
    "        if not os.path.isdir(exercise_folder_path): continue\n",
    "        exercise_label = exercise_label_raw.lower().replace(\" \", \"_\")\n",
    "        print(f\"\\nProcessing baseline exercise: {exercise_label} (from folder: {exercise_label_raw})\")\n",
    "\n",
    "        current_exercise_config_dict = exercise_config.get(exercise_label)\n",
    "        if not current_exercise_config_dict:\n",
    "             print(f\"  W: No config found in EXERCISE_CONFIG for '{exercise_label}'. Skipping this exercise.\")\n",
    "             continue # Skip to the next exercise if no config defined\n",
    "\n",
    "        all_reps_by_angle = {'left': [], 'right': [], 'front': [], 'unknown': []}\n",
    "        # Store as {angle: [(vid_idx, (sequence, true_reps)), ...]}\n",
    "        labeled_data_by_angle = {'left': [], 'right': [], 'front': [], 'unknown': []}\n",
    "        averaged_baseline_data[exercise_label] = {}\n",
    "        tuned_counting_params[exercise_label] = {}\n",
    "        exercise_specific_thresholds[exercise_label] = {}\n",
    "        video_files = sorted([f for f in os.listdir(exercise_folder_path) if f.lower().endswith(('.mp4', '.avi', '.mov'))])\n",
    "        print(f\"  Found {len(video_files)} video file(s).\")\n",
    "\n",
    "        # Loop 1: Process videos, parse angle/reps, segment, normalize time, store reps by PARSED angle\n",
    "        for video_idx, video_file in enumerate(video_files):\n",
    "            video_path = os.path.join(exercise_folder_path, video_file)\n",
    "            true_reps, parsed_angle = None, 'unknown'\n",
    "            rep_match = re.search(r'_(\\d+)reps', video_file, re.IGNORECASE)\n",
    "            if rep_match: true_reps = int(rep_match.group(1))\n",
    "            angle_match = re.search(r'_(left|right|front)', video_file, re.IGNORECASE)\n",
    "            if angle_match: parsed_angle = angle_match.group(1).lower()\n",
    "            # print(f\"    Video {video_idx+1}: {video_file}, Reps={true_reps}, Angle={parsed_angle}\") # Verbose\n",
    "\n",
    "            raw_seq = process_single_video(video_path, hrnet_model)\n",
    "            if raw_seq is None or raw_seq.shape[0] == 0: continue\n",
    "            spatially_norm_seq = normalize_pose_sequence(raw_seq, joint_names)\n",
    "            if spatially_norm_seq is None or np.all(np.isnan(spatially_norm_seq)): continue\n",
    "            cleaned_seq = handle_nan_values(spatially_norm_seq)\n",
    "            if cleaned_seq.shape[0] < 2: continue\n",
    "            if true_reps is not None:\n",
    "                 # Store original video index for reference in tuning output\n",
    "                 labeled_data_by_angle[parsed_angle].append((video_idx, (cleaned_seq, true_reps)))\n",
    "#####\n",
    "            \n",
    "            # Segment using default params initially for consistent rep extraction for averaging\n",
    "            # Pass the specific config dict now\n",
    "            repetitions = segment_repetitions(cleaned_seq, exercise_label, joint_names, current_exercise_config_dict) # ADDED argument\n",
    "            if not repetitions: continue\n",
    "            for rep_segment in repetitions:\n",
    "                if rep_segment is not None and rep_segment.shape[0] >= 2:\n",
    "                    time_norm_rep = time_normalize_sequence(rep_segment, target_rep_length)\n",
    "                    # Check shape consistency (all should have target_rep_length * num_coords)\n",
    "                    expected_coords = len(joint_names) * 2\n",
    "                    if not np.isnan(time_norm_rep).all() and time_norm_rep.shape == (target_rep_length, expected_coords):\n",
    "                        all_reps_by_angle[parsed_angle].append(time_norm_rep)\n",
    "                    # else: print(f\"W: Skipping rep segment due to NaN/shape mismatch after time norm. Shape: {time_norm_rep.shape}\") # Verbose\n",
    "\n",
    "\n",
    "        # Loop 2: Process per detected angle category\n",
    "        all_reps_across_angles = []\n",
    "        for angle in ['left', 'right', 'front', 'unknown']:\n",
    "            # Get data for tuning, pass only (sequence, true_reps) tuples\n",
    "            labeled_data_for_this_angle_tuples = [data for idx, data in labeled_data_by_angle[angle]]\n",
    "            normalized_reps_for_this_angle = all_reps_by_angle[angle]\n",
    "            if not normalized_reps_for_this_angle: continue # Skip if no reps for this angle\n",
    "            print(f\"\\n  Processing angle category: '{angle}' for {exercise_label}\")\n",
    "            all_reps_across_angles.extend(normalized_reps_for_this_angle)\n",
    "\n",
    "            # Tune parameters for this angle if labeled data exists\n",
    "            if labeled_data_for_this_angle_tuples:\n",
    "                 tuned_p, tuned_d = tune_counting_parameters(exercise_label, labeled_data_by_angle[angle], # Pass original list with indices\n",
    "                                                            joint_names, exercise_config, prominence_range, distance_range)\n",
    "                 tuned_counting_params[exercise_label][angle] = (tuned_p, tuned_d)\n",
    "            else: tuned_counting_params[exercise_label][angle] = (DEFAULT_PEAK_PROMINENCE, DEFAULT_PEAK_DISTANCE)\n",
    "\n",
    "            # Average sequences for this angle\n",
    "            reps_stack = np.stack(normalized_reps_for_this_angle, axis=0)\n",
    "            averaged_sequence_angle = np.nanmean(reps_stack, axis=0)\n",
    "            averaged_baseline_data[exercise_label][angle] = np.nan_to_num(averaged_sequence_angle, nan=0.0)\n",
    "            print(f\"    Stored averaged baseline for angle '{angle}'.\")\n",
    "\n",
    "            # Derive threshold for this angle\n",
    "            derived_threshold_angle = None\n",
    "            if len(normalized_reps_for_this_angle) >= min_reps_for_threshold:\n",
    "                intra_distances_angle = []\n",
    "                safe_avg_angle = np.nan_to_num(averaged_sequence_angle, nan=0.0)\n",
    "                for norm_rep in normalized_reps_for_this_angle:\n",
    "                     safe_rep = np.nan_to_num(norm_rep, nan=0.0)\n",
    "                     if safe_rep.shape == safe_avg_angle.shape:\n",
    "                         try: dist, _ = fastdtw(safe_rep, safe_avg_angle, dist=euclidean); intra_distances_angle.append(dist)\n",
    "                         except Exception: pass # Ignore DTW errors during threshold calc? Or handle?\n",
    "                # Check if enough valid distances were calculated\n",
    "                if len(intra_distances_angle) >= min_reps_for_threshold:\n",
    "                     mean_dist, std_dist = np.mean(intra_distances_angle), np.std(intra_distances_angle)\n",
    "                     derived_threshold_angle = mean_dist + threshold_std_multiplier * std_dist\n",
    "                     print(f\"    Derived Threshold for angle '{angle}': {derived_threshold_angle:.2f} (Mean={mean_dist:.2f}, Std={std_dist:.2f})\")\n",
    "                # else: print(f\"W: Not enough valid distances ({len(intra_distances_angle)}) calculated for threshold angle '{angle}'.\") # Verbose\n",
    "            exercise_specific_thresholds[exercise_label][angle] = derived_threshold_angle\n",
    "\n",
    "        # Calculate and Store Overall Average and Threshold\n",
    "        print(f\"\\n  Processing 'overall' category for {exercise_label}\")\n",
    "        if len(all_reps_across_angles) >= min_reps_for_threshold:\n",
    "             print(f\"    Averaging {len(all_reps_across_angles)} total repetitions for 'overall' baseline.\")\n",
    "             overall_reps_stack = np.stack(all_reps_across_angles, axis=0)\n",
    "             overall_averaged_sequence = np.nanmean(overall_reps_stack, axis=0)\n",
    "             averaged_baseline_data[exercise_label]['overall'] = np.nan_to_num(overall_averaged_sequence, nan=0.0)\n",
    "             print(f\"    Stored 'overall' averaged baseline.\")\n",
    "\n",
    "             overall_intra_distances = []\n",
    "             safe_overall_avg = np.nan_to_num(overall_averaged_sequence, nan=0.0)\n",
    "             for norm_rep in all_reps_across_angles:\n",
    "                  safe_rep = np.nan_to_num(norm_rep, nan=0.0)\n",
    "                  if safe_rep.shape == safe_overall_avg.shape:\n",
    "                       try: dist, _ = fastdtw(safe_rep, safe_overall_avg, dist=euclidean); overall_intra_distances.append(dist)\n",
    "                       except Exception: pass\n",
    "             if len(overall_intra_distances) >= min_reps_for_threshold:\n",
    "                  mean_dist, std_dist = np.mean(overall_intra_distances), np.std(overall_intra_distances)\n",
    "                  derived_threshold_overall = mean_dist + threshold_std_multiplier * std_dist\n",
    "                  exercise_specific_thresholds[exercise_label]['overall'] = derived_threshold_overall\n",
    "                  print(f\"    Derived 'Overall' Threshold: {derived_threshold_overall:.2f} (Mean={mean_dist:.2f}, Std={std_dist:.2f})\")\n",
    "             else: exercise_specific_thresholds[exercise_label]['overall'] = None\n",
    "        else:\n",
    "             print(f\"    W: Not enough total valid reps ({len(all_reps_across_angles)}) to create 'overall' baseline/threshold.\")\n",
    "             averaged_baseline_data[exercise_label]['overall'] = None\n",
    "             exercise_specific_thresholds[exercise_label]['overall'] = None\n",
    "\n",
    "    end_time_total = time.time()\n",
    "    print(f\"\\nBaseline processing complete in {end_time_total - start_time_total:.2f}s.\")\n",
    "    return averaged_baseline_data, tuned_counting_params, exercise_specific_thresholds\n",
    "\n",
    "\n",
    "# --- Recognition Function (Using Config, Angle Hint or Overall) ---\n",
    "def recognize_exercise(new_video_path, averaged_baseline_data, hrnet_model, joint_names, exercise_config, # Pass config\n",
    "                       target_rep_length, tuned_counting_params, exercise_specific_thresholds,\n",
    "                       hint_angle=None, fallback_threshold=DTW_DISTANCE_THRESHOLD):\n",
    "    \"\"\"Recognizes exercise using angle hint, config-driven logic, angle-specific baselines/thresholds.\"\"\"\n",
    "    print(f\"\\n--- Recognizing Exercise (Angle Hint='{hint_angle}') for: {os.path.basename(new_video_path)} ---\")\n",
    "    start_time = time.time()\n",
    "    if tuned_counting_params is None: tuned_counting_params = {}\n",
    "    if exercise_specific_thresholds is None: exercise_specific_thresholds = {}\n",
    "    default_p, default_d = DEFAULT_PEAK_PROMINENCE, DEFAULT_PEAK_DISTANCE\n",
    "\n",
    "    # Step 1: Process Video -> Spatial Norm -> Clean\n",
    "    new_raw_seq = process_single_video(new_video_path, hrnet_model)\n",
    "    if new_raw_seq is None or new_raw_seq.shape[0] == 0: print(\"E: Failed processing video.\"); return [], 0\n",
    "    new_spatially_norm_seq = normalize_pose_sequence(new_raw_seq, joint_names)\n",
    "    if new_spatially_norm_seq is None or np.all(np.isnan(new_spatially_norm_seq)): print(\"E: Failed spatial normalization.\"); return [], 0\n",
    "    new_sequence_clean = handle_nan_values(new_spatially_norm_seq)\n",
    "    if new_sequence_clean.shape[0] < 2: print(\"W: Sequence too short after cleaning.\"); return [], 0\n",
    "\n",
    "    # Step 2: Time-Normalize WHOLE Cleaned Sequence for Initial Guess\n",
    "    new_sequence_time_norm = time_normalize_sequence(new_sequence_clean, target_rep_length)\n",
    "    if np.isnan(new_sequence_time_norm).all(): print(\"E: Time normalization failed for whole sequence.\"); return [], 0\n",
    "    new_sequence_time_norm = np.nan_to_num(new_sequence_time_norm, nan=0.0)\n",
    "\n",
    "    # Step 3: Initial Guess (using 'overall' baselines) & Parameter Selection\n",
    "    initial_min_distance, initial_best_label = float('inf'), None\n",
    "    segment_p, segment_d = default_p, default_d\n",
    "    segment_exercise_config_dict = None\n",
    "\n",
    "    if not averaged_baseline_data: print(\"E: No baseline data.\"); return [], 0\n",
    "    # print(\"  Performing initial comparison against 'overall' baselines...\") # Verbose\n",
    "    for ex_label, angle_dict in averaged_baseline_data.items():\n",
    "        avg_base_seq = angle_dict.get('overall')\n",
    "        if avg_base_seq is None or new_sequence_time_norm.shape != avg_base_seq.shape: continue\n",
    "        safe_base = np.nan_to_num(avg_base_seq, nan=0.0)\n",
    "        try: dist, _ = fastdtw(new_sequence_time_norm, safe_base, dist=euclidean)\n",
    "        except Exception: continue\n",
    "        if dist < initial_min_distance: initial_min_distance, initial_best_label = dist, ex_label\n",
    "\n",
    "    # Select segmentation parameters\n",
    "    angle_for_tuning_lookup = hint_angle if hint_angle in ['left', 'right', 'front'] else 'front'\n",
    "    if initial_best_label:\n",
    "        segment_exercise_config_dict = exercise_config.get(initial_best_label)\n",
    "        if segment_exercise_config_dict:\n",
    "             tuned_p, tuned_d = tuned_counting_params.get(initial_best_label, {}).get(angle_for_tuning_lookup, (default_p, default_d))\n",
    "             segment_p, segment_d = tuned_p, tuned_d\n",
    "             # print(f\"  Initial guess: {initial_best_label}. Using its '{angle_for_tuning_lookup}' tuned params for segmentation (P={segment_p:.3f}, D={segment_d}).\") # Verbose\n",
    "        else: initial_best_label = None # Reset guess if no config\n",
    "    # else: print(f\"  Initial guess failed/inconclusive. Using default params for segmentation.\") # Verbose\n",
    "\n",
    "    # Step 4: Segment Test Video\n",
    "    # Pass label guess (or unknown) and the full config dict\n",
    "    segment_label_for_logic = initial_best_label if initial_best_label else \"unknown\"\n",
    "    test_repetitions = segment_repetitions(new_sequence_clean, segment_label_for_logic, joint_names, exercise_config,\n",
    "                                           peak_prominence=segment_p, peak_distance=segment_d)\n",
    "    num_detected_reps = len(test_repetitions)\n",
    "    print(f\"  Detected {num_detected_reps} potential repetitions in test video.\")\n",
    "    if num_detected_reps == 0: return [], 0\n",
    "\n",
    "    # Step 5: Compare Each Repetition Segment (Angle-Aware)\n",
    "    repetition_results = []\n",
    "    comparison_angle_key = hint_angle if hint_angle in ['left', 'right', 'front'] else 'overall'\n",
    "    print(f\"  Comparing each detected repetition against '{comparison_angle_key}' baselines...\")\n",
    "\n",
    "    for i, rep_segment in enumerate(test_repetitions):\n",
    "        # 5a. Time-Normalize\n",
    "        if rep_segment is None or rep_segment.shape[0] < 2: repetition_results.append( (i+1, \"Error: Invalid Segment\", float('inf')) ); continue\n",
    "        time_norm_rep_segment = time_normalize_sequence(rep_segment, target_rep_length)\n",
    "        if np.isnan(time_norm_rep_segment).all(): repetition_results.append( (i+1, \"Error: Time Norm Failed\", float('inf')) ); continue\n",
    "        time_norm_rep_segment = np.nan_to_num(time_norm_rep_segment, nan=0.0)\n",
    "\n",
    "        # 5b. Compare rep against baselines matching the comparison_angle_key\n",
    "        min_rep_distance, best_rep_label_for_this_rep = float('inf'), None\n",
    "        for exercise_label, angle_dict in averaged_baseline_data.items():\n",
    "            baseline_to_compare = angle_dict.get(comparison_angle_key)\n",
    "            if baseline_to_compare is None or time_norm_rep_segment.shape != baseline_to_compare.shape: continue\n",
    "            safe_baseline = np.nan_to_num(baseline_to_compare, nan=0.0)\n",
    "            try: distance, _ = fastdtw(time_norm_rep_segment, safe_baseline, dist=euclidean)\n",
    "            except Exception: continue\n",
    "            if distance < min_rep_distance: min_rep_distance, best_rep_label_for_this_rep = distance, exercise_label\n",
    "\n",
    "        # 5c. Apply threshold specific to the matched exercise and comparison_angle_key\n",
    "        matched_label_for_rep = \"No Match / Inconclusive\"\n",
    "        threshold_to_use = fallback_threshold\n",
    "        if best_rep_label_for_this_rep:\n",
    "            threshold_specific = exercise_specific_thresholds.get(best_rep_label_for_this_rep, {}).get(comparison_angle_key)\n",
    "            if threshold_specific is None and comparison_angle_key != 'overall':\n",
    "                 threshold_specific = exercise_specific_thresholds.get(best_rep_label_for_this_rep, {}).get('overall')\n",
    "            if threshold_specific is not None: threshold_to_use = threshold_specific\n",
    "            if min_rep_distance < threshold_to_use: matched_label_for_rep = best_rep_label_for_this_rep\n",
    "            else: matched_label_for_rep = f\"No Match (Closest: {best_rep_label_for_this_rep}, Dist: {min_rep_distance:.2f} >= {threshold_to_use:.2f})\"\n",
    "        repetition_results.append( (i+1, matched_label_for_rep, min_rep_distance) )\n",
    "\n",
    "    # Step 6: Return Results\n",
    "    end_time = time.time()\n",
    "    print(f\"--- Recognition finished in {end_time - start_time:.2f}s ---\")\n",
    "    return repetition_results, num_detected_reps\n",
    "\n",
    "print(\"\\n === All Core Processing Functions Defined (Config-Driven, Angle-Aware) === \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "904f8bc9-6fdd-4dcf-891d-de96bdb6d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5.7: Baseline Loading Function ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d356a-80ab-4d31-9a4e-932d20c16682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "bc5b2dbf-6848-4186-ba5b-b343eacf3c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline loading function updated (load_baseline_data uses angle from filename).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5.7: Baseline Loading Function (Updated for Overall Average) ===\n",
    "# === Cell 5.7: Baseline Loading Function (Using Angle from Filename) ===\n",
    "import re # Make sure re is imported\n",
    "\n",
    "def load_baseline_data(baseline_root_dir, hrnet_model, joint_names, target_rep_length, exercise_config,\n",
    "                       prominence_range, distance_range, # These 7 are positional\n",
    "                       threshold_std_multiplier=DEFAULT_THRESHOLD_STD_MULTIPLIER, # Keyword 1\n",
    "                       min_reps_for_threshold=DEFAULT_MIN_REPS_FOR_THRESHOLD    # Keyword 2\n",
    "                       ):\n",
    "    \"\"\"\n",
    "    Loads baselines, parses angle from filename, tunes params per angle,\n",
    "    averages per angle AND overall, calculates thresholds per angle AND overall.\n",
    "    Returns nested dicts: averaged_baselines, tuned_counting_params, exercise_specific_thresholds\n",
    "    \"\"\"\n",
    "    # Initialize nested dictionaries\n",
    "    averaged_baseline_data = {} # Structure: {exercise: {angle: avg_sequence}}\n",
    "    tuned_counting_params = {}  # Structure: {exercise: {angle: (p, d)}}\n",
    "    exercise_specific_thresholds = {} # Structure: {exercise: {angle: threshold}}\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    print(f\"Loading Baselines (using angle from filename)... from: {baseline_root_dir}\")\n",
    "    if not os.path.isdir(baseline_root_dir): return averaged_baseline_data, tuned_counting_params, exercise_specific_thresholds\n",
    "\n",
    "    for exercise_label in sorted(os.listdir(baseline_root_dir)):\n",
    "        exercise_folder_path = os.path.join(baseline_root_dir, exercise_label)\n",
    "        if not os.path.isdir(exercise_folder_path): continue\n",
    "        print(f\"\\nProcessing baseline exercise: {exercise_label}\")\n",
    "\n",
    "        # Initialize storage for this exercise (per angle)\n",
    "        # Use specific angle keys + 'unknown' + 'overall' later\n",
    "        all_reps_by_angle = {'left': [], 'right': [], 'front': [], 'unknown': []}\n",
    "        labeled_data_by_angle = {'left': [], 'right': [], 'front': [], 'unknown': []}\n",
    "        # Initialize result dicts for this exercise\n",
    "        averaged_baseline_data[exercise_label] = {}\n",
    "        tuned_counting_params[exercise_label] = {}\n",
    "        exercise_specific_thresholds[exercise_label] = {}\n",
    "\n",
    "        video_files = sorted([f for f in os.listdir(exercise_folder_path) if f.lower().endswith(('.mp4', '.avi', '.mov'))])\n",
    "        print(f\"  Found {len(video_files)} video file(s).\")\n",
    "\n",
    "        # --- Loop 1: Process videos, parse angle/reps, segment, normalize time, store reps by PARSED angle ---\n",
    "        for video_file in video_files:\n",
    "            video_path = os.path.join(exercise_folder_path, video_file)\n",
    "\n",
    "            # --- Parse True Rep Count ---\n",
    "            true_reps = None\n",
    "            rep_match = re.search(r'_(\\d+)reps', video_file, re.IGNORECASE)\n",
    "            if rep_match: true_reps = int(rep_match.group(1))\n",
    "\n",
    "            # --- Parse Angle ---\n",
    "            parsed_angle = 'unknown' # Default if not found\n",
    "            # Look for _left_, _right_, _front_ patterns (case-insensitive)\n",
    "            angle_match = re.search(r'_(left|right|front)', video_file, re.IGNORECASE)\n",
    "            if angle_match:\n",
    "                parsed_angle = angle_match.group(1).lower()\n",
    "            # else: print(f\"    Warning: Angle (left/right/front) not found in filename: {video_file}. Treating as 'unknown'.\")\n",
    "\n",
    "            print(f\"    Video: {video_file}, True Reps: {true_reps}, Parsed Angle: {parsed_angle}\")\n",
    "\n",
    "            # --- Process Video ---\n",
    "            raw_seq = process_single_video(video_path, hrnet_model, EXPECTED_JOINT_COUNT)\n",
    "            if raw_seq is None or raw_seq.shape[0] == 0: continue\n",
    "            spatially_norm_seq = normalize_pose_sequence(raw_seq, joint_names)\n",
    "            if spatially_norm_seq is None or np.all(np.isnan(spatially_norm_seq)): continue\n",
    "            cleaned_seq = handle_nan_values(spatially_norm_seq)\n",
    "            if cleaned_seq.shape[0] < 2: continue\n",
    "\n",
    "            # Store cleaned sequence and true reps for tuning (by parsed angle)\n",
    "            if true_reps is not None:\n",
    "                labeled_data_by_angle[parsed_angle].append((cleaned_seq, true_reps))\n",
    "\n",
    "            # Segment using default params\n",
    "            repetitions = segment_repetitions(cleaned_seq, exercise_label, joint_names, current_exercise_config_dict)\n",
    "            if not repetitions: continue\n",
    "\n",
    "            # Time normalize and store reps by parsed angle\n",
    "            for rep_segment in repetitions:\n",
    "                if rep_segment is not None and rep_segment.shape[0] >= 2:\n",
    "                    time_norm_rep = time_normalize_sequence(rep_segment, target_rep_length)\n",
    "                    if not np.isnan(time_norm_rep).all():\n",
    "                        all_reps_by_angle[parsed_angle].append(time_norm_rep)\n",
    "        # --- End Video Loop ---\n",
    "\n",
    "        # --- Loop 2: Process per detected angle category ---\n",
    "        all_reps_across_angles = [] # Collect all valid reps for overall average\n",
    "        valid_angles_processed = []\n",
    "\n",
    "        # Iterate through specific angles found + 'unknown'\n",
    "        for angle in ['left', 'right', 'front', 'unknown']:\n",
    "            labeled_data_for_this_angle = labeled_data_by_angle[angle]\n",
    "            normalized_reps_for_this_angle = all_reps_by_angle[angle]\n",
    "\n",
    "            if not normalized_reps_for_this_angle:\n",
    "                 # print(f\"  No valid repetitions found for angle category: '{angle}'.\") # Verbose\n",
    "                 continue # Skip tuning, averaging, threshold for this angle\n",
    "\n",
    "            print(f\"\\n  Processing angle category: '{angle}' for {exercise_label}\")\n",
    "            valid_angles_processed.append(angle)\n",
    "            all_reps_across_angles.extend(normalized_reps_for_this_angle) # Add to overall list\n",
    "\n",
    "            # Tune parameters for this angle\n",
    "            if labeled_data_for_this_angle:\n",
    "                tuned_p, tuned_d = tune_counting_parameters(exercise_label, labeled_data_for_this_angle, joint_names, prominence_range, distance_range)\n",
    "                tuned_counting_params[exercise_label][angle] = (tuned_p, tuned_d)\n",
    "            else: tuned_counting_params[exercise_label][angle] = (DEFAULT_PEAK_PROMINENCE, DEFAULT_PEAK_DISTANCE)\n",
    "\n",
    "            # Average sequences for this angle\n",
    "            reps_stack = np.stack(normalized_reps_for_this_angle, axis=0)\n",
    "            averaged_sequence_angle = np.nanmean(reps_stack, axis=0)\n",
    "            averaged_baseline_data[exercise_label][angle] = np.nan_to_num(averaged_sequence_angle, nan=0.0)\n",
    "            print(f\"    Stored averaged baseline for angle '{angle}'.\")\n",
    "\n",
    "            # Derive threshold for this angle\n",
    "            derived_threshold_angle = None\n",
    "            if len(normalized_reps_for_this_angle) >= min_reps_for_threshold:\n",
    "                intra_distances_angle = []\n",
    "                safe_avg_angle = np.nan_to_num(averaged_sequence_angle, nan=0.0)\n",
    "                for norm_rep in normalized_reps_for_this_angle:\n",
    "                     safe_rep = np.nan_to_num(norm_rep, nan=0.0)\n",
    "                     if safe_rep.shape == safe_avg_angle.shape:\n",
    "                         try: dist, _ = fastdtw(safe_rep, safe_avg_angle, dist=euclidean); intra_distances_angle.append(dist)\n",
    "                         except Exception: pass\n",
    "                if len(intra_distances_angle) >= min_reps_for_threshold:\n",
    "                     mean_dist, std_dist = np.mean(intra_distances_angle), np.std(intra_distances_angle)\n",
    "                     derived_threshold_angle = mean_dist + threshold_std_multiplier * std_dist\n",
    "                     print(f\"    Derived Threshold for angle '{angle}': {derived_threshold_angle:.2f}\")\n",
    "            exercise_specific_thresholds[exercise_label][angle] = derived_threshold_angle\n",
    "        # --- End Angle Loop ---\n",
    "\n",
    "        # --- Calculate and Store Overall Average and Threshold ---\n",
    "        print(f\"\\n  Processing 'overall' category for {exercise_label}\")\n",
    "        if len(all_reps_across_angles) >= min_reps_for_threshold:\n",
    "             print(f\"    Averaging {len(all_reps_across_angles)} total repetitions for 'overall' baseline.\")\n",
    "             overall_reps_stack = np.stack(all_reps_across_angles, axis=0)\n",
    "             overall_averaged_sequence = np.nanmean(overall_reps_stack, axis=0)\n",
    "             averaged_baseline_data[exercise_label]['overall'] = np.nan_to_num(overall_averaged_sequence, nan=0.0)\n",
    "             print(f\"    Stored 'overall' averaged baseline.\")\n",
    "\n",
    "             # Derive overall threshold\n",
    "             overall_intra_distances = []\n",
    "             safe_overall_avg = np.nan_to_num(overall_averaged_sequence, nan=0.0)\n",
    "             for norm_rep in all_reps_across_angles:\n",
    "                  safe_rep = np.nan_to_num(norm_rep, nan=0.0)\n",
    "                  if safe_rep.shape == safe_overall_avg.shape:\n",
    "                       try: dist, _ = fastdtw(safe_rep, safe_overall_avg, dist=euclidean); overall_intra_distances.append(dist)\n",
    "                       except Exception: pass\n",
    "             if len(overall_intra_distances) >= min_reps_for_threshold:\n",
    "                  mean_dist, std_dist = np.mean(overall_intra_distances), np.std(overall_intra_distances)\n",
    "                  derived_threshold_overall = mean_dist + threshold_std_multiplier * std_dist\n",
    "                  exercise_specific_thresholds[exercise_label]['overall'] = derived_threshold_overall\n",
    "                  print(f\"    Derived 'Overall' Threshold: {derived_threshold_overall:.2f}\")\n",
    "             else: exercise_specific_thresholds[exercise_label]['overall'] = None\n",
    "        else:\n",
    "             print(f\"    Warning: Not enough total valid reps ({len(all_reps_across_angles)}) to create 'overall' baseline/threshold.\")\n",
    "             averaged_baseline_data[exercise_label]['overall'] = None\n",
    "             exercise_specific_thresholds[exercise_label]['overall'] = None\n",
    "        # --- End Overall Calculation ---\n",
    "\n",
    "    # --- Function Return ---\n",
    "    end_time_total = time.time()\n",
    "    # ... (print completion messages) ...\n",
    "    return averaged_baseline_data, tuned_counting_params, exercise_specific_thresholds\n",
    "\n",
    "\n",
    "# ... (Rest of Cell 5 functions: _extract_trajectory_and_find_peaks, segment_repetitions, etc.) ...\n",
    "# !!! Important: The 'calculate_exercise_trajectory' function is no longer needed with this structure !!!\n",
    "# !!! You can REMOVE the definition of 'calculate_exercise_trajectory' from Cell 5.3 !!!\n",
    "# Ensure segment_repetitions, count_repetitions, tune_counting_parameters now use EXERCISE_CONFIG\n",
    "\n",
    "# Need to update segment_repetitions, count_repetitions, tune_counting_parameters\n",
    "# to fetch logic from EXERCISE_CONFIG instead of having internal if/elifs\n",
    "\n",
    "print(\"Baseline loading function updated (load_baseline_data uses angle from filename).\")\n",
    "# print(\"REMOVE calculate_exercise_trajectory. UPDATE segment_repetitions, count_repetitions, tune_counting_parameters TO USE EXERCISE_CONFIG.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d6741595-17f1-48b0-a7e7-7eb1046c1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5.8: Recognition Function ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db971025-e997-4ec8-b1cd-efb38d496935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "4ce5635b-fb63-4972-9491-f96b42c39fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition function updated (recognize_exercise supports angle-specific comparison).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5.8: Recognition Function (Updated for Angle-Specific Comparison) ===\n",
    "\n",
    "def recognize_exercise(new_video_path, averaged_baseline_data, hrnet_model, joint_names,\n",
    "                       target_rep_length, tuned_counting_params, exercise_specific_thresholds, # Added thresholds dict\n",
    "                       fallback_threshold=DTW_DISTANCE_THRESHOLD): # Global fallback\n",
    "    \"\"\"\n",
    "    Recognizes exercise using angle detection, angle-specific (or overall) baselines,\n",
    "    angle-specific (or overall/fallback) thresholds, and tuned segmentation parameters.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Recognizing Exercise (Angle-Aware Per Rep) for: {os.path.basename(new_video_path)} ---\")\n",
    "    start_time = time.time()\n",
    "    if tuned_counting_params is None: tuned_counting_params = {}\n",
    "    if exercise_specific_thresholds is None: exercise_specific_thresholds = {}\n",
    "    default_p, default_d = DEFAULT_PEAK_PROMINENCE, DEFAULT_PEAK_DISTANCE\n",
    "\n",
    "    # --- Step 1: Process Video -> Spatial Norm -> Clean ---\n",
    "    new_raw_seq = process_single_video(new_video_path, hrnet_model, EXPECTED_JOINT_COUNT)\n",
    "    if new_raw_seq is None or new_raw_seq.shape[0] == 0: return [], 0\n",
    "    new_spatially_norm_seq = normalize_pose_sequence(new_raw_seq, joint_names)\n",
    "    if new_spatially_norm_seq is None or np.all(np.isnan(new_spatially_norm_seq)): return [], 0\n",
    "    new_sequence_clean = handle_nan_values(new_spatially_norm_seq)\n",
    "    if new_sequence_clean.shape[0] < 2: return [], 0\n",
    "\n",
    "    # --- Step 2: Detect Angle of Test Video ---\n",
    "    test_video_angle = detect_camera_angle(new_sequence_clean, joint_names)\n",
    "    print(f\"  Detected test video angle: '{test_video_angle}'\")\n",
    "\n",
    "    # --- Step 3: Initial Guess & Parameter Selection for Segmentation ---\n",
    "    # (This part remains similar, but we store the initial guess label)\n",
    "    new_sequence_time_norm = time_normalize_sequence(new_sequence_clean, target_rep_length) # For initial guess\n",
    "    if np.isnan(new_sequence_time_norm).all(): return [], 0 # Handle failure\n",
    "    new_sequence_time_norm = np.nan_to_num(new_sequence_time_norm, nan=0.0)\n",
    "\n",
    "    initial_min_distance, initial_best_label = float('inf'), None\n",
    "    segment_p, segment_d = default_p, default_d\n",
    "    segment_label_guess = 'unknown'\n",
    "\n",
    "    if not averaged_baseline_data: print(\"E: No baseline data.\"); return [], 0\n",
    "    print(\"  Performing initial comparison for segmentation parameter selection...\")\n",
    "    # Compare against FRONT view baselines for initial guess? Or OVERALL? Let's use overall if available.\n",
    "    guess_angle_key = 'overall' if any('overall' in d for d in averaged_baseline_data.values() if isinstance(d, dict)) else 'front' # Prefer overall if exists\n",
    "\n",
    "    for ex_label, angle_dict in averaged_baseline_data.items():\n",
    "        avg_base_seq = angle_dict.get(guess_angle_key) # Get overall or front baseline\n",
    "        if avg_base_seq is None or new_sequence_time_norm.shape != avg_base_seq.shape: continue\n",
    "        safe_base = np.nan_to_num(avg_base_seq, nan=0.0)\n",
    "        try: dist, _ = fastdtw(new_sequence_time_norm, safe_base, dist=euclidean)\n",
    "        except Exception: continue\n",
    "        if dist < initial_min_distance: initial_min_distance, initial_best_label = dist, ex_label\n",
    "\n",
    "    # Select parameters based on initial guess AND detected angle\n",
    "    # Use tuned params specific to the detected angle of the test video if known\n",
    "    angle_to_tune_with = test_video_angle if test_video_angle != 'unknown' else 'front' # Use front if unknown? or overall? Needs decision. Let's try front for now.\n",
    "\n",
    "    if initial_best_label:\n",
    "        segment_label_guess = initial_best_label # Use label for trajectory logic\n",
    "        # Get params tuned for the specific detected angle (or fallback)\n",
    "        tuned_p, tuned_d = tuned_counting_params.get(initial_best_label, {}).get(angle_to_tune_with, (default_p, default_d))\n",
    "        segment_p, segment_d = tuned_p, tuned_d\n",
    "        print(f\"  Initial guess: {initial_best_label}. Using its '{angle_to_tune_with}' tuned params for segmentation (P={segment_p:.3f}, D={segment_d}).\")\n",
    "    else:\n",
    "        print(f\"  Initial guess failed. Using default params for segmentation.\")\n",
    "        # segment_label_guess remains 'unknown'\n",
    "\n",
    "    # --- Step 4: Segment Test Video ---\n",
    "    test_repetitions = segment_repetitions(new_sequence_clean, segment_label_guess, joint_names,\n",
    "                                           peak_prominence=segment_p, peak_distance=segment_d)\n",
    "    num_detected_reps = len(test_repetitions)\n",
    "    print(f\"  Detected {num_detected_reps} potential repetitions in test video.\")\n",
    "    if num_detected_reps == 0: return [], 0\n",
    "\n",
    "    # --- Step 5: Compare Each Repetition Segment (Angle-Aware) ---\n",
    "    repetition_results = []\n",
    "    print(f\"  Comparing each detected repetition against '{test_video_angle if test_video_angle != 'unknown' else 'overall'}' baselines...\")\n",
    "\n",
    "    # Determine which baseline/threshold key to use based on detected angle\n",
    "    comparison_angle_key = test_video_angle if test_video_angle != 'unknown' else 'overall'\n",
    "\n",
    "    for i, rep_segment in enumerate(test_repetitions):\n",
    "        # 5a. Time-Normalize\n",
    "        if rep_segment is None or rep_segment.shape[0] < 2:\n",
    "             repetition_results.append( (i+1, \"Error: Invalid Segment\", float('inf')) ); continue\n",
    "        time_norm_rep_segment = time_normalize_sequence(rep_segment, target_rep_length)\n",
    "        if np.isnan(time_norm_rep_segment).all():\n",
    "            repetition_results.append( (i+1, \"Error: Time Norm Failed\", float('inf')) ); continue\n",
    "        time_norm_rep_segment = np.nan_to_num(time_norm_rep_segment, nan=0.0)\n",
    "\n",
    "        # 5b. Compare rep against baselines matching the detected angle (or overall)\n",
    "        min_rep_distance = float('inf')\n",
    "        best_rep_label_for_this_rep = None\n",
    "\n",
    "        for exercise_label, angle_dict in averaged_baseline_data.items():\n",
    "            # Get the specific baseline for the determined angle key ('left','right','front', or 'overall')\n",
    "            baseline_to_compare = angle_dict.get(comparison_angle_key)\n",
    "\n",
    "            # Skip if no baseline exists for this exercise/angle combination\n",
    "            if baseline_to_compare is None or time_norm_rep_segment.shape != baseline_to_compare.shape:\n",
    "                 # print(f\"DEBUG: No baseline for {exercise_label} angle '{comparison_angle_key}' or shape mismatch.\")\n",
    "                 continue\n",
    "\n",
    "            safe_baseline = np.nan_to_num(baseline_to_compare, nan=0.0)\n",
    "            try:\n",
    "                distance, _ = fastdtw(time_norm_rep_segment, safe_baseline, dist=euclidean)\n",
    "                if distance < min_rep_distance:\n",
    "                    min_rep_distance = distance\n",
    "                    best_rep_label_for_this_rep = exercise_label\n",
    "            except Exception as e:\n",
    "                 print(f\"      Warn: DTW Error Rep {i+1} vs {exercise_label}/{comparison_angle_key}: {e}\")\n",
    "                 continue\n",
    "\n",
    "        # 5c. Apply threshold (specific to the matched exercise and angle/overall)\n",
    "        matched_label_for_rep = \"No Match / Inconclusive\" # Default\n",
    "        threshold_to_use = fallback_threshold # Start with global fallback\n",
    "\n",
    "        if best_rep_label_for_this_rep: # If a closest match was found\n",
    "             # Get the specific threshold for the matched exercise and comparison angle key\n",
    "             threshold_specific = exercise_specific_thresholds.get(best_rep_label_for_this_rep, {}).get(comparison_angle_key)\n",
    "             if threshold_specific is not None:\n",
    "                  threshold_to_use = threshold_specific\n",
    "             # else: print(f\"Debug: Using fallback threshold for {best_rep_label_for_this_rep}/{comparison_angle_key}\") # Optional\n",
    "\n",
    "             # Now compare distance to the selected threshold\n",
    "             if min_rep_distance < threshold_to_use:\n",
    "                  matched_label_for_rep = best_rep_label_for_this_rep\n",
    "             else:\n",
    "                  # Update label to show why it failed threshold\n",
    "                  matched_label_for_rep = f\"No Match (Closest: {best_rep_label_for_this_rep}, Dist: {min_rep_distance:.2f} >= {threshold_to_use:.2f})\"\n",
    "\n",
    "\n",
    "        repetition_results.append( (i+1, matched_label_for_rep, min_rep_distance) )\n",
    "\n",
    "    # --- Step 6: Return Results ---\n",
    "    end_time = time.time()\n",
    "    print(f\"--- Recognition finished in {end_time - start_time:.2f}s ---\")\n",
    "    return repetition_results, num_detected_reps\n",
    "\n",
    "print(\"Recognition function updated (recognize_exercise supports angle-specific comparison).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede226d1-ffb9-47dc-a57c-fac4043ae831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06deaf-378b-4c3c-b75d-2a06049684d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1736d66b-eae1-48f7-8b72-f1775a28fabd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdcf6a-b01d-4799-8434-b3dff9cb859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7e710e-3085-4fff-914d-c94ebf5bfb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "6466ce65-7903-4244-9c67-99596f2727b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running Configuration PreCheck \n",
      "✅ MODEL_PATH: Found ('pose_hrnet_w48_256x192.pth').\n",
      "✅ BASELINE_ROOT_DIR: Found ('C:/Users/michel.marien_icarew/Documents/Privé/Opleiding/Mastervakken/Deep Neural Networks/Opdacht 2/baselines2').\n",
      "✅ VIDEO_TO_RECOGNIZE_PATH: Found ('test_excercise.mp4').\n",
      "✅ DTW_DISTANCE_THRESHOLD: Set (500.0).\n",
      "✅ TARGET_REP_LENGTH: Set (100).\n",
      "✅ EXPECTED_JOINT_COUNT: Set (17).\n",
      "✅ JOINT_NAMES_LIST: Defined with correct length (17).\n",
      "✅ HRNET_MODEL: Loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(\" Running Configuration PreCheck \")\n",
    "config_ok = True\n",
    "error_messages = []\n",
    "\n",
    "if 'MODEL_PATH' not in locals() or not MODEL_PATH:\n",
    "    error_messages.append(\"❌ MODEL_PATH: Not defined.\")\n",
    "    config_ok = False\n",
    "elif not os.path.exists(MODEL_PATH):\n",
    "    error_messages.append(f\"❌ MODEL_PATH: File not found at '{MODEL_PATH}'.\")\n",
    "    config_ok = False\n",
    "else:\n",
    "    print(f\"✅ MODEL_PATH: Found ('{os.path.basename(MODEL_PATH)}').\")\n",
    "\n",
    "if 'BASELINE_ROOT_DIR' not in locals() or not BASELINE_ROOT_DIR:\n",
    "    error_messages.append(\"❌ BASELINE_ROOT_DIR: Not defined.\")\n",
    "    config_ok = False\n",
    "elif not os.path.isdir(BASELINE_ROOT_DIR):\n",
    "    error_messages.append(f\"❌ BASELINE_ROOT_DIR: Directory not found at '{BASELINE_ROOT_DIR}'. Please create it or correct the path.\")\n",
    "    config_ok = False\n",
    "else:\n",
    "    print(f\"✅ BASELINE_ROOT_DIR: Found ('{BASELINE_ROOT_DIR}').\")\n",
    "\n",
    "if 'VIDEO_TO_RECOGNIZE_PATH' not in locals() or not VIDEO_TO_RECOGNIZE_PATH:\n",
    "    error_messages.append(\"❌ VIDEO_TO_RECOGNIZE_PATH: Not defined.\")\n",
    "    config_ok = False\n",
    "elif not os.path.exists(VIDEO_TO_RECOGNIZE_PATH):\n",
    "    error_messages.append(f\"❌ VIDEO_TO_RECOGNIZE_PATH: File not found at '{VIDEO_TO_RECOGNIZE_PATH}'.\")\n",
    "    config_ok = False\n",
    "else:\n",
    "    print(f\"✅ VIDEO_TO_RECOGNIZE_PATH: Found ('{os.path.basename(VIDEO_TO_RECOGNIZE_PATH)}').\")\n",
    "\n",
    "if 'DTW_DISTANCE_THRESHOLD' not in locals() or not isinstance(DTW_DISTANCE_THRESHOLD, (int, float, np.number)) or np.isnan(DTW_DISTANCE_THRESHOLD):\n",
    "    error_messages.append(\"❌ DTW_DISTANCE_THRESHOLD: Not defined or not a valid number.\")\n",
    "    config_ok = False\n",
    "else:\n",
    "    print(f\"✅ DTW_DISTANCE_THRESHOLD: Set ({DTW_DISTANCE_THRESHOLD}).\")\n",
    "\n",
    "if 'TARGET_REP_LENGTH' not in locals() or not isinstance(TARGET_REP_LENGTH, int) or TARGET_REP_LENGTH <= 0:\n",
    "    error_messages.append(\"❌ TARGET_REP_LENGTH: Not defined or not a positive integer.\")\n",
    "    config_ok = False\n",
    "else:\n",
    "    print(f\"✅ TARGET_REP_LENGTH: Set ({TARGET_REP_LENGTH}).\")\n",
    "\n",
    "if 'EXPECTED_JOINT_COUNT' not in locals() or not isinstance(EXPECTED_JOINT_COUNT, int) or EXPECTED_JOINT_COUNT <= 0:\n",
    "    error_messages.append(\"❌ EXPECTED_JOINT_COUNT: Not defined or not a positive integer.\")\n",
    "    config_ok = False\n",
    "else:\n",
    "    print(f\"✅ EXPECTED_JOINT_COUNT: Set ({EXPECTED_JOINT_COUNT}).\")\n",
    "    if 'JOINT_NAMES_LIST' not in locals() or not isinstance(JOINT_NAMES_LIST, list):\n",
    "        error_messages.append(\"❌ JOINT_NAMES_LIST: Not defined or not a list.\")\n",
    "        config_ok = False\n",
    "    elif len(JOINT_NAMES_LIST) != EXPECTED_JOINT_COUNT:\n",
    "        error_messages.append(f\"❌ JOINT_NAMES_LIST: Length ({len(JOINT_NAMES_LIST)}) does not match EXPECTED_JOINT_COUNT ({EXPECTED_JOINT_COUNT}).\")\n",
    "        config_ok = False\n",
    "    else:\n",
    "        print(f\"✅ JOINT_NAMES_LIST: Defined with correct length ({len(JOINT_NAMES_LIST)}).\")\n",
    "if 'HRNET_MODEL' not in locals() or HRNET_MODEL is None:\n",
    "     error_messages.append(\"❌ HRNET_MODEL: Model was not loaded successfully in Cell 3.\")\n",
    "     config_ok = False\n",
    "else:\n",
    "     print(\"✅ HRNET_MODEL: Loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae5607-b138-4b87-a3c2-0fd9eca6b7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "00125ec4-ab74-4cac-916c-2f2da4fb3be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      " Step 1: Loading Baselines, Tuning Params, Deriving Thresholds \n",
      "=================================================================\n",
      "Loading Baselines (using angle from filename)... from: C:/Users/michel.marien_icarew/Documents/Privé/Opleiding/Mastervakken/Deep Neural Networks/Opdacht 2/baselines2\n",
      "\n",
      "Processing baseline exercise: tricep Pushdown\n",
      "  Found 4 video file(s).\n",
      "    Video: tricep_pushdown_3reps_front_video1.mp4, True Reps: 3, Parsed Angle: front\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'current_exercise_config_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[257], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m derived_thresholds \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# --- Load Averaged Baseline Data, Tuned Params, AND Derived Thresholds ---\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Calls the latest function defined in Cell 5\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m averaged_baselines, tuned_params, derived_thresholds \u001b[38;5;241m=\u001b[39m load_baseline_data(\n\u001b[0;32m     19\u001b[0m     BASELINE_ROOT_DIR,          \u001b[38;5;66;03m# Positional 1\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     HRNET_MODEL,                \u001b[38;5;66;03m# Positional 2\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     JOINT_NAMES_LIST,           \u001b[38;5;66;03m# Positional 3\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     TARGET_REP_LENGTH,          \u001b[38;5;66;03m# Positional 4\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     EXERCISE_CONFIG,            \u001b[38;5;66;03m# Positional 5\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     TUNING_PROMINENCE_RANGE,    \u001b[38;5;66;03m# Positional 6\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     TUNING_DISTANCE_RANGE,      \u001b[38;5;66;03m# Positional 7\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     threshold_std_multiplier\u001b[38;5;241m=\u001b[39mDEFAULT_THRESHOLD_STD_MULTIPLIER, \u001b[38;5;66;03m# Keyword 1\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     min_reps_for_threshold\u001b[38;5;241m=\u001b[39mDEFAULT_MIN_REPS_FOR_THRESHOLD   \u001b[38;5;66;03m# Keyword 2\u001b[39;00m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Check results\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m averaged_baselines: \u001b[38;5;66;03m# Check if primary result is non-empty\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[253], line 73\u001b[0m, in \u001b[0;36mload_baseline_data\u001b[1;34m(baseline_root_dir, hrnet_model, joint_names, target_rep_length, exercise_config, prominence_range, distance_range, threshold_std_multiplier, min_reps_for_threshold)\u001b[0m\n\u001b[0;32m     70\u001b[0m     labeled_data_by_angle[parsed_angle]\u001b[38;5;241m.\u001b[39mappend((cleaned_seq, true_reps))\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Segment using default params\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m repetitions \u001b[38;5;241m=\u001b[39m segment_repetitions(cleaned_seq, exercise_label, joint_names, current_exercise_config_dict)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repetitions: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Time normalize and store reps by parsed angle\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'current_exercise_config_dict' is not defined"
     ]
    }
   ],
   "source": [
    "if 'HRNET_MODEL' not in locals() or HRNET_MODEL is None:\n",
    "    print(\"HRNet Model not loaded. Please run Cell 3 successfully first.\")\n",
    "elif 'config_ok' not in locals() or not config_ok:\n",
    "    print(\"Configuration check in Cell 5.5 failed. Please fix issues before running baseline loading.\")\n",
    "\n",
    "else:\n",
    "    print(\"=================================================================\")\n",
    "    print(\" Step 1: Loading Baselines, Tuning Params, Deriving Thresholds \")\n",
    "    print(\"=================================================================\")\n",
    "\n",
    "    # Reset variables before loading (important for re-running the cell)\n",
    "    averaged_baselines = {}\n",
    "    tuned_params = {}\n",
    "    derived_thresholds = {}\n",
    "\n",
    "    # --- Load Averaged Baseline Data, Tuned Params, AND Derived Thresholds ---\n",
    "    # Calls the latest function defined in Cell 5\n",
    "    averaged_baselines, tuned_params, derived_thresholds = load_baseline_data(\n",
    "        BASELINE_ROOT_DIR,          # Positional 1\n",
    "        HRNET_MODEL,                # Positional 2\n",
    "        JOINT_NAMES_LIST,           # Positional 3\n",
    "        TARGET_REP_LENGTH,          # Positional 4\n",
    "        EXERCISE_CONFIG,            # Positional 5\n",
    "        TUNING_PROMINENCE_RANGE,    # Positional 6\n",
    "        TUNING_DISTANCE_RANGE,      # Positional 7\n",
    "        threshold_std_multiplier=DEFAULT_THRESHOLD_STD_MULTIPLIER, # Keyword 1\n",
    "        min_reps_for_threshold=DEFAULT_MIN_REPS_FOR_THRESHOLD   # Keyword 2\n",
    "    )\n",
    "\n",
    "    # Check results\n",
    "    if averaged_baselines: # Check if primary result is non-empty\n",
    "        print(\"\\n--- Baseline Data Loaded/Processed Successfully --- 👍\")\n",
    "        # Add more checks if needed for tuned_params and derived_thresholds\n",
    "    else:\n",
    "        print(\"\\n--- Baseline Data Loading/Processing Failed --- 👎\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bfb06-47e2-4772-b3f6-df2c70daa99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6.5: Display Derived Thresholds (Optional) ===\n",
    "\n",
    "import pprint # For potentially prettier dictionary printing\n",
    "\n",
    "print(\"\\n--- Derived Recognition Thresholds (Per Angle + Overall) ---\")\n",
    "if 'derived_thresholds' in locals() and derived_thresholds:\n",
    "    pprint.pprint(derived_thresholds)\n",
    "    print(\"----------------------------------------------------------\")\n",
    "elif 'derived_thresholds' in locals():\n",
    "     print(\"  No thresholds were derived (dictionary is empty).\")\n",
    "else:\n",
    "    print(\"  Derived thresholds dictionary not found. Did Cell 6 run successfully?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17a2cd-6d02-4598-b633-b4b37465bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTW_DISTANCE_THRESHOLD = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910509c-2d9a-409e-8993-d16d4f5500c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set to 'left', 'right', 'front', or leave as None to use 'overall' baselines/thresholds\n",
    "manual_angle_hint = None # Or 'left', 'right', 'front'\n",
    "\n",
    "\n",
    "video_to_test = VIDEO_TO_RECOGNIZE_PATH \n",
    "proceed_to_recognition = True\n",
    "if 'averaged_baselines' not in locals() or not averaged_baselines \\\n",
    "   or 'tuned_params' not in locals() or tuned_params is None \\\n",
    "   or 'derived_thresholds' not in locals() or derived_thresholds is None: # Added check for thresholds\n",
    "    error_messages.append(\"❌ Error: Baseline data, tuned parameters, or derived thresholds not found.\")\n",
    "    proceed_to_recognition = False\n",
    "# ... (other checks) ...\n",
    "elif not os.path.exists(video_to_test):\n",
    "     error_messages.append(f\"❌ Error: Test video file not found at '{video_to_test}'. Please check the path.\")\n",
    "     proceed_to_recognition = False\n",
    "\n",
    "# --- Only proceed if all checks passed ---\n",
    "if proceed_to_recognition:\n",
    "    # ... (print header) ...\n",
    "\n",
    "    # --- Run Recognition (Pass derived_thresholds) ---\n",
    "    list_of_rep_results, total_reps_detected = recognize_exercise(\n",
    "        video_to_test,\n",
    "        averaged_baselines,\n",
    "        HRNET_MODEL,\n",
    "        JOINT_NAMES_LIST,\n",
    "        DTW_DISTANCE_THRESHOLD, # This is now mainly a fallback\n",
    "        TARGET_REP_LENGTH,\n",
    "        tuned_params,\n",
    "        derived_thresholds # Pass the derived thresholds here\n",
    "    )\n",
    "if proceed_to_recognition:\n",
    "    print(\"==============================================\")\n",
    "    print(f\"    Step 2: Recognizing Exercise in Video (Per Rep) \")\n",
    "    print(f\"    Video: {os.path.basename(video_to_test)} \")\n",
    "    if manual_angle_hint: print(f\"    Angle Hint: '{manual_angle_hint}'\")\n",
    "    print(\"==============================================\")\n",
    "\n",
    "    # --- Run Recognition (Pass hint_angle and config) ---\n",
    "    list_of_rep_results, total_reps_detected = recognize_exercise(\n",
    "        new_video_path=video_to_test,\n",
    "        averaged_baseline_data=averaged_baselines,\n",
    "        hrnet_model=HRNET_MODEL,\n",
    "        joint_names=JOINT_NAMES_LIST,\n",
    "        exercise_config=EXERCISE_CONFIG, # Pass the config\n",
    "        target_rep_length=TARGET_REP_LENGTH,\n",
    "        tuned_counting_params=tuned_params,\n",
    "        exercise_specific_thresholds=derived_thresholds,\n",
    "        hint_angle=manual_angle_hint, # Pass the hint\n",
    "        fallback_threshold=DTW_DISTANCE_THRESHOLD # Pass fallback\n",
    "    )\n",
    "\n",
    "    # --- Print Results ---\n",
    "    print(\"\\n==============================================\")\n",
    "    print(f\"    FINAL RESULT for {os.path.basename(video_to_test)}\")\n",
    "    print(\"==============================================\")\n",
    "    print(f\"  Detected {total_reps_detected} Repetitions.\") # Uses count from segmentation step\n",
    "\n",
    "    if not list_of_rep_results:\n",
    "        print(\"  No valid repetition results to display (segmentation might have failed or video was problematic).\")\n",
    "    else:\n",
    "        print(\"\\n  --- Per-Repetition Analysis ---\")\n",
    "        match_counts = {}\n",
    "        successful_reps = 0\n",
    "        for rep_index, matched_label, min_distance in list_of_rep_results:\n",
    "            status = \"\"\n",
    "            # Check for specific error strings first\n",
    "            if isinstance(matched_label, str) and matched_label.startswith(\"Error\"):\n",
    "                 status = f\"-> {matched_label}\"\n",
    "            # Check for inconclusive match (below threshold check is now inside recognize_exercise)\n",
    "            elif isinstance(matched_label, str) and matched_label.startswith(\"No Match\"):\n",
    "                 status = f\"-> {matched_label}\" # Includes threshold info now\n",
    "            # Otherwise, it's a successful match for this rep\n",
    "            else:\n",
    "                status = f\"-> Match: {matched_label} (Dist: {min_distance:.2f})\"\n",
    "                match_counts[matched_label] = match_counts.get(matched_label, 0) + 1\n",
    "                successful_reps += 1\n",
    "            print(f\"    Rep {rep_index}: {status}\")\n",
    "\n",
    "        print(\"\\n  --- Overall Summary ---\")\n",
    "        if successful_reps > 0:\n",
    "             if match_counts:\n",
    "                 # Find the exercise label with the highest count\n",
    "                 majority_label = max(match_counts, key=match_counts.get)\n",
    "                 majority_count = match_counts[majority_label]\n",
    "                 print(f\"  Overall Predominant Match: {majority_label} ({majority_count}/{successful_reps} successfully matched reps)\")\n",
    "             else:\n",
    "                 # This case should not happen if successful_reps > 0, but included for safety\n",
    "                  print(\"  No specific exercises were matched consistently.\")\n",
    "             # Use total_reps_detected which comes from segmentation\n",
    "             print(f\"  Total Reps Successfully Matched (below threshold): {successful_reps} / {total_reps_detected}\")\n",
    "        elif total_reps_detected > 0: # Reps detected but none matched\n",
    "             print(\"  No repetitions were successfully matched to any baseline exercise.\")\n",
    "        else: # No reps detected at all\n",
    "             print(\"  No repetitions were detected in the video to analyze.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26510931-4f2c-47f6-8b70-277f84256e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bfe235-fa4c-4cc6-94f4-c9615dc84b82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
