{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "052c6234-e8b0-4110-bf60-dc46c0a5d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ffmpeg-python>=0.2.0\n",
    "#!pip install matplotlib>=3.0.2\n",
    "#!pip install munkres>=1.1.2\n",
    "#!pip install numpy>=1.16\n",
    "#!pip install opencv-python>=3.4\n",
    "#!pip install Pillow>=5.4\n",
    "#!pip install vidgear>=0.1.4\n",
    "#!pip install torch>=1.4.0\n",
    "#!pip install torchvision>=0.5.0\n",
    "#!pip install tqdm>=4.26\n",
    "#!pip install tensorboard>=1.11\n",
    "#!pip install tensorboardX>=1.4\n",
    "#!pip install backports.entry-points-selectable\n",
    "#!pip install backports.tempfile\n",
    "#!pip uninstall -y setuptools\n",
    "#!pip install setuptools==65.5.0\n",
    "#!pip install fastdtw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd39210-49ef-44d2-98c4-707846926938",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6e19b7-7a76-4316-8ebd-4d9e021ef5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pathlib\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from SimpleHRNet import SimpleHRNet\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.signal import find_peaks\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a048790-fa43-4c11-a08b-ed6476983fe0",
   "metadata": {},
   "source": [
    "### Constants & Configuration\n",
    "\n",
    "This cell defines all constants, file paths, processing parameters and exercise-specific configurations.\n",
    "\n",
    "* **File and Model Paths:**\n",
    "    * `Model_path`: (`pathlib.Path`) - The full path to downloaded SimpleHRNet model weights\n",
    "    * `Video_baseline_path`: (`pathlib.Path`) - The full path to the main folder containing subfolders for each baseline exercise video (filenames should include angle and rep count (`squat_10reps_front_video1.mp4`).\n",
    "    * `Video_test_path`: (`pathlib.Path`) - The full path to the test video file\n",
    "* **Model & Pose Structure:**\n",
    "    * `Expected_Joint_Count`: (`int`, Unit = Count) - The number of keypoints the HRNet model detects (17 for COCO).\n",
    "    * `Jointnames_list`: (`list`, Unit = Name) - List of names corresponding to the keypoint indices output by the model (must match model output)\n",
    "\n",
    "* **Processing Settings:**\n",
    "* `Target_rep_length`: (`int`, Unit = Frames / Timesteps). This parameter defines a standard length (number of data points or \"frames\") for every single repetition segment after it has been detected because exercise repetitions can vary in duration. To compare them using DTW, they need to be represented on the same timescale. The `time_normalize_sequence` function takes a repetition segment and resamples it using linear interpolation to produce a new sequence that always has exactly the same number of frames as `Target_rep_length`. By normalizing all repetitions we ensure that the DTW distances reflect differences in the shape of the movement over that standard duration.\n",
    "  \n",
    "* **User Settings:**\n",
    "    * `DTW_Distance_threshold`: (`float`, Unit = Unitless Distance) - Used if a specific derived threshold isn't available. Lower values are stricter for matching\n",
    "    * `Tuning_Prominance_range`: (`list` of `float`, Unit = Unitless Ratio) - Defines the search space for the `peak_prominence` parameter during automatic tuning of repetition counting.\n",
    "    * `Tuning_Distance_range`: (`list` of `int`, Unit = Frames) - Search space for `peak_distance` parameter during automatic tuning. Controls the minimum number of frames separating detected repetitions\n",
    "    * `Smoothing_window_size`: (`int`, Unit = Frames) - Size of moving average window to smooth trajectory signal before peak detection (reduce noise)\n",
    "    * `Default_Peak_Prominence`: (`float`, Unit = Unitless Ratio) - Prominence value used for repetition counting if tuning fails\n",
    "    * `Default_Peak_Distance`: (`int`, Unit = Frames) - Minimum distance between peaks used for repetition counting if tuning fails\n",
    "    * `Threshold_STD_Multiplier`: (`float`, Unit = Unitless) - Factor (N) used when deriving thresholds automatically (Mean + N * StdDev)\n",
    "    * `Min_REPS_to_calc_threshold`: (`int`, Unit = Count) - Min. number of valid repetitions within a specific baseline angle to derive a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee4a028-1453-450b-bcfd-c76c98565fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_path = pathlib.Path(\"C:/Users/michel.marien_icarew/Documents/Privé/Opleiding/Mastervakken/Deep Neural Networks/Opdacht 2/simple-HRNet/weights/pose_hrnet_w48_256x192.pth\") # Path to HRNet weights\n",
    "Video_baseline_path = \"C:/Users/michel.marien_icarew/Documents/Privé/Opleiding/Mastervakken/Deep Neural Networks/Opdacht 2/baselines2\"\n",
    "Video_test_path = \"C:/Users/michel.marien_icarew/Documents/Privé/Opleiding/Mastervakken/Deep Neural Networks/Opdacht 2/test_videos/test_excercise.mp4\" # Video file to analyze\n",
    "\n",
    "# load pre-trained data\n",
    "Use_precalculated_parameters = True # True to load from file, False to recreate baseline\n",
    "Precomputed_model_parameters = \"exercise_learned_summary.xlsx\" # File with parameters\n",
    "\n",
    "# Model & Pose Settings\n",
    "Expected_Joint_Count = 17 # COCO keypoints format\n",
    "Jointnames_list = [\"Nose\",\"Left Eye\",\"Right Eye\",\"Left Ear\",\"Right Ear\",\"Left Shoulder\", \"Right Shoulder\",\"Left Elbow\",\"Right Elbow\",\"Left Wrist\",\n",
    "                    \"Right Wrist\", \"Left Hip\",\"Right Hip\",\"Left Knee\",\"Right Knee\",\"Left Ankle\",\"Right Ankle\"]\n",
    "\n",
    "# Recognition & Processing Parameters\n",
    "Target_rep_length = 100\n",
    "DTW_Distance_threshold = 500.0\n",
    "\n",
    "# Parameters for Repetition Counting Tuning (Grid Search Ranges)\n",
    "Tuning_Prominance_range = [0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14]\n",
    "Tuning_Distance_range = [5, 8, 10, 12, 15, 20, 25, 30, 35]\n",
    "\n",
    "# Parameters for Threshold Derivation\n",
    "Threshold_STD_Multiplier = 2.0 # Default for mean + N*stddev calculation\n",
    "Min_REPS_to_calc_threshold = 1     # Min reps needed in baseline angle category to derive threshold\n",
    "\n",
    "# Fallback if tuning fails or not enough data\n",
    "Smoothing_window_size = 5\n",
    "Default_Peak_Prominence = 0.05\n",
    "Default_Peak_Distance = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f92e24-9d2c-44d9-8b95-1334741d488a",
   "metadata": {},
   "source": [
    "#### Configuration Block: `EXERCISE_CONFIG`\n",
    "\n",
    "This dictionary (`EXERCISE_CONFIG`) defines the movement trajectory for each specific exercise during the repetition counting steps. The keys of the dictionary (`'tricep_pushdown'`) should match the corresponding baseline subfolder names (pay attention to lowercase/underscores).\n",
    "\n",
    "Running this code block defines the `EXERCISE_CONFIG` variable in the notebook's memory.\n",
    "\n",
    "* `EXERCISE_CONFIG`: (`dict`)\n",
    "    * Keys: Exercise label strings (`'squat'`).\n",
    "    * Values: A nested dictionary containing configuration for that specific exercise. Currently, it holds the `'trajectory_logic'`.\n",
    "\n",
    "For each exercise, the nested `'trajectory_logic'` dictionary specifies:\n",
    "\n",
    "* `'metric'`: (`str`)\n",
    "    * Defines what measurement represents the cyclical motion of the repetition (`'elbow_angle'`, `'hip_y'`, `'shoulder_y'`).\n",
    "    * Unit = Degrees for angles, normalized Y-coordinate units for `_y` metrics).\n",
    "* `'joints'`: (`list` of `str`)\n",
    "    * Specifies the base names of the joints required to calculate the chosen `metric`. For metrics involving paired joints (like angles or averaging Y-coordinates), the code will automatically look for 'Left' and 'Right' versions (if 'Shoulder' is listed for a `_y` metric, it will try to find 'Left Shoulder' and 'Right Shoulder'). For angle metrics needing 3 joints, list them in order (`['Shoulder', 'Elbow', 'Wrist']`).\n",
    "    * Unit = Joint names (strings).\n",
    "* `'invert_for_valley'`: (`bool`)\n",
    "    * Tells the peak-finding algorithm if the moment of a rep. cycle corresponds to a min. value (`True`) or a max.m value (`False`) in the calculated `trajectory` signal (bottom of a squat, lowest point of a dip, most extended arm in a bicep curl).\n",
    "    * Unit = Boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaed6a0a-b22b-4fac-8ae2-17e4bf9881cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXERCISE_CONFIG = {\n",
    "    'tricep_pushdown': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'elbow_angle', # Use 'elbow_angle' # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder', 'Elbow', 'Wrist'], # Base joint names\n",
    "            'invert_for_valley': False # Peak angle marks end of repetition\n",
    "        },\n",
    "    },\n",
    "    'tricep_dips': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'shoulder_y', # Use Y-coordinate of shoulders # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder'], # Base joint names\n",
    "            'invert_for_valley': True # Lowest point (valley) marks end of repetition\n",
    "        },\n",
    "    },\n",
    "    'squat': {\n",
    "         'trajectory_logic': {\n",
    "             'metric': 'knee_angle', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "             'joints': ['Hip', 'Knee', 'Ankle'], # Base joint names\n",
    "             'invert_for_valley': True # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'barbell_biceps': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'wrist_y', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder', 'Elbow', 'Wrist'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'bench_press': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'elbow_y', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Elbow'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'chest_fly_machine': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'wrist_x', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Wrist'], # Base joint names\n",
    "            'invert_for_valley': True # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'deadlift': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'shoulder_y', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder', 'Hip', 'Knee'], # Base joint names\n",
    "            'invert_for_valley': True # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'decline_bench_press': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'shoulder_angle', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder', 'Elbow', 'Wrist'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'hammer_curl': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'wrist_y', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Wrist'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'hip_thrust': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'hip_y', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Hip'], # Base joint names\n",
    "            'invert_for_valley': True # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'incline_bench_press': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'elbow_angle', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Wrist', 'Elbow', 'Shoulder'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'lat_pulldown': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'wrist_y', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Wrist'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'lateral_raise': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'shoulder_angle', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Hip', 'Shoulder', 'Wrist'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'leg_extension': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'ankle_y', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Ankle'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'leg_raises': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'hip_angle', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder','Hip','Ankle'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'plank': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'hip_y', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Hip'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'pull_Up': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'nose_y', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Nose'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'push-up': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'ankle_angle', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder', 'Ankle', 'Wrist'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'romanian_deadlift': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'hip_angle', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder','Hip', 'Knee'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'russian_twist': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'wrist_x', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Wrist'], # Base joint names\n",
    "            'invert_for_valley': True # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    'shoulder_press': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'wrist_y', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Wrist'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "    },\n",
    "    't_bar_row': {\n",
    "        'trajectory_logic': {\n",
    "            'metric': 'elbow_angle', # Use Y-coordinate of hips # also possible: 'shoulder_y', 'hip_y', 'wrist_y', 'knee_angle', etc\n",
    "            'joints': ['Shoulder', 'Elbow', 'Wrist'], # Base joint names\n",
    "            'invert_for_valley': False # Lowest point (valley) marks end of repetition\n",
    "         },\n",
    "     },\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d682a0b-456c-4287-be50-f7dfceeb6644",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "This cell loads the pose estimation model (`SimpleHRNet`) and selects GPU (`cuda`) or CPU for computation based on availability.\n",
    "\n",
    "* `DEVICE`: (`torch.device`)\n",
    "    * Hardware device (CPU or CUDA-enabled GPU) that the model will run on.\n",
    "* `HRNET_MODEL`: (`SimpleHRNet` object or `None`)\n",
    "    * Initialized SimpleHRNet model object, if loading fails, it remains `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f059e07-4cb4-4b92-a9dd-1462294c841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cpu'\n",
      "SimpleHRNet model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "HRNET_MODEL = None\n",
    "try:\n",
    "    model_c_value = 48\n",
    "    if not Model_path.exists():\n",
    "         raise FileNotFoundError(f\"Model checkpoint file not found at {Model_path}\")\n",
    "\n",
    "    HRNET_MODEL = SimpleHRNet(c=model_c_value,\n",
    "                              nof_joints=Expected_Joint_Count,\n",
    "                              checkpoint_path=Model_path,\n",
    "                              device=DEVICE,\n",
    "                              multiperson=False,\n",
    "                              max_batch_size=16)\n",
    "    print(\"SimpleHRNet model loaded successfully.\")\n",
    "except FileNotFoundError as nofile_error:\n",
    "    print(nofile_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23faa618-044d-4cf2-90ff-64c2479b1f47",
   "metadata": {},
   "source": [
    "### Subfunctions\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Subfunctions for the main functions later on\n",
    "\n",
    "* **`get_keypoint(frame_kps, kp_index)`:**\n",
    "    * To retrieve the X, Y coordinates for a single joint (`kp_index`) from a single frame (`frame_kps`, reshaped to `(num_joints, 2)`).\n",
    "    * Outcome: The coordinates of the joint if found and not NaN, otherwise `None`.\n",
    "    * Unit = Coordinate values.\n",
    "* **`calculate_angle(p1, p2, p3)`:**\n",
    "    * To compute the angle at point `p2` by the vectors connecting `p1-p2` and `p3-p2`. Used for joint angles\n",
    "    * Outcome: The calculated angle if all points are valid, otherwise `None`.\n",
    "    * Unit = Degrees.\n",
    "* **`handle_nan_values(sequence)`:**\n",
    "    * Clean a pose sequence by removing frames (rows) that contain any `NaN` (can occur if one or more joints were not detected in a frame).\n",
    "    * Outcome: `numpy.array` containing only the frames where all joints were detected.\n",
    "    * Unit = Same as the input sequence.\n",
    "* **`time_normalize_sequence(sequence, target_length)`:**\n",
    "    * To standardize the length of a single repetition to a fixed number of steps (`target_length`). It resamples the sequence using linear interpolation.\n",
    "    * Outcome: A `numpy.array` with shape (`target_length`, number_of_coordinates), where the movement has been stretched or compressed in time.\n",
    "    * Unit = Same as the input sequence but with time axis now normalized to `target_length` steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "621e5c70-0704-441d-a1e3-868f0674be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoint(frame_kps, kp_index):\n",
    "    \"\"\"Safely get keypoint coordinates (x, y) from a frame's keypoint array.\"\"\"\n",
    "    if frame_kps is not None and 0 <= kp_index < frame_kps.shape[0]:\n",
    "        coords = frame_kps[kp_index]\n",
    "        if not np.isnan(coords).any():\n",
    "            return coords\n",
    "    \n",
    "    return None\n",
    "\n",
    "def calculate_angle(p1, p2, p3):\n",
    "    \"\"\"Calculates the angle (in degrees) at p2 formed by p1-p2-p3.\"\"\"\n",
    "    if p1 is None or p2 is None or p3 is None: \n",
    "        return None\n",
    "    \n",
    "    v1, v2 = np.array(p1) - np.array(p2), np.array(p3) - np.array(p2)\n",
    "    mag1, mag2 = np.linalg.norm(v1), np.linalg.norm(v2)\n",
    "    if mag1 * mag2 == 0 or np.isclose(mag1 * mag2, 0): \n",
    "        return None\n",
    "    \n",
    "    cos_angle = np.clip(np.dot(v1, v2) / (mag1 * mag2), -1.0, 1.0)\n",
    "    angle = np.degrees(np.arccos(cos_angle))    \n",
    "    return angle\n",
    "\n",
    "def handle_nan_values(sequence):\n",
    "    \"\"\"Handles NaN values in a pose sequence (frames, coords). Removes frames containing any NaNs.\"\"\"\n",
    "    if sequence is None or sequence.ndim != 2 or sequence.shape[0] == 0:\n",
    "        return np.empty((0, sequence.shape[1] if sequence is not None and sequence.ndim == 2 else 0))\n",
    "    \n",
    "    valid_frames_mask = ~np.isnan(sequence).any(axis=1)\n",
    "    return sequence[valid_frames_mask]\n",
    "\n",
    "def time_normalize_sequence(sequence, target_length):\n",
    "    \"\"\"Resamples a pose sequence (frames, coords) to a target length using linear interpolation.\"\"\"\n",
    "    if sequence is None or sequence.shape[0] < 2:\n",
    "        num_coords = sequence.shape[1] if sequence is not None and sequence.ndim == 2 else Expected_Joint_Count * 2\n",
    "        return np.full((target_length, num_coords), np.nan)\n",
    "\n",
    "    num_frames, num_coords = sequence.shape\n",
    "    original_indices = np.linspace(0, num_frames - 1, num_frames)\n",
    "    target_indices = np.linspace(0, num_frames - 1, target_length)\n",
    "    normalized_sequence = np.zeros((target_length, num_coords))\n",
    "\n",
    "    for j in range(num_coords):\n",
    "        valid_mask = ~np.isnan(sequence[:, j])\n",
    "        if np.sum(valid_mask) < 2:\n",
    "            normalized_sequence[:, j] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                normalized_sequence[:, j] = np.interp(target_indices, original_indices[valid_mask], sequence[valid_mask, j])\n",
    "            except Exception: \n",
    "                normalized_sequence[:, j] = np.nan\n",
    "\n",
    "    if np.isnan(normalized_sequence).any():\n",
    "        try:\n",
    "             df = pd.DataFrame(normalized_sequence)\n",
    "             df.ffill(inplace=True)\n",
    "             df.bfill(inplace=True)\n",
    "             normalized_sequence = df.to_numpy()\n",
    "        except NameError:\n",
    "             normalized_sequence = np.nan_to_num(normalized_sequence, nan=0.0)\n",
    "\n",
    "    return np.nan_to_num(normalized_sequence, nan=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf4b36-8fe5-4e0b-a3d9-bccd615b1c66",
   "metadata": {},
   "source": [
    "### Angle Detection\n",
    "\n",
    "This cell estimates the camera's viewing angle relative to the person in the video ('left', 'right', or 'front') using the average horizontal positions of the shoulders.\n",
    "\n",
    "* **`detect_camera_angle(pose_sequence, joint_names, min_valid_frames=10, width_threshold_factor=0.35)`:**\n",
    "    * Estimate camera angle by analyzing the average horizontal positions of the 'Left Shoulder' and 'Right Shoulder' joints over time.\n",
    "    * Parameters:\n",
    "        * `pose_sequence`: (`numpy.array`, Unit = Normalized coordinates) - The input pose data, expected shape (frames, num_coords).\n",
    "        * `joint_names`: (`list` of `str`) - List of joint names.\n",
    "        * `min_valid_frames`: (`int`, Unit = Frames) - Minimum number of frames where both shoulders must be detected for the angle detection to continue.\n",
    "        * `width_threshold_factor`: (`float`, Unit = Unitless ratio) - Sensitivity. The absolute difference in average shoulder X-positions must exceed this fraction of the average detected shoulder width to be classified as a side view.\n",
    "    * A string indicating the estimated angle, Unit = Category ('left', 'right', 'front', 'unknown').\n",
    "    * Internal calculations:\n",
    "        * Identifies frames where both left and right shoulders are reliably detected or returns 'unknown' if there are too few (`< min_valid_frames`).\n",
    "        * Calculates the average X-position of each shoulder (`avg_x_left`, `avg_x_right`).\n",
    "        * Calculates the average horizontal distance between the shoulders (`avg_shoulder_width`)\n",
    "        * Calculates the difference (`x_difference = avg_x_right - avg_x_left`).\n",
    "        * Compares the absolute `x_difference` to threshold (`avg_shoulder_width * width_threshold_factor`).\n",
    "        * Return 'right' if difference is large and positive (right shoulder further right on screen) thus camera view from right\n",
    "        * Return 'left' if difference is large and negative (right shoulder further left on screen) thus camera view from left\n",
    "        * Otherwise return 'front' (shoulders relatively aligned horizontally) or 'unknown' if necessary joints aren't found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da4b54a-7d05-429c-aa8a-ff0a328aacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_camera_angle(pose_sequence, joint_names, min_valid_frames=10, width_threshold_factor=0.35):\n",
    "    \"\"\"\n",
    "    Analyzes pose sequence for camera angle based on shoulder positions.\n",
    "    \"\"\"\n",
    "    if pose_sequence is None or pose_sequence.ndim != 2 or pose_sequence.shape[0] < min_valid_frames:\n",
    "        return 'unknown'\n",
    "\n",
    "    try:\n",
    "        lshoulder_idx = joint_names.index('Left Shoulder')\n",
    "        rshoulder_idx = joint_names.index('Right Shoulder')\n",
    "        lshoulder_x_idx, rshoulder_x_idx = 2 * lshoulder_idx, 2 * rshoulder_idx\n",
    "\n",
    "        if max(lshoulder_x_idx, rshoulder_x_idx) >= pose_sequence.shape[1]:\n",
    "             return 'unknown'\n",
    "\n",
    "        valid_mask = ~np.isnan(pose_sequence[:, lshoulder_x_idx]) & \\\n",
    "                     ~np.isnan(pose_sequence[:, rshoulder_x_idx])\n",
    "\n",
    "        if np.sum(valid_mask) < min_valid_frames:\n",
    "            return 'unknown'\n",
    "\n",
    "        valid_lsx = pose_sequence[valid_mask, lshoulder_x_idx]\n",
    "        valid_rsx = pose_sequence[valid_mask, rshoulder_x_idx]\n",
    "\n",
    "        avg_x_left = np.mean(valid_lsx)\n",
    "        avg_x_right = np.mean(valid_rsx)\n",
    "\n",
    "        avg_shoulder_width = np.mean(np.abs(valid_lsx - valid_rsx))\n",
    "\n",
    "        if avg_shoulder_width < 1e-6:\n",
    "             return 'front'\n",
    "\n",
    "        x_difference = avg_x_right - avg_x_left\n",
    "\n",
    "        threshold = avg_shoulder_width * width_threshold_factor\n",
    "\n",
    "        detected_angle = 'front'\n",
    "        if x_difference > threshold:\n",
    "            detected_angle = 'right'\n",
    "        elif x_difference < -threshold:\n",
    "            detected_angle = 'left'\n",
    "\n",
    "        return detected_angle\n",
    "\n",
    "    except ValueError:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc88ba9-7abf-42bd-98f6-3e98f5ef20d8",
   "metadata": {},
   "source": [
    "### Video Processing en Normalization\n",
    "\n",
    "This cell extracts the raw pose data from a video file using the loaded HRNet model and performs normalization on the extracted raw poses.\n",
    "\n",
    "**`process_single_video(video_path, model, Expected_Joint_Count)`:**\n",
    "* To read a specified video file frame-by-frame, apply the loaded pose estimation `model` (`HRNET_MODEL`) to each frame, and extract the raw joint coordinates. It assumes a single person is the primary subject.\n",
    "    * Returns a 2D NumPy array where each row represents a frame and columns contain the flattened (x, y) coordinates for all `Expected_Joint_Count` joints (shape `(num_frames, 34)` for 17 joints). \n",
    "    * Unit = Pixel coordinates (relative to the video frame dimensions).\n",
    "\n",
    "**`normalize_pose_sequence(pose_sequence, joint_names, ref_joint1_name=\"Left Shoulder\", ref_joint2_name=\"Right Shoulder\")`:**\n",
    "* To perform spatial normalization on the raw pose sequence obtained from `process_single_video`. This aims to make the pose data invariant to the person's location within the video frame and their scale (distance from the camera).\n",
    "    * Inner logic: For each frame, it calculates a center point (midpoint between `ref_joint1` and `ref_joint2`, examples are Left/Right Shoulder) and a scale factor (distance between `ref_joint1` and `ref_joint2`). It then recalculates all joint coordinates relative to this center point and scales them by the calculated scale factor.\n",
    "    * Returns a `numpy.array` of the same shape as the input `pose_sequence`, but containing the normalized coordinates.\n",
    "    * Unit = Unitless normalized coordinates. The values represent positions relative to the reference joints, scaled by the distance between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50d41674-f0cd-406b-8689-dcff98dd8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_video(video_path, model, Expected_Joint_Count=Expected_Joint_Count):\n",
    "    \"\"\"Processes a single video, extracts joints, returns NumPy array (frames, num_coords).\"\"\"\n",
    "    if not os.path.exists(video_path): \n",
    "        print(f\"Video not found: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    if not video.isOpened(): \n",
    "        print(f\"Cannot open video: {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    video_joints, frame_count = [], 0 \n",
    "    expected_coords = Expected_Joint_Count * 2\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = video.read()\n",
    "            if not ret: \n",
    "                break\n",
    "            try:\n",
    "                joints = model.predict(frame)\n",
    "                if joints is None or joints.shape[0]==0 or joints.shape[1]!=Expected_Joint_Count: \n",
    "                    frame_coords=np.full(expected_coords,np.nan)\n",
    "                else: \n",
    "                    frame_coords=joints[0,:,:2].flatten() \n",
    "                    frame_coords = frame_coords if frame_coords.shape[0]==expected_coords else np.full(expected_coords,np.nan)\n",
    "            except Exception as e: \n",
    "                frame_coords = np.full(expected_coords, np.nan)\n",
    "            \n",
    "            video_joints.append(frame_coords)\n",
    "            frame_count += 1\n",
    "    finally: \n",
    "        video.release()\n",
    "    return np.array(video_joints) if video_joints else None\n",
    "\n",
    "def normalize_pose_sequence(pose_sequence, joint_names, ref_joint1_name=\"Left Shoulder\", ref_joint2_name=\"Right Shoulder\"):\n",
    "    \"\"\"Spatially normalizes a pose sequence based on reference joints.\"\"\"\n",
    "    if pose_sequence is None or pose_sequence.shape[0] == 0: \n",
    "        return np.array([])\n",
    "    try: \n",
    "        ref_joint1_idx, ref_joint2_idx = joint_names.index(ref_joint1_name), joint_names.index(ref_joint2_name)\n",
    "    except ValueError: \n",
    "        print(f\"Ref joints not found in normalize\")\n",
    "        return np.full_like(pose_sequence, np.nan)\n",
    "    \n",
    "    normalized_frames = []\n",
    "    num_coords = pose_sequence.shape[1]\n",
    "    \n",
    "    if num_coords == 0 or num_coords % 2 != 0 or num_coords != len(joint_names) * 2:\n",
    "        print(f\"Coordinate mismatch in normalize_pose_sequence ({num_coords} vs {len(joint_names)*2}).\")\n",
    "        return np.full_like(pose_sequence, np.nan)\n",
    "\n",
    "    for frame in pose_sequence:\n",
    "        try:\n",
    "             if frame.shape[0] != num_coords: \n",
    "                 normalized_frames.append(np.full(num_coords, np.nan))\n",
    "                 continue\n",
    "             frame_joints = frame.reshape(-1, 2)\n",
    "             if frame_joints.shape[0] != len(joint_names): \n",
    "                 normalized_frames.append(np.full(num_coords, np.nan))\n",
    "                 continue\n",
    "             ref1, ref2 = get_keypoint(frame_joints, ref_joint1_idx), get_keypoint(frame_joints, ref_joint2_idx)\n",
    "            \n",
    "             if ref1 is None or ref2 is None: \n",
    "                 normalized_frames.append(np.full(num_coords, np.nan))\n",
    "                 continue\n",
    "            \n",
    "             center, scale = (np.array(ref1) + np.array(ref2)) / 2.0, np.linalg.norm(np.array(ref1) - np.array(ref2))\n",
    "             if scale == 0 or np.isclose(scale, 0) or np.isnan(scale): \n",
    "                 normalized_frames.append(np.full(num_coords, np.nan))\n",
    "                 continue\n",
    "             normalized_frames.append(((frame_joints - center) / scale).flatten())\n",
    "        except Exception as e:\n",
    "            normalized_frames.append(np.full(num_coords, np.nan))\n",
    "            continue\n",
    "    \n",
    "    return np.array(normalized_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899eedbc-f5dd-4823-9d27-0d92d1696548",
   "metadata": {},
   "source": [
    "### Trajectory Calculation\n",
    "\n",
    "This cell is to take a pose sequence andmcalculates a 1-dimensional time series that represents the key repetitive movement of that exercise.\n",
    "\n",
    "**`calculate_trajectory(pose_sequence, joint_names, trajectory_config)`:**\n",
    "* Calculate the 1D trajectory signal according to the rules specified in the `trajectory_config` dictionary. It handles different calculation methods ('metrics') like joint angles or coordinate positions\n",
    "    *  Parameters:\n",
    "        * `pose_sequence`: (`numpy.array`, Unit = Normalized coordinates) - The cleaned, spatially normalized pose sequence (frames, coords).\n",
    "        * `joint_names`: (`list` of `str`) - Ordered list of joint names.\n",
    "        * `trajectory_config`: (`dict`) - The specific `trajectory_logic` dictionary for the current exercise, extracted from `EXERCISE_CONFIG`.\n",
    "    * Outcome: Returns a tuple containing:\n",
    "        * `trajectory`: (`numpy.array` or `None`) - A 1D NumPy array `(num_frames,)` with the calculated metric over time. Returns `None` if fails.\n",
    "        * `invert_for_valley`: (`bool`) - Indicating if the peak finding logic should invert this trajectory to find valleys.\n",
    "    * Unit = degrees for `_angle` metrics, normalized units for `_y` or `_x` metrics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a950803-5f12-464a-9fcf-543c676cdc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trajectory(pose_sequence, joint_names, trajectory_config):\n",
    "    \"\"\"Calculates the 1D trajectory signal based on configuration.\"\"\"\n",
    "    if pose_sequence is None or pose_sequence.ndim!= 2 or pose_sequence.shape[0] < 2: \n",
    "        return None, False\n",
    "\n",
    "    metric = trajectory_config.get('metric', 'unknown')\n",
    "    base_joint_names = trajectory_config.get('joints', [])\n",
    "    invert_for_valley = trajectory_config.get('invert_for_valley', False)\n",
    "    trajectory = None\n",
    "\n",
    "    try:\n",
    "        if not base_joint_names: \n",
    "            raise ValueError(\"No 'joints' specified in trajectory_config\")\n",
    "\n",
    "        if metric.endswith('_angle'):\n",
    "            if len(base_joint_names) != 3: \n",
    "                raise ValueError(f\"{metric} needs 3 joints in config\")\n",
    "            \n",
    "            j1_base, j2_base, j3_base = base_joint_names[0], base_joint_names[1], base_joint_names[2]\n",
    "            angles_left, angles_right = [], []\n",
    "            try: \n",
    "                l_j1_idx = joint_names.index(f'Left {j1_base}')\n",
    "                l_j2_idx = joint_names.index(f'Left {j2_base}')\n",
    "                l_j3_idx = joint_names.index(f'Left {j3_base}')\n",
    "                r_j1_idx = joint_names.index(f'Right {j1_base}')\n",
    "                r_j2_idx = joint_names.index(f'Right {j2_base}')\n",
    "                r_j3_idx = joint_names.index(f'Right {j3_base}')\n",
    "                indices_valid = True\n",
    "            except (ValueError, IndexError): \n",
    "                indices_valid = False \n",
    "\n",
    "            if indices_valid:\n",
    "                for frame_coords in pose_sequence:\n",
    "                    f_j = frame_coords.reshape(-1, 2)\n",
    "                    p1l,p2l,p3l = get_keypoint(f_j,l_j1_idx), get_keypoint(f_j,l_j2_idx), get_keypoint(f_j,l_j3_idx)\n",
    "                    p1r,p2r,p3r = get_keypoint(f_j,r_j1_idx), get_keypoint(f_j,r_j2_idx), get_keypoint(f_j,r_j3_idx)\n",
    "                    angles_left.append(calculate_angle(p1l, p2l, p3l) if all(p is not None for p in [p1l, p2l, p3l]) else np.nan)\n",
    "                    angles_right.append(calculate_angle(p1r, p2r, p3r) if all(p is not None for p in [p1r, p2r, p3r]) else np.nan)\n",
    "                traj_l, traj_r = np.array(angles_left), np.array(angles_right)\n",
    "                trajectory = np.nanmean([traj_l, traj_r], axis=0) \n",
    "            else: \n",
    "                 j1_idx, j2_idx, j3_idx = joint_names.index(j1_base), joint_names.index(j2_base), joint_names.index(j3_base)\n",
    "                 angles = []\n",
    "                 for frame_coords in pose_sequence:\n",
    "                      f_j = frame_coords.reshape(-1, 2)\n",
    "                      p1, p2, p3 = get_keypoint(f_j, j1_idx), get_keypoint(f_j, j2_idx), get_keypoint(f_j, j3_idx)\n",
    "                      angles.append(calculate_angle(p1, p2, p3) if all(p is not None for p in [p1, p2, p3]) else np.nan)\n",
    "                 trajectory = np.array(angles)\n",
    "\n",
    "        elif metric.endswith('_y') or metric.endswith('_x'):\n",
    "            coord_idx = 1 if metric.endswith('_y') else 0 \n",
    "            base_joint = base_joint_names[0]\n",
    "            try: \n",
    "                 l_joint_idx = joint_names.index(f'Left {base_joint}')\n",
    "                 r_joint_idx = joint_names.index(f'Right {base_joint}')\n",
    "                 if max(2*l_joint_idx+coord_idx, 2*r_joint_idx+coord_idx) >= pose_sequence.shape[1]: \n",
    "                     raise ValueError(\"Joint index out of bounds\")\n",
    "                 l_coord = pose_sequence[:, 2 * l_joint_idx + coord_idx]\n",
    "                 r_coord = pose_sequence[:, 2 * r_joint_idx + coord_idx]\n",
    "                 trajectory = np.nanmean([l_coord, r_coord], axis=0)\n",
    "            except (ValueError, IndexError):\n",
    "                 joint_idx = joint_names.index(base_joint)\n",
    "                 if 2*joint_idx+coord_idx >= pose_sequence.shape[1]: \n",
    "                     raise ValueError(\"Joint index out of bounds\")\n",
    "                 trajectory = pose_sequence[:, 2 * joint_idx + coord_idx]\n",
    "        else:\n",
    "             raise ValueError(f\"Unknown metric type given in config: '{metric}'\")\n",
    "\n",
    "        if trajectory is None or trajectory.shape[0] == 0 or np.all(np.isnan(trajectory)): \n",
    "            return None, False\n",
    "        nan_mask = np.isnan(trajectory)\n",
    "        if np.any(nan_mask):\n",
    "            indices = np.arange(len(trajectory)) \n",
    "            valid_indices = np.flatnonzero(~nan_mask)\n",
    "            if len(valid_indices) < 2: \n",
    "                return None, False\n",
    "            trajectory[nan_mask] = np.interp(indices[nan_mask], valid_indices, trajectory[valid_indices])\n",
    "        if np.any(np.isnan(trajectory)):\n",
    "             trajectory = np.nan_to_num(trajectory, nan=np.nanmean(trajectory))\n",
    "             if np.any(np.isnan(trajectory)): trajectory = np.nan_to_num(trajectory, nan=0.0)\n",
    "        return trajectory, invert_for_valley\n",
    "\n",
    "    except (ValueError, IndexError) as e: \n",
    "        print(f\"Error finding/using joints for metric '{metric}': {e}\")\n",
    "        return None, False\n",
    "    except Exception as e: \n",
    "        print(f\"Error calculating trajectory with metric '{metric}': {e}\")\n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41085b6e-da27-401a-a003-c4208016dfd1",
   "metadata": {},
   "source": [
    "### Extract trajectory\n",
    "\n",
    "This cell takes a numerical trajectory (movement over time) and identify the locations (frame indices) of significant peaks within that signal. By using the `invert_trajectory`, it can also find significant valleys. This peak/valley detection is used for identifying individual repetition boundaries in the functions `segment_repetitions` and `count_repetitions`.\n",
    "\n",
    "1.  **`extract_trajectory(trajectory, smoothing_window, prominence, distance, invert_trajectory=False)`:**\n",
    "    * To smooth an input trajectory signal and find the indices of peaks that meet specific prominence and distance criteria.\n",
    "    *   Parameters:\n",
    "        * `trajectory`: (`numpy.array`, Unit = Varies - degrees, normalized units) - The 1D input signal calculated by `calculate_trajectory`.\n",
    "        * `smoothing_window`: (`int`, Unit = Frames) - The number of frames to use for the moving average filter applied before peak detection.\n",
    "        * `prominence`: (`float`, Unit = Unitless ratio) - Relative prominence factor. A peak must stand out from its surrounding troughs by at least this fraction of the smoothed signal's overall range.\n",
    "        * `distance`: (`int`, Unit = Frames) - The minimum number of frames required between consecutive detected peaks.\n",
    "        * `invert_trajectory`: (`bool`) - If `True`, the trajectory is inverted (`-trajectory`) before smoothing and peak finding. This effectively finds significant valleys (local minima) in the original signal.\n",
    "    * A 1D NumPy array containing the frame indices where valid peaks were detected in the unsmoothed trajectory's timeline, or `None` if the input is invalid, smoothing/peak finding fails, or no peaks are found.\n",
    "    * Unit = Frame index (unitless count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7acf015d-9c4d-4fae-9b1e-5f31bed2a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trajectory(trajectory, smoothing_window, prominence, distance, invert_trajectory=False):\n",
    "    \"\"\"detect a trajectory and find peaks.\"\"\"\n",
    "    if trajectory is None or trajectory.ndim != 1 or trajectory.shape[0] < max(distance * 2, smoothing_window, 1): \n",
    "        return None\n",
    "\n",
    "    processed_trajectory = trajectory.copy()\n",
    "    if invert_trajectory: processed_trajectory = -processed_trajectory\n",
    "    if len(processed_trajectory) < smoothing_window: \n",
    "        return None\n",
    "\n",
    "    smoothed_trajectory = np.convolve(processed_trajectory, np.ones(smoothing_window)/smoothing_window, mode='valid')\n",
    "    if smoothed_trajectory.shape[0] < max(distance, 1): \n",
    "        return None\n",
    "\n",
    "    data_range = np.ptp(smoothed_trajectory)\n",
    "    required_prominence = max(data_range * prominence, 1e-6) if data_range > 1e-9 else 1e-6\n",
    "\n",
    "    try:\n",
    "        smoothed_indices_offset = (len(processed_trajectory) - len(smoothed_trajectory)) // 2\n",
    "        peak_indices_smoothed, _ = find_peaks(smoothed_trajectory, prominence=required_prominence, distance=distance)\n",
    "        peak_indices_original = peak_indices_smoothed + smoothed_indices_offset\n",
    "        peak_indices_original = peak_indices_original[(peak_indices_original >= 0) & (peak_indices_original < len(processed_trajectory))]\n",
    "        return peak_indices_original\n",
    "    except Exception as e: \n",
    "        print(f\"Peak finding failed: {e}\") \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83c81f-6351-4d6b-b9e9-2e14acae48db",
   "metadata": {},
   "source": [
    "### Segmentation and Counting\n",
    "\n",
    "This cell is analyzing the movement trajectory to identify repetitions.\n",
    "\n",
    "1.  **`segment_repetitions(pose_sequence, exercise_label, joint_names, exercise_config, smoothing_window=Smoothing_window_size, peak_prominence=Default_Peak_Prominence, peak_distance=Default_Peak_Distance)`:**\n",
    "    * To divide a given `pose_sequence` (assumed to contain multiple repetitions of the specified `exercise_label`) into a list of smaller sequences, where each smaller sequence corresponds to a single detected repetition.\n",
    "    *   Parameters:\n",
    "        * `pose_sequence`: (`numpy.array`, Unit = Normalized coordinates) - The input sequence to segment.\n",
    "        * `exercise_label`: (`str`) - The name of the exercise, used to look up the correct processing logic in `exercise_config`.\n",
    "        * `joint_names`: (`list` of `str`) - Ordered list of joint names.\n",
    "        * `exercise_config`: (`dict`) - The main configuration dictionary containing trajectory logic for all exercises.\n",
    "        * `smoothing_window`, `peak_prominence`, `peak_distance`: (`int`/`float`, Unit = Frames/Unitless ratio) - Parameters controlling the peak detection process\n",
    "    * A list, where each element is a `numpy.array` representing the pose data for a single detected repetition segment (shape `(segment_frames, num_coords)`). Returns an empty list if no valid repetitions are found or if the configuration is missing.\n",
    "    * Unit = Each array contains normalized coordinates.\n",
    "\n",
    "2.  **`count_repetitions(pose_sequence, exercise_label, joint_names, exercise_config, prominence=Default_Peak_Prominence, distance=Default_Peak_Distance, smoothing_window=Smoothing_window_size)`:**\n",
    "    * To estimate the total number of repetitions within the input `pose_sequence` for the given `exercise_label` using `prominence` and `distance` parameters (values found during baseline loading).\n",
    "    *   Parameters:\n",
    "        * `pose_sequence`: (`numpy.array`, Unit = Normalized coordinates) - The input sequence to analyze.\n",
    "        * `exercise_label`: (`str`) - The name of the exercise.\n",
    "        * `joint_names`: (`list` of `str`) - Ordered list of joint names.\n",
    "        * `exercise_config`: (`dict`) - The main configuration dictionary.\n",
    "        * `prominence`, `distance`, `smoothing_window`: (`float`/`int`, Unit = Unitless ratio/Frames) - Peak detection parameters.\n",
    "    * The estimated number of repetitions detected in the sequence. Returns 0 if the config is missing or no peaks are found.\n",
    "    * Unit = Count (unitless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "737a1455-c2c8-49c6-93b7-b664c3ea6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_repetitions(pose_sequence, exercise_label, joint_names, exercise_config, smoothing_window=Smoothing_window_size,\n",
    "                        peak_prominence=Default_Peak_Prominence, peak_distance=Default_Peak_Distance):\n",
    "    \"\"\"Segments a pose sequence into individual repetitions using EXERCISE_CONFIG.\"\"\"\n",
    "    \n",
    "    current_exercise_cfg_dict = exercise_config.get(exercise_label, {})\n",
    "    trajectory_logic_cfg = current_exercise_cfg_dict.get('trajectory_logic')\n",
    "    if not trajectory_logic_cfg:\n",
    "        return []\n",
    "\n",
    "    trajectory, invert = calculate_trajectory(pose_sequence, joint_names, trajectory_logic_cfg)\n",
    "    peak_indices = extract_trajectory(trajectory, smoothing_window, peak_prominence, peak_distance, invert_trajectory=invert)\n",
    "\n",
    "    if peak_indices is None or len(peak_indices) < 2: \n",
    "        return []\n",
    "    repetitions = [pose_sequence[peak_indices[i]:peak_indices[i+1], :] for i in range(len(peak_indices) - 1)]\n",
    "    return [rep for rep in repetitions if rep.shape[0] > 1]\n",
    "\n",
    "def count_repetitions(pose_sequence, exercise_label, joint_names, exercise_config, prominence=Default_Peak_Prominence, distance=Default_Peak_Distance,\n",
    "                      smoothing_window=Smoothing_window_size):\n",
    "    \"\"\"Counts repetitions using EXERCISE_CONFIG and specified parameters.\"\"\"\n",
    "    \n",
    "    current_exercise_cfg_dict = exercise_config.get(exercise_label)\n",
    "    if not current_exercise_cfg_dict: \n",
    "        return 0\n",
    "    trajectory_logic_cfg = current_exercise_cfg_dict.get('trajectory_logic')\n",
    "    if not trajectory_logic_cfg: \n",
    "        return 0\n",
    "\n",
    "    trajectory, invert = calculate_trajectory(pose_sequence, joint_names, trajectory_logic_cfg)\n",
    "    peak_indices = extract_trajectory(trajectory, smoothing_window, prominence, distance, invert_trajectory=invert)\n",
    "\n",
    "    return len(peak_indices) if peak_indices is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6048d-cf9c-47ef-88c9-7ba12dca2b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5227bafb-c16c-4528-9652-469cd4b23994",
   "metadata": {},
   "source": [
    "### Parameter Tuning Function\n",
    "\n",
    "This cell finds the optimal `peak_prominence` and `peak_distance` values for the repetition counting. It does this by trying out different parameter combinations on the baseline videos for per exercise and selecting the parameter pair that minimizes the total counting error.\n",
    "\n",
    "1.  **`tune_counting_parameters(exercise_label, labeled_video_data, joint_names, exercise_config, prominence_range, distance_range)`:**\n",
    "    * To perform a grid search over `prominence_range` and `distance_range` to find the combination of `peak_prominence` and `peak_distance` that most accurately counts repetitions across the provided `labeled_video_data` for the given `exercise_label`.\n",
    "    *  Parameters:\n",
    "        * `exercise_label`: (`str`) - The name of the exercise being tuned.\n",
    "        * `labeled_video_data`: (`list` of `tuple`) - Data for tuning. Each element is `(video_index, (cleaned_pose_sequence, true_reps))`. The `cleaned_pose_sequence` is the spatially normalized sequence *before* time normalization. `true_reps` is the ground truth count parsed from the filename.\n",
    "        * `joint_names`: (`list` of `str`) - Ordered list of joint names.\n",
    "        * `exercise_config`: (`dict`) - The main configuration dictionary.\n",
    "        * `prominence_range`: (`list` of `float`) - List of `peak_prominence` values to test.\n",
    "        * `distance_range`: (`list` of `int`) - List of `peak_distance` values to test.\n",
    "    * Returns a tuple containing the best `(prominence, distance)` parameters found for this exercise and prints summary of search process\n",
    "    * Unit = Prominence is a unitless ratio, Distance is in frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a50c6ace-9be4-46d9-961e-6117a780a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_counting_parameters(exercise_label, labeled_video_data, joint_names, exercise_config, prominence_range, distance_range):\n",
    "    \"\"\"Performs grid search using the refactored peak finding and config.\"\"\"\n",
    "    print(f\" Tuning repetition counting parameters for: {exercise_label}...\")\n",
    "    \n",
    "    best_params = (Default_Peak_Prominence, Default_Peak_Distance)\n",
    "    \n",
    "    min_total_error = float('inf')\n",
    "    \n",
    "    if not labeled_video_data: \n",
    "        print(\"No labeled data provided for tuning.\")\n",
    "        return best_params\n",
    "\n",
    "    current_exercise_cfg_dict = exercise_config.get(exercise_label)\n",
    "    \n",
    "    if not current_exercise_cfg_dict: \n",
    "        print(f\"No config for {exercise_label} in tuning\")\n",
    "        return best_params\n",
    "    \n",
    "    trajectory_logic_cfg = current_exercise_cfg_dict.get('trajectory_logic')\n",
    "    \n",
    "    if not trajectory_logic_cfg: \n",
    "        print(f\"No trajectory_logic in config for {exercise_label}\")\n",
    "        return best_params\n",
    "\n",
    "    num_combinations = len(prominence_range) * len(distance_range)\n",
    "    \n",
    "    default_error = 0\n",
    "    \n",
    "    precalculated_trajectories = {}\n",
    "\n",
    "    for i, (vid_idx, (sequence, true_reps)) in enumerate(labeled_video_data): \n",
    "        if sequence is None or sequence.ndim != 2 or sequence.shape[0] < 2: \n",
    "            continue\n",
    "        trajectory, invert = calculate_trajectory(sequence, joint_names, trajectory_logic_cfg)\n",
    "        \n",
    "        if trajectory is not None: \n",
    "            precalculated_trajectories[vid_idx] = (trajectory, invert, true_reps)\n",
    "\n",
    "    if not precalculated_trajectories: \n",
    "        print(\"No valid trajectories calculated for tuning.\") \n",
    "        return best_params\n",
    "\n",
    "    for vid_idx, (traj, inv, true_r) in precalculated_trajectories.items():\n",
    "         peak_indices_def = extract_trajectory(traj, Smoothing_window_size, Default_Peak_Prominence, Default_Peak_Distance, invert_trajectory=inv)\n",
    "         calc_reps_def = len(peak_indices_def) if peak_indices_def is not None else 0\n",
    "         default_error += abs(calc_reps_def - true_r)\n",
    "\n",
    "    # Grid search\n",
    "    for p_idx, p in enumerate(prominence_range):\n",
    "        for d_idx, d in enumerate(distance_range):\n",
    "            current_total_error = 0\n",
    "            for vid_idx, (traj, inv, true_r) in precalculated_trajectories.items():\n",
    "                peak_indices = extract_trajectory(traj, Smoothing_window_size, p, d, invert_trajectory=inv)\n",
    "                calculated_reps = len(peak_indices) if peak_indices is not None else 0\n",
    "                current_total_error += abs(calculated_reps - true_r)\n",
    "\n",
    "            if current_total_error < min_total_error:\n",
    "                min_total_error = current_total_error\n",
    "                best_params = (p, d)\n",
    "            if min_total_error == 0: \n",
    "                break\n",
    "        if min_total_error == 0: \n",
    "            break\n",
    "\n",
    "    print(f\"Tuning complete. Best Params: P={best_params[0]:.3f}, D={best_params[1]}, Min Error={min_total_error}. (Error with default parameters = {default_error})\")\n",
    "\n",
    "    if precalculated_trajectories:\n",
    "        print(f\"Detected reps using Best params (P={best_params[0]:.3f}, D={best_params[1]}):\")\n",
    "        total_calculated, total_true = 0, 0\n",
    "\n",
    "        for vid_idx, (traj, inv, true_r) in sorted(precalculated_trajectories.items()):\n",
    "            peak_indices = extract_trajectory(traj, Smoothing_window_size, best_params[0], best_params[1], invert_trajectory=inv)\n",
    "            calculated_reps = len(peak_indices) if peak_indices is not None else 0\n",
    "            print(f\"Video Index {vid_idx}: Detected={calculated_reps}, True={true_r}\")   ###### nog naam van videofile nog toevoegen/vervangen\n",
    "            total_calculated += calculated_reps\n",
    "            total_true += true_r\n",
    "        print(f\"Total Reps: Detected={total_calculated}, True={total_true}, Overall Error={abs(total_calculated - total_true)}\")\n",
    "        \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989fb71e-d093-4be5-90a6-fc017222b166",
   "metadata": {},
   "source": [
    "### Loading baseline videos\n",
    "\n",
    "This cell processing all baseline video files. It iterates through each exercise folder provided in `Video_baseline_path`.\n",
    "* Inner logic:\n",
    "    * Parses Information: Extracts the ground truth repetition count (`XX`) and camera angle (`'left'`, `'right'`, or `'front'`) from each baseline video's filename (using the format `exercise_label_XXreps_angle_something.mp4`).\n",
    "    * Processes Videos: Extracts and normalizes pose sequences for each video.\n",
    "    * Segment Repetitions: Breaks down each video's cleaned sequence into individual repetition segments\n",
    "    * Groups by Angle: Groups the time-normalized repetition segments + labeled data (sequence + true reps) based on the `parsed_angle` for each exercise.\n",
    "    * Tunes Parameters per Angle: For each angle category ('left', 'right', 'front', 'unknown') finds the optimal peak detection settings.\n",
    "    * Averages Baselines per Angle: For each angle category with data, it calculates and stores the average time-normalized repetition sequence.\n",
    "    * Derives Thresholds per Angle: For each angle category, it calculates a suggested DTW threshold based on the variation between individual reps\n",
    "    * Calculates Overall Threshold: After processing all angles, it averages all valid reps per exercise to create an 'overall' baseline threshold.\n",
    "\n",
    "1.  **`load_baseline_data(Video_baseline_path, hrnet_model, joint_names, Target_rep_length, exercise_config, prominence_range, distance_range, threshold_std_multiplier=Threshold_STD_Multiplier, min_reps_for_threshold=Min_REPS_to_calc_threshold)`:**\n",
    "    * Load baseline videos, organize data by exercise and parsed filename angle, tune counting parameters per angle, create averaged baselines per angle and overall\n",
    "    * Outcome: Returns a tuple containing three nested dictionaries:\n",
    "        1.  `averaged_baseline_data`: (`dict`) Structure: `{exercise_label: {angle_key: numpy.array}}`. Contains the averaged time-normalized pose sequence for each exercise under keys for detected angles ('left', 'right', 'front', 'unknown') and 'overall'.\n",
    "        2.  `tuned_counting_params`: (`dict`) Structure: `{exercise_label: {angle_key: (prominence, distance)}}`. Contains the best `(prominence, distance)` tuple found for counting reps for each exercise/angle combination.\n",
    "        3.  `exercise_specific_thresholds`: (`dict`) Structure: `{exercise_label: {angle_key: float or None}}`. Contains the derived DTW threshold calculated for each exercise/angle combination. Value is `None` if too few reps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6547ecfa-5d8b-4ab7-b7ef-79ef916d4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_baseline_data(Video_baseline_path, hrnet_model, joint_names, Target_rep_length, exercise_config, prominence_range, distance_range,\n",
    "                       threshold_std_multiplier=Threshold_STD_Multiplier, min_reps_for_threshold=Min_REPS_to_calc_threshold):\n",
    "    \"\"\"\n",
    "    Loads baselines, parses angle from filename, tunes params per angle, averages per angle and overall and calculates thresholds.\n",
    "    Returns dicts: averaged_baselines, tuned_counting_params, exercise_specific_thresholds\n",
    "    \"\"\"\n",
    "    averaged_baseline_data = {}\n",
    "    tuned_counting_params = {}\n",
    "    exercise_specific_thresholds = {}\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    print(f\"Loading Baselines (Config-Driven, Angle from Filename)... from: {Video_baseline_path}\")\n",
    "    if not os.path.isdir(Video_baseline_path):\n",
    "        print(f\"Baseline directory not found: {Video_baseline_path}\")\n",
    "        return averaged_baseline_data, tuned_counting_params, exercise_specific_thresholds\n",
    "\n",
    "    for exercise_label_raw in sorted(os.listdir(Video_baseline_path)):\n",
    "        exercise_folder_path = os.path.join(Video_baseline_path, exercise_label_raw)\n",
    "        if not os.path.isdir(exercise_folder_path): \n",
    "            continue\n",
    "        exercise_label = exercise_label_raw.lower().replace(\" \", \"_\")\n",
    "        print(f\"\\nProcessing baseline exercise: {exercise_label} (from folder: {exercise_label_raw})\")\n",
    "\n",
    "        current_exercise_config_dict = exercise_config.get(exercise_label)\n",
    "        if not current_exercise_config_dict:\n",
    "            print(f\" No config found for '{exercise_label}'. Skipping.\") \n",
    "            continue\n",
    "\n",
    "        all_reps_by_angle = {'left': [], 'right': [], 'front': [], 'unknown': []}\n",
    "        labeled_data_by_angle = {'left': [], 'right': [], 'front': [], 'unknown': []} # Stores (vid_idx, (sequence, true_reps))\n",
    "        averaged_baseline_data[exercise_label] = {}\n",
    "        tuned_counting_params[exercise_label] = {}\n",
    "        exercise_specific_thresholds[exercise_label] = {}\n",
    "\n",
    "        video_files = sorted([f for f in os.listdir(exercise_folder_path) if f.lower().endswith(('.mp4', '.avi', '.mov'))])\n",
    "        print(f\"Found {len(video_files)} video file(s).\")\n",
    "\n",
    "        for video_idx, video_file in enumerate(video_files):\n",
    "            video_path = os.path.join(exercise_folder_path, video_file)\n",
    "            true_reps, parsed_angle = None, 'unknown'\n",
    "            rep_match = re.search(r'_(\\d+)reps', video_file, re.IGNORECASE)\n",
    "            if rep_match: true_reps = int(rep_match.group(1))\n",
    "            angle_match = re.search(r'_(left|right|front)', video_file, re.IGNORECASE)\n",
    "            if angle_match: parsed_angle = angle_match.group(1).lower()\n",
    "\n",
    "            raw_seq = process_single_video(video_path, hrnet_model)\n",
    "            \n",
    "            if raw_seq is None or raw_seq.shape[0] == 0: \n",
    "                continue\n",
    "            \n",
    "            spatially_norm_seq = normalize_pose_sequence(raw_seq, joint_names)\n",
    "            \n",
    "            if spatially_norm_seq is None or np.all(np.isnan(spatially_norm_seq)): \n",
    "                continue\n",
    "            cleaned_seq = handle_nan_values(spatially_norm_seq)\n",
    "            if cleaned_seq.shape[0] < 2: \n",
    "                continue\n",
    "\n",
    "            if true_reps is not None:\n",
    "                 labeled_data_by_angle[parsed_angle].append((video_idx, (cleaned_seq, true_reps)))\n",
    "\n",
    "            repetitions = segment_repetitions(cleaned_seq, exercise_label, joint_names, exercise_config)\n",
    "            if not repetitions: \n",
    "                continue\n",
    "\n",
    "            for rep_segment in repetitions:\n",
    "                if rep_segment is not None and rep_segment.shape[0] >= 2:\n",
    "                    time_norm_rep = time_normalize_sequence(rep_segment, Target_rep_length)\n",
    "                    expected_coords = len(joint_names) * 2\n",
    "                    if not np.isnan(time_norm_rep).all() and time_norm_rep.shape == (Target_rep_length, expected_coords):\n",
    "                        all_reps_by_angle[parsed_angle].append(time_norm_rep)\n",
    "\n",
    "        all_reps_across_angles = []\n",
    "        for angle in ['left', 'right', 'front', 'unknown']:\n",
    "            labeled_data_for_this_angle = labeled_data_by_angle[angle]\n",
    "            normalized_reps_for_this_angle = all_reps_by_angle[angle]\n",
    "            \n",
    "            if not normalized_reps_for_this_angle: \n",
    "                continue\n",
    "                \n",
    "            all_reps_across_angles.extend(normalized_reps_for_this_angle)\n",
    "\n",
    "            if labeled_data_for_this_angle:\n",
    "                 tuned_p, tuned_d = tune_counting_parameters(exercise_label, labeled_data_for_this_angle,\n",
    "                                                            joint_names, exercise_config, prominence_range, distance_range)\n",
    "                 tuned_counting_params[exercise_label][angle] = (tuned_p, tuned_d)\n",
    "            else: \n",
    "                tuned_counting_params[exercise_label][angle] = (Default_Peak_Prominence, Default_Peak_Distance)\n",
    "\n",
    "            reps_stack = np.stack(normalized_reps_for_this_angle, axis=0)\n",
    "            averaged_sequence_angle = np.nanmean(reps_stack, axis=0)\n",
    "            averaged_baseline_data[exercise_label][angle] = np.nan_to_num(averaged_sequence_angle, nan=0.0)\n",
    "\n",
    "            derived_threshold_angle = None\n",
    "            if len(normalized_reps_for_this_angle) >= min_reps_for_threshold:\n",
    "                intra_distances_angle = []\n",
    "                safe_avg_angle = np.nan_to_num(averaged_sequence_angle, nan=0.0)\n",
    "                for norm_rep in normalized_reps_for_this_angle:\n",
    "                     safe_rep = np.nan_to_num(norm_rep, nan=0.0)\n",
    "                     if safe_rep.shape == safe_avg_angle.shape:\n",
    "                         try: \n",
    "                             dist, _ = fastdtw(safe_rep, safe_avg_angle, dist=euclidean) \n",
    "                             intra_distances_angle.append(dist)\n",
    "                         except Exception: \n",
    "                             pass\n",
    "                if len(intra_distances_angle) >= min_reps_for_threshold:\n",
    "                     mean_dist, std_dist = np.mean(intra_distances_angle), np.std(intra_distances_angle)\n",
    "                     derived_threshold_angle = mean_dist + threshold_std_multiplier * std_dist\n",
    "            exercise_specific_thresholds[exercise_label][angle] = derived_threshold_angle\n",
    "\n",
    "        if len(all_reps_across_angles) >= min_reps_for_threshold:\n",
    "             overall_reps_stack = np.stack(all_reps_across_angles, axis=0)\n",
    "             overall_averaged_sequence = np.nanmean(overall_reps_stack, axis=0)\n",
    "             averaged_baseline_data[exercise_label]['overall'] = np.nan_to_num(overall_averaged_sequence, nan=0.0)\n",
    "             overall_intra_distances = []\n",
    "             safe_overall_avg = np.nan_to_num(overall_averaged_sequence, nan=0.0)\n",
    "             for norm_rep in all_reps_across_angles:\n",
    "                  safe_rep = np.nan_to_num(norm_rep, nan=0.0)\n",
    "                  if safe_rep.shape == safe_overall_avg.shape:\n",
    "                       try: \n",
    "                           dist, _ = fastdtw(safe_rep, safe_overall_avg, dist=euclidean) \n",
    "                           overall_intra_distances.append(dist)\n",
    "                       except Exception: \n",
    "                           pass\n",
    "             if len(overall_intra_distances) >= min_reps_for_threshold:\n",
    "                  mean_dist, std_dist = np.mean(overall_intra_distances), np.std(overall_intra_distances)\n",
    "                  derived_threshold_overall = mean_dist + threshold_std_multiplier * std_dist\n",
    "                  exercise_specific_thresholds[exercise_label]['overall'] = derived_threshold_overall\n",
    "             else: \n",
    "                 exercise_specific_thresholds[exercise_label]['overall'] = None\n",
    "        else:\n",
    "             averaged_baseline_data[exercise_label]['overall'] = None\n",
    "             exercise_specific_thresholds[exercise_label]['overall'] = None\n",
    "\n",
    "    end_time_total = time.time()\n",
    "    print(f\"\\nBaseline processing complete in {time.time() - start_time_total:.2f}s.\")\n",
    "    return averaged_baseline_data, tuned_counting_params, exercise_specific_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62343d38-6938-421c-ba14-7f5a5350572c",
   "metadata": {},
   "source": [
    "### Recognition Function\n",
    "\n",
    "This cell takes a new video file and compares it against the baseline data to classify the exercise performed in each detected repetition.\n",
    "\n",
    "1.  **`recognize_exercise(new_video_path, averaged_baseline_data, hrnet_model, joint_names, exercise_config, Target_rep_length, tuned_counting_params, exercise_specific_thresholds, hint_angle=None, fallback_threshold=DTW_Distance_threshold)`:**\n",
    "    * To process a new video, segment it into repetitions (using parameters potentially tuned for an initial best guess exercise and the hinted angle), compare each time-normalized repetition segment against angle-specific (or overall) averaged baselines using DTW, apply exercise- and angle-specific (or overall/fallback) thresholds, and return the classification result for each repetition.\n",
    "    * Parameters:\n",
    "        * `new_video_path`: (`str` or `pathlib.Path`) - Path to the video file to be analyzed.\n",
    "        * `averaged_baseline_data`: (`dict`) - Nested dictionary `{exercise: {angle: avg_sequence}}` returned by `load_baseline_data`.\n",
    "        * `hrnet_model`: (`SimpleHRNet` object) - The loaded pose estimation model.\n",
    "        * `joint_names`: (`list` of `str`) - Ordered list of joint names.\n",
    "        * `exercise_config`: (`dict`) - The main configuration dictionary defining exercise-specific logic.\n",
    "        * `Target_rep_length`: (`int`, Unit = Frames) - Target length for time normalization.\n",
    "        * `tuned_counting_params`: (`dict`) - Nested dictionary `{exercise: {angle: (prominence, distance)}}` returned by `load_baseline_data`.\n",
    "        * `exercise_specific_thresholds`: (`dict`) - Nested dictionary `{exercise: {angle: threshold}}` returned by `load_baseline_data`.\n",
    "        * `hint_angle`: (`str` or `None`, Unit = Category label) - Optional hint ('left', 'right', 'front'). If provided, comparison uses this angle's baselines/thresholds. If `None`, uses 'overall' baselines/thresholds. Default is `None`.\n",
    "        * `fallback_threshold`: (`float`, Unit = Unitless Distance) - The global DTW threshold used if no specific derived threshold is available for the exercise/angle combination being compared.\n",
    "    * Outcome: Returns a tuple containing:\n",
    "        1.  `repetition_results`: (`list` of `tuple`) - A list with tuples representing a detected repetition: `(rep_index, matched_label, min_distance)`.\n",
    "            * `rep_index`: (`int`) index.\n",
    "            * `matched_label`: (`str`) Contains the name of the matched exercise if `min_distance` was below the relevant threshold, otherwise \"No Match / Inconclusive (...)\"\n",
    "            * `min_distance`: (`float`) The lowest DTW distance found when comparing this repetition against the baselines.\n",
    "        2.  `num_detected_reps`: (`int`) - The total number of segments detected in the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "592253a8-f2e7-4180-89c3-ce99e497ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_exercise(new_video_path, averaged_baseline_data, hrnet_model, joint_names, exercise_config,\n",
    "                       Target_rep_length, tuned_counting_params, exercise_specific_thresholds,\n",
    "                       hint_angle=None, fallback_threshold=DTW_Distance_threshold):\n",
    "    \"\"\"Recognizes exercise\"\"\"\n",
    "    print(f\"\\n Recognizing Exercise (Angle Hint='{hint_angle}') for: {os.path.basename(new_video_path)} ---\")\n",
    "    start_time = time.time()\n",
    "    if tuned_counting_params is None: \n",
    "        tuned_counting_params = {}\n",
    "    if exercise_specific_thresholds is None: \n",
    "        exercise_specific_thresholds = {}\n",
    "    default_p, default_d = Default_Peak_Prominence, Default_Peak_Distance\n",
    "\n",
    "    new_raw_seq = process_single_video(new_video_path, hrnet_model)\n",
    "    if new_raw_seq is None or new_raw_seq.shape[0] == 0: \n",
    "        print(\"Failed processing video.\")\n",
    "        return [], 0\n",
    "    new_spatially_norm_seq = normalize_pose_sequence(new_raw_seq, joint_names)\n",
    "    if new_spatially_norm_seq is None or np.all(np.isnan(new_spatially_norm_seq)): \n",
    "        print(\"Error\")\n",
    "        return [], 0\n",
    "    new_sequence_clean = handle_nan_values(new_spatially_norm_seq)\n",
    "    if new_sequence_clean.shape[0] < 2: \n",
    "        print(\"Error\") \n",
    "        return [], 0\n",
    "\n",
    "    new_sequence_time_norm = time_normalize_sequence(new_sequence_clean, Target_rep_length)\n",
    "    if np.isnan(new_sequence_time_norm).all(): \n",
    "        print(\"Error: Time normalization failed for whole sequence.\")\n",
    "        return [], 0\n",
    "    new_sequence_time_norm = np.nan_to_num(new_sequence_time_norm, nan=0.0)\n",
    "\n",
    "    initial_min_distance, initial_best_label = float('inf'), None\n",
    "    segment_p, segment_d = default_p, default_d\n",
    "    segment_exercise_config_dict = None\n",
    "\n",
    "    if not averaged_baseline_data: \n",
    "        print(\"Error: No baseline data.\")\n",
    "        return [], 0\n",
    "        \n",
    "    for ex_label, angle_dict in averaged_baseline_data.items():        \n",
    "        avg_base_seq = angle_dict.get('overall')\n",
    "        \n",
    "        if avg_base_seq is None: \n",
    "            continue\n",
    "        \n",
    "        if avg_base_seq.shape[0] != Target_rep_length or avg_base_seq.ndim != 2 or new_sequence_time_norm.shape[1] != avg_base_seq.shape[1]: \n",
    "            continue\n",
    "        \n",
    "        safe_base = np.nan_to_num(avg_base_seq, nan=0.0)\n",
    "        \n",
    "        try: \n",
    "            dist, _ = fastdtw(new_sequence_time_norm, safe_base, dist=euclidean)\n",
    "        except Exception: \n",
    "            continue\n",
    "        \n",
    "        if dist < initial_min_distance: \n",
    "            initial_min_distance, initial_best_label = dist, ex_label\n",
    "\n",
    "    angle_for_tuning_lookup = hint_angle if hint_angle in ['left', 'right', 'front'] else 'front'\n",
    "\n",
    "    if initial_best_label:\n",
    "        segment_exercise_config_dict = exercise_config.get(initial_best_label)\n",
    "        if segment_exercise_config_dict:\n",
    "             tuned_p, tuned_d = tuned_counting_params.get(initial_best_label, {}).get(angle_for_tuning_lookup, (default_p, default_d))\n",
    "             segment_p, segment_d = tuned_p, tuned_d\n",
    "        else:\n",
    "             print(f\"No configuration for initial guess '{initial_best_label}'. Using default parameters.\")\n",
    "             initial_best_label = None\n",
    "    else:\n",
    "        print(f\"Initial guess failed. Using default\")\n",
    "\n",
    "    segment_label_for_logic = initial_best_label if initial_best_label else \"unknown\"\n",
    "    test_repetitions = segment_repetitions(new_sequence_clean, segment_label_for_logic, joint_names, exercise_config,\n",
    "                                           peak_prominence=segment_p, peak_distance=segment_d)\n",
    "    num_detected_reps = len(test_repetitions)\n",
    "    print(f\"Detected {num_detected_reps} potential repetitions in test video.\")\n",
    "    \n",
    "    if num_detected_reps == 0: \n",
    "        return [], 0\n",
    "\n",
    "    repetition_results = []\n",
    "    comparison_angle_key = hint_angle if hint_angle in ['left', 'right', 'front'] else 'overall'\n",
    "    print(f\"Comparing each detected repetition against '{comparison_angle_key}' baselines...\")\n",
    "\n",
    "    for i, rep_segment in enumerate(test_repetitions):\n",
    "        if rep_segment is None or rep_segment.shape[0] < 2: \n",
    "            repetition_results.append( (i+1, \"Error: Invalid Segment\", float('inf')) )\n",
    "            continue\n",
    "        \n",
    "        time_norm_rep_segment = time_normalize_sequence(rep_segment, Target_rep_length)\n",
    "        if np.isnan(time_norm_rep_segment).all(): \n",
    "            repetition_results.append( (i+1, \"Error: Time Norm Failed\", float('inf')) )\n",
    "            continue\n",
    "        time_norm_rep_segment = np.nan_to_num(time_norm_rep_segment, nan=0.0)\n",
    "\n",
    "        min_rep_distance, best_rep_label_for_this_rep = float('inf'), None\n",
    "        for exercise_label, angle_dict in averaged_baseline_data.items():\n",
    "            baseline_to_compare = angle_dict.get(comparison_angle_key)\n",
    "            if baseline_to_compare is None or time_norm_rep_segment.shape != baseline_to_compare.shape: \n",
    "                continue\n",
    "\n",
    "            safe_baseline = np.nan_to_num(baseline_to_compare, nan=0.0)\n",
    "            try: \n",
    "                distance, _ = fastdtw(time_norm_rep_segment, safe_baseline, dist=euclidean)\n",
    "            except Exception as e: \n",
    "                print(f\"DTW error Rep {i+1} vs {exercise_label}/{comparison_angle_key}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            if distance < min_rep_distance: \n",
    "                min_rep_distance, best_rep_label_for_this_rep = distance, exercise_label\n",
    "\n",
    "        matched_label_for_rep = \"No Match / Inconclusive\"\n",
    "        threshold_to_use = fallback_threshold\n",
    "        if best_rep_label_for_this_rep:\n",
    "            threshold_specific = exercise_specific_thresholds.get(best_rep_label_for_this_rep, {}).get(comparison_angle_key)\n",
    "            if threshold_specific is None and comparison_angle_key != 'overall': \n",
    "                 threshold_specific = exercise_specific_thresholds.get(best_rep_label_for_this_rep, {}).get('overall')\n",
    "\n",
    "            if threshold_specific is not None: \n",
    "                 threshold_to_use = threshold_specific\n",
    "            if min_rep_distance < threshold_to_use:\n",
    "                 matched_label_for_rep = best_rep_label_for_this_rep\n",
    "            else: \n",
    "                 matched_label_for_rep = f\"No Match (Closest: {best_rep_label_for_this_rep}, Dist: {min_rep_distance:.2f} >= {threshold_to_use:.2f})\"\n",
    "\n",
    "        repetition_results.append( (i+1, matched_label_for_rep, min_rep_distance) )\n",
    "\n",
    "    return repetition_results, num_detected_reps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee2b3b-9754-4c36-aad8-20016a9ef706",
   "metadata": {},
   "source": [
    "### Load baseline data\n",
    "\n",
    "This cell executes the baseline creation and processing pipeline\n",
    "\n",
    "1.  `averaged_baselines`: (`dict`)\n",
    "    * Contains the final averaged, time-normalized pose sequence for each detected angle ('left', 'right', 'front', 'unknown') and the 'overall' average for each exercise.\n",
    "    * `{exercise_label: {angle_key: numpy.array}}` (`{'squat': {'front': array([...]), 'overall': array([...])}}`)\n",
    "    * Unit = Arrays contain unitless normalized coordinates.\n",
    "2.  `tuned_params`: (`dict`)\n",
    "    * Stores the optimal 'peak_prominence' and 'peak_distance' tuple for each exercise and angle category based on the labeled baseline videos.\n",
    "    * Structure: `{exercise_label: {angle_key: (prominence, distance)}}` (`{'squat': {'front': (0.06, 15), 'overall': (0.05, 10)}}`)\n",
    "    * Unit = Prominence (float) is unitless ratio, Distance (int) is in frames.\n",
    "4.  `derived_thresholds`: (`dict`)\n",
    "    * Stores the DTW recognition threshold calculated for each exercise and angle category (including 'overall') based on the variance within repetitions (Mean + N * stddev of intra-exercise distances). Contains `None` if a threshold couldn't be derived (too few repetitions).\n",
    "    * Structure: `{exercise_label: {angle_key: threshold_value or None}}` (`{'squat': {'front': 450.7, 'overall': 510.2}}`)\n",
    "    * Unit = Float, unitless distance (same scale as DTW results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf5c95-b29d-43e9-91ce-0b1c4d1f4009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bc6d984-17a8-4828-b9a7-b1b13c770753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learned_data_to_excel(config_data, tuned_params_data, threshold_data, filename, averaged_baseline_data,output_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Exports the exercise configuration, tuned parameters and derived thresholds to separate sheets in an Excel file.\n",
    "    \"\"\"\n",
    "    excel_filepath = os.path.join(output_dir, filename)\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    numpy_filepath = os.path.join(output_dir, f\"{base_name}_baselines.npy\")\n",
    "\n",
    "    # Prepare Excel data\n",
    "    config_list, tuned_list, threshold_list = [], [], []\n",
    "    for exercise, config in config_data.items(): \n",
    "        logic = config.get('trajectory_logic', {}) \n",
    "        config_list.append({'Exercise': exercise, 'Metric': logic.get('metric'), 'Joints': ', '.join(logic.get('joints', [])), 'Invert_for_Valley': logic.get('invert_for_valley')})\n",
    "    for exercise, angle_dict in tuned_params_data.items():\n",
    "         for angle, params in angle_dict.items(): \n",
    "             tuned_list.append({'Exercise': exercise, 'Angle': angle, 'Tuned_Prominence': params[0] if params else None, 'Tuned_Distance': params[1] if params else None})\n",
    "    for exercise, angle_dict in threshold_data.items():\n",
    "         for angle, threshold_val in angle_dict.items(): \n",
    "             threshold_list.append({'Exercise': exercise, 'Angle': angle, 'Derived_Threshold': threshold_val})\n",
    "\n",
    "    df_config = pd.DataFrame(config_list)\n",
    "    df_tuned = pd.DataFrame(tuned_list)\n",
    "    df_thresholds = pd.DataFrame(threshold_list)\n",
    "\n",
    "    with pd.ExcelWriter(excel_filepath, engine='openpyxl') as writer:\n",
    "        df_config.to_excel(writer, sheet_name='Exercise_Config', index=False)\n",
    "        df_tuned.to_excel(writer, sheet_name='Tuned_Counting_Params', index=False)\n",
    "        df_thresholds.to_excel(writer, sheet_name='Derived_Thresholds', index=False)\n",
    "    print(f\"\\n Exported data to file: {filename}...\")\n",
    "\n",
    "    np.save(numpy_filepath, averaged_baseline_data, allow_pickle=True)\n",
    "    print(f\" Averaged baselines exported to: {numpy_filepath}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc68a43-695b-4a92-9c12-23b431381629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5a22dea-b74e-49ba-b07c-4db4190c3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_precomputed_data_from_excel(base_filename, input_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Loads config, params, thresholds from Excel AND baselines from NumPy.)\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoad data from file: {base_filename}\")\n",
    "    excel_filepath = os.path.join(input_dir, base_filename)\n",
    "    base_name, _ = os.path.splitext(base_filename)\n",
    "    numpy_filepath = os.path.join(input_dir, f\"{base_name}_baselines.npy\")\n",
    "\n",
    "    loaded_avg_baselines = {}\n",
    "    loaded_tuned_params = {}\n",
    "    loaded_derived_thresholds = {}\n",
    "\n",
    "    df_tuned = pd.read_excel(excel_filepath, sheet_name='Tuned_Counting_Params')\n",
    "    for _, row in df_tuned.iterrows():\n",
    "        ex, ang = row['Exercise'], row['Angle']\n",
    "        prom = float(row['Tuned_Prominence'])\n",
    "        dist = int(row['Tuned_Distance'])\n",
    "        \n",
    "        if ex not in loaded_tuned_params: \n",
    "            loaded_tuned_params[ex] = {}\n",
    "        \n",
    "        loaded_tuned_params[ex][ang] = (prom if pd.notna(prom) else Default_Peak_Prominence,\n",
    "                                        dist if pd.notna(dist) else Default_Peak_Distance)\n",
    "\n",
    "    df_thresh = pd.read_excel(excel_filepath, sheet_name='Derived_Thresholds')\n",
    "    for _, row in df_thresh.iterrows():\n",
    "        ex, ang = row['Exercise'], row['Angle']\n",
    "        thresh_val = None\n",
    "        try:\n",
    "             thresh_val = float(row['Derived_Threshold'])\n",
    "             if np.isnan(thresh_val): thresh_val = None\n",
    "        except (TypeError, ValueError): \n",
    "            pass\n",
    "        \n",
    "        if ex not in loaded_derived_thresholds: \n",
    "            loaded_derived_thresholds[ex] = {}\n",
    "        loaded_derived_thresholds[ex][ang] = thresh_val\n",
    "\n",
    "    loaded_avg_baselines = np.load(numpy_filepath, allow_pickle=True).item()\n",
    "    print(f\"Loaded averaged baselines from: {numpy_filepath}\")\n",
    "\n",
    "    return loaded_avg_baselines, loaded_tuned_params, loaded_derived_thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc16a63-ee16-4976-9b9a-204a4d011e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "536d7e64-46a1-4bcc-b85e-4a1ffd144adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load data from file: exercise_learned_summary.xlsx\n",
      "Loaded averaged baselines from: .\\exercise_learned_summary_baselines.npy\n",
      "\n",
      " Exporting learned data to file: exercise_learned_summary.xlsx...\n",
      "  Metadata exported to: .\\exercise_learned_summary.xlsx\n",
      " Averaged baselines exported to: .\\exercise_learned_summary_baselines.npy\n",
      "\n",
      " Calculated recognition thresholds (Per Angle and Overall)\n"
     ]
    }
   ],
   "source": [
    "# In case of re-running the cell\n",
    "averaged_baselines = {}\n",
    "tuned_params = {}\n",
    "derived_thresholds = {}\n",
    "\n",
    "if Precomputed_model_parameters: \n",
    "    averaged_baselines, tuned_params, derived_thresholds = load_precomputed_data_from_excel(Precomputed_model_parameters)\n",
    "else: \n",
    "    averaged_baselines, tuned_params, derived_thresholds = load_baseline_data(\n",
    "        Video_baseline_path,\n",
    "        HRNET_MODEL,\n",
    "        Jointnames_list,\n",
    "        Target_rep_length,\n",
    "        EXERCISE_CONFIG,\n",
    "        Tuning_Prominance_range,\n",
    "        Tuning_Distance_range,\n",
    "        threshold_std_multiplier=Threshold_STD_Multiplier,\n",
    "        min_reps_for_threshold=Min_REPS_to_calc_threshold)\n",
    "\n",
    "if averaged_baselines:\n",
    "    learned_data_to_excel(\n",
    "    config_data=EXERCISE_CONFIG,\n",
    "    tuned_params_data=tuned_params,\n",
    "    threshold_data=derived_thresholds,\n",
    "    averaged_baseline_data=averaged_baselines, \n",
    "    filename=Precomputed_model_parameters)\n",
    "    print(\"\\n Calculated recognition thresholds (Per Angle and Overall)\")\n",
    "else:\n",
    "    print(\"\\n Baseline Data Loading/Processing Failed👎\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17905dae-3b22-4395-8d8c-f400f4bdcf9b",
   "metadata": {},
   "source": [
    "### Testvideo\n",
    "\n",
    "This cell analyzing a new video file.\n",
    "\n",
    "**User Inputs in this Cell:**\n",
    "* `video_to_test`: (`str` or `pathlib.Path`)\n",
    "    * Specifies the full path to the video file to be classified. Defaults to `Video_test_path` from Cell 2 if not changed.\n",
    "* `manual_angle_hint`: (`str` or `None`)\n",
    "    * Option allows you to tell the recognition function the expected camera angle ('left', 'right', 'front').\n",
    "    * Unit = Category label (`'left'`, `'right'`, `'front'`) or `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17a2cd-6d02-4598-b633-b4b37465bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DTW_Distance_threshold = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c0238e4-ad57-41b9-93ee-2c80906d2c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================================================================\n",
      " Recognizing Exercise in Video\n",
      " Video: test_excercise.mp4 \n",
      "==========================================================================================================================================\n",
      "\n",
      " Recognizing Exercise (Angle Hint='None') for: test_excercise.mp4 ---\n",
      "Detected 5 potential repetitions in test video.\n",
      "Comparing each detected repetition against 'overall' baselines...\n",
      "\n",
      "==========================================================================================================================================\n",
      " Results for test_excercise.mp4\n",
      "==========================================================================================================================================\n",
      " Detected 5 Repetitions.\n",
      "\n",
      " Per repetition Analysis\n",
      " Rep 1: -> Match: leg_extension (Dist: 844.21)\n",
      " Rep 2: -> No Match (Closest: leg_extension, Dist: 1224.46 >= 1049.06)\n",
      " Rep 3: -> Match: tricep_pushdown (Dist: 1529.70)\n",
      " Rep 4: -> Match: tricep_pushdown (Dist: 1911.55)\n",
      " Rep 5: -> No Match (Closest: leg_extension, Dist: 1532.61 >= 1049.06)\n",
      "\n",
      " Overall Summary\n",
      "Overall match: tricep_pushdown (2/3 successfully matched reps)\n",
      " Total reps successfully matched: 3 / 5\n",
      "==========================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_to_test = Video_test_path\n",
    "\n",
    "# Optional angle hint\n",
    "manual_angle_hint = None #'left', 'right', 'front'\n",
    "\n",
    "#if proceed_to_recognition:\n",
    "print(\"==========================================================================================================================================\")\n",
    "print(f\" Recognizing Exercise in Video\")\n",
    "print(f\" Video: {os.path.basename(video_to_test)} \")\n",
    "print(\"==========================================================================================================================================\")\n",
    "\n",
    "list_of_rep_results, total_reps_detected = recognize_exercise(\n",
    "    new_video_path=video_to_test,\n",
    "    averaged_baseline_data=averaged_baselines,\n",
    "    hrnet_model=HRNET_MODEL,\n",
    "    joint_names=Jointnames_list,\n",
    "    exercise_config=EXERCISE_CONFIG,\n",
    "    Target_rep_length=Target_rep_length,\n",
    "    tuned_counting_params=tuned_params,\n",
    "    exercise_specific_thresholds=derived_thresholds,\n",
    "    hint_angle=manual_angle_hint,\n",
    "    fallback_threshold=DTW_Distance_threshold)\n",
    "\n",
    "print(\"\\n==========================================================================================================================================\")\n",
    "print(f\" Results for {os.path.basename(video_to_test)}\")\n",
    "print(\"==========================================================================================================================================\")\n",
    "print(f\" Detected {total_reps_detected} Repetitions.\")\n",
    "\n",
    "if not list_of_rep_results:\n",
    "    print(\"No valid repetition results to display\")\n",
    "else:\n",
    "    print(\"\\n Per repetition Analysis\")\n",
    "    match_counts = {}\n",
    "    successful_reps = 0\n",
    "    overall_best_label = None\n",
    "    valid_matches = [res[1] for res in list_of_rep_results if isinstance(res[1], str) and not res[1].startswith(\"Error\") and not res[1].startswith(\"No Match\")]\n",
    "    if valid_matches:\n",
    "         temp_match_counts = {}\n",
    "         for label in valid_matches: \n",
    "             temp_match_counts[label] = temp_match_counts.get(label, 0) + 1\n",
    "         if temp_match_counts: \n",
    "             overall_best_label = max(temp_match_counts, key=temp_match_counts.get)\n",
    "\n",
    "    for rep_index, matched_label, min_distance in list_of_rep_results:\n",
    "        status = \"\"\n",
    "        if isinstance(matched_label, str) and matched_label.startswith(\"Error\"):\n",
    "             status = f\"-> {matched_label}\"\n",
    "        elif isinstance(matched_label, str) and matched_label.startswith(\"No Match\"):\n",
    "             status = f\"-> {matched_label}\"\n",
    "        else:\n",
    "            status = f\"-> Match: {matched_label} (Dist: {min_distance:.2f})\"\n",
    "            match_counts[matched_label] = match_counts.get(matched_label, 0) + 1\n",
    "            successful_reps += 1\n",
    "        print(f\" Rep {rep_index}: {status}\")\n",
    "\n",
    "    print(\"\\n Overall Summary\")\n",
    "    if successful_reps > 0:\n",
    "         if match_counts:\n",
    "             majority_label = max(match_counts, key=match_counts.get)\n",
    "             majority_count = match_counts[majority_label]\n",
    "             print(f\"Overall match: {majority_label} ({majority_count}/{successful_reps} successfully matched reps)\")\n",
    "         else:\n",
    "              print(\" No specific exercises were matched consistently.\")\n",
    "         print(f\" Total reps successfully matched: {successful_reps} / {total_reps_detected}\")\n",
    "    elif total_reps_detected > 0:\n",
    "         print(\"No repetitions were successfully matched to any baseline exercise.\")\n",
    "    else:\n",
    "         print(\"No repetitions were detected in the video\")\n",
    "print(\"==========================================================================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d936f-aa4d-4189-af8e-9cb9b30c153e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
